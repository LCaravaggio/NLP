{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Vamos a entrenar un modelo de lenguaje neuronal feed-forward basado en una ventana de contexto fija y embeddings estáticos. Como datos de entrenamiento, vamos a usar recetas de cocina en español.\n","\n","-----------------------\n","\n","Tarea: responder donde dice **PREGUNTA**"],"metadata":{"id":"DwU_koArH8JW"}},{"cell_type":"markdown","source":["## Configuración del entorno"],"metadata":{"id":"q2xVhaQ46Wyq"}},{"cell_type":"code","source":["!pip install -qU datasets spacy watermark"],"metadata":{"id":"3cHmzFdYbDTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!python -m spacy download es_core_news_sm"],"metadata":{"id":"dDUxxsDNLkN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%reload_ext watermark"],"metadata":{"id":"Xv815WnZBI41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%watermark -vmp datasets,spacy,torch,numpy,pandas,tqdm"],"metadata":{"id":"zmbjr89RBKpR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para usar GPU, arriba a la derecha seleccionar \"Change runtime type\" --> \"T4 GPU\".\n","\n","Es un buena idea desarrollar con CPU, y usar GPU para la corrida final, para que Google no nos limite el uso. En esta notebook puede ser útil usar GPU."],"metadata":{"id":"Hts4XQ9L9ptO"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"9QknGEM69eO1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 1**: ¿para qué sirve específicamente trabajar con GPU?"],"metadata":{"id":"D3xtEBcwqjO5"}},{"cell_type":"markdown","source":["## Dataset\n","\n","Vamos a usar un [corpus de recetas de SomosNLP](https://huggingface.co/datasets/somosnlp/RecetasDeLaAbuela)."],"metadata":{"id":"pN9ZrHk5h6mL"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"somosnlp/RecetasDeLaAbuela\", \"version_1\")"],"metadata":{"id":"gzVdx6xVh6Sj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vemos la estructura:\n","print(dataset)"],"metadata":{"id":"xMpZStLajfTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Conservamos pais = \"ESP\":\n","dataset = dataset.filter(lambda x: x[\"Pais\"] == \"ESP\")"],"metadata":{"id":"gsyy4VwxApuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# vemos un ejemplo al azar:\n","dataset[\"train\"][300]"],"metadata":{"id":"alRfTgMcjnez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A veces los textos son listas no parseadas como tales.\n","# En tal caso, hacemos un join de la lista.\n","import re\n","\n","def preprocess(example):\n","    \"\"\"\n","    \"\"\"\n","    if example[\"Pasos\"].startswith(\"[\"):\n","        pasos_list = eval(example[\"Pasos\"].encode('unicode_escape'))\n","        example[\"Pasos\"] = \" \".join(pasos_list)\n","    # Eliminamos whitespace duplicado:\n","    example[\"Pasos\"] = re.sub(r'\\s+', ' ', example[\"Pasos\"])\n","    return example\n","\n","dataset = dataset.map(preprocess)"],"metadata":{"id":"my5Jr-FHARC4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset[\"train\"][300]"],"metadata":{"id":"abSposkFCH7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hacemos un partición train/test y achicamos (solo para trabajar mas rapido). Y conservamos solo el texto de las recetas."],"metadata":{"id":"uhD4vWA9-24O"}},{"cell_type":"code","source":["dataset = dataset.shuffle(seed=33)"],"metadata":{"id":"7jv8x2Kb-7ew"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_train = dataset[\"train\"].select(range(0, 4_000))[\"Pasos\"]\n","texts_test = dataset[\"train\"].select(range(4_000, 8_000))[\"Pasos\"]"],"metadata":{"id":"Z7rgvwpCrY_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import textwrap\n","\n","print(textwrap.fill(texts_train[33], 100))"],"metadata":{"id":"W-jz484l2gYV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Construcción del vocabulario y tokenización\n","\n","Vamos a usar el tokenizer para español de `spacy`.\n","\n","El objetivo es generar una **lista de n-gramas para entrenar la\n","NN**. e.g con n=4, queremos tuplas de (3 palabras de contexto, 1 target).\n","\n","Vamos a:\n","\n","* Considerar como parte del vocabulario todas las palabras que ocurran al menos dos veces.\n","* Hacer \"padding\" con BOS y EOS tokens.\n","* Tokenizar cada documento y convertir a token IDs según el vocab.\n","* Pasar de tokens a n-gramas y generar una sola lista con todos los samples de entrenamiento.\n"],"metadata":{"id":"gOR6scsQjyay"}},{"cell_type":"code","source":["# tokenizer con reglas de puntacion, contracciones, etc:\n","import spacy\n","\n","tokenizer = spacy.load('es_core_news_sm')"],"metadata":{"id":"M6-oSyEvnXFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veamos un ejemplo:\n","doc = tokenizer(texts_train[0])\n","print(doc.text)\n","for i, token in enumerate(doc):\n","    print(token.text)\n","    if i > 15:\n","        break"],"metadata":{"id":"wGRXkOtFDqCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","def create_vocab(docs: list, min_frec=2) -> tuple:\n","    \"\"\"Crea un vocabulario a partir de una lista de docs.\n","    Returns:\n","        Dos diccionarios: token2idx (palabra -> índice) y idx2token (índice -> palabra)\n","    \"\"\"\n","    # NOTE esto se puede acelerar paralelizando la tokenizacion con datasets.map()\n","    # y luego usar e.g. pandas explode().value_counts(). Además, podriamos\n","    # aprovechar y ya guardar el dataset de train tokenizado.\n","    str2count = {}\n","    for doc in tqdm(docs):\n","        for token in tokenizer(doc):\n","            token = token.text\n","            str2count[token] = str2count.get(token, 0) + 1\n","    # filtrar por min_frec:\n","    str2count = {token: count for token, count in str2count.items() if count >= min_frec}\n","    # ordenar de mayor a menor frecuencia:\n","    str2count = dict(sorted(str2count.items(), key=lambda x: x[1], reverse=True))\n","    # Mapeamos cada token a un índice distinto\n","    token2idx = {token: idx for idx, token in enumerate(str2count)}\n","    # Agregamos \"<unk>\", \"<bos>\", \"<eos>\"  al vocab:\n","    token2idx[\"<unk>\"] = len(str2count)\n","    token2idx[\"<bos>\"] = len(str2count) + 1\n","    token2idx[\"<eos>\"] = len(str2count) + 2\n","    # \"Invertir\" el diccionario:\n","    idx2token = {idx: token for idx, token in enumerate(token2idx)}\n","    return token2idx, idx2token\n","\n","\n","token2idx, idx2token = create_vocab(texts_train)"],"metadata":{"id":"frt0caaLEr8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(token2idx))\n","print(token2idx[\"<unk>\"], token2idx[\"<bos>\"], token2idx[\"<eos>\"])"],"metadata":{"id":"TJXOkxQ0HUqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import Tensor\n","\n","def tokenize(doc: str, ngram_order: int = 4) -> Tensor:\n","  \"\"\"Convierte documento a tensor de token IDs.\n","  Agrega n-1 BOS y 1 EOS tokens (end-of-seq. y beg-of-seq).\n","  \"\"\"\n","  token_ids = [token2idx.get(token.text, token2idx[\"<unk>\"]) for token in tokenizer(doc)]\n","  # agregamos BOS y EOS tokens:\n","  token_ids = [token2idx[\"<bos>\"]] * (ngram_order - 1) + token_ids + [token2idx[\"<eos>\"]]\n","  return torch.tensor(token_ids, dtype=torch.long)\n","\n","print(texts_train[0])\n","print(tokenize(texts_train[0])[:20])"],"metadata":{"id":"_UtFtN3nFwzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def doc2ngrams(doc: str, ngram_order: int = 4) -> list:\n","  \"\"\"Convierte un documento en tuplas de\n","  ([ idx_i-context_size, ..., idx_i-1 ], target_idx), donde cada elemento de la tupla\n","  es un tensor de token IDs.\n","  \"\"\"\n","  token_ids = tokenize(doc, ngram_order=ngram_order)\n","  ngrams_list = [\n","      (token_ids[(i-ngram_order):(i-1)], token_ids[i-1])\n","      for i in range(ngram_order, len(token_ids) + 1)\n","  ]\n","  return ngrams_list"],"metadata":{"id":"PI9LY8AjMfq0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# por ejemplo:\n","doc_ = texts_train[0]\n","token_ids_ = tokenize(doc_)\n","ngrams_ = doc2ngrams(doc_)\n","\n","print(doc_)\n","print(token_ids_[:10])\n","print(ngrams_)"],"metadata":{"id":"3kfR3OzHMgoH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# armamos todos los ngrams de training:\n","ngrams_train = []\n","for doc in tqdm(texts_train):\n","  ngrams_train.extend(doc2ngrams(doc, ngram_order=4))"],"metadata":{"id":"FW93Ku_RLfHZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(ngrams_train[:2])"],"metadata":{"id":"G1ye2KkYQBwQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Armado de _batches_\n","\n","Armamos los batches para entrenar el modelo. Para esto usamos la clase `DataLoader` de PyTorch. En cada iteración, el `DataLoader` nos devuelve un batch de ejemplos. No necesitamos una _collate function_ porque ya todos los ejemplos tienen igual dimensión (no necesitamos padding)."],"metadata":{"id":"9YjMSjHnLmtE"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","batch_size = 32\n","\n","train_loader = DataLoader(ngrams_train, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"YvTLRl7JLnE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veamos los primeros dos batches de entrenamiento:\n","torch.manual_seed(33)\n","for i, data in enumerate(train_loader):\n","    print(f\"### batch {i}\")\n","    print(f\"Shapes = {[s.shape for s in data]}\")\n","    print(\"Primeros 5 ejemplos:\")\n","    print(\"- Features:\")\n","    print(data[0][:5])\n","    print(\"- Targets:\")\n","    print(data[1][:5])\n","    print()\n","    if i == 1:\n","        break"],"metadata":{"id":"1cBUbcqEMYNR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 2**: ¿Qué información tiene cada ejemplo en un batch?\n","\n","**PREGUNTA 3**: ¿para qué sirve hacer procesamiento en batches?"],"metadata":{"id":"tAk34-7bc5Bo"}},{"cell_type":"markdown","source":["## Modelo\n","\n","Armamos una red bien sencilla con una hidden layer. Es la misma arquitectura que Figure 7.17 de [Jurafksy](https://web.stanford.edu/~jurafsky/slp3/). Usamos embeddings con inicialización random pero podríamos empezar con embeddings pre-entrenados.\n","\n","NOTE: Como vamos a usar [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), no tenemos que aplicar softmax porque espera \"raw, unnormalized scores for each class\" i.e. logits.\n","\n","**PREGUNTA 4**: ¿qué ventaja tiene inicializar la red con embeddings pre-entrenados? ¿qué embeddings podríamos usar para esto?\n"],"metadata":{"id":"Dtx-oJRAcsRy"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","class NGramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size, embedding_dim, hidden_size, ngram_order):\n","        super().__init__()\n","        context_size = ngram_order - 1\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, inputs):\n","        embeds = self.embeddings(inputs) # shape (bsz, context_size, embed_dim)\n","        concatenated_embeds = embeds.flatten(1) # shape (bsz, context_size * embed_dim)\n","        hidden = F.relu(self.linear1(concatenated_embeds)) # shape (bsz, hidden_size)\n","        logits = self.linear2(hidden) # shape (bsz, vocab_size)\n","        return logits\n"],"metadata":{"id":"j_xcKS3NcrB3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 5**: ¿en qué atributos de NGramLanguageModel están los pesos de la red?\n","\n","**PREGUNTA 6**: ¿Qué representa el método forward?"],"metadata":{"id":"Ifa2Go7cchDj"}},{"cell_type":"markdown","source":["## Entrenamiento\n"],"metadata":{"id":"6Ec3_tL41LDs"}},{"cell_type":"code","source":["# Instanciamos el modelo\n","neural_lm = NGramLanguageModel(\n","    vocab_size=len(token2idx),\n","    embedding_dim=50,\n","    hidden_size=32,\n","    ngram_order=4,\n",")\n","neural_lm = neural_lm.to(device)"],"metadata":{"id":"zpWEyxzrQlM-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Funcion de pérdida y optimizador\n","from torch import optim\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(neural_lm.parameters(), lr=1e-3)"],"metadata":{"id":"sSGYXaczQgFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loop de entrenamiento (sin datos de validación)\n","\n","def train_epoch(model, optimizer, train_loader, log_steps=2000, device=None):\n","    \"\"\"Entrena 1 epoch\n","    \"\"\"\n","    total_loss = 0\n","    steps_done = 0\n","    n_steps = len(train_loader)\n","    for context, target in tqdm(train_loader, total=n_steps):\n","        context = context.to(device)\n","        target = target.to(device)\n","        optimizer.zero_grad()\n","        logits = model(context)\n","        loss = loss_fn(logits, target)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","        steps_done += 1\n","        train_loss = total_loss / steps_done\n","        if steps_done % log_steps == 0:\n","            print(f\"    [steps={steps_done}] train_loss: {train_loss:.4f}\")\n","    return train_loss\n","\n","def train(\n","    model, optimizer, train_loader, n_epochs, device=None):\n","  \"\"\"Entrena el modelo durante n_epochs.\n","  \"\"\"\n","  for epoch in range(n_epochs):\n","      print(f\"Epoch {epoch} / {n_epochs}\")\n","      epoch_loss = train_epoch(model, optimizer, train_loader, device=device)\n","      print(f\"Training loss = {epoch_loss:.3f}\")"],"metadata":{"id":"ZZx2A2qN2Gjy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 7**: ¿Qué es un epoch? ¿Qué es un paso de optimización? ¿Cómo podemos calcular la cantidad máxima de pasos de optimización del entrenamiento?"],"metadata":{"id":"MdYXfq2xdMxA"}},{"cell_type":"code","source":["# entrenamos!\n","num_epochs = 1\n","train(neural_lm, optimizer, train_loader, num_epochs, device=device)"],"metadata":{"id":"eR6yqrAU2EhI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generación de texto"],"metadata":{"id":"u6pPesmuTke6"}},{"cell_type":"code","source":["def text2input(context_str: str, ngram_order: int = 4) -> Tensor:\n","    \"\"\"Convierte contexto en un input para la NN (tensor de input IDs)\n","    \"\"\"\n","    ngrams = doc2ngrams(context_str, ngram_order=ngram_order)\n","    # el input es el \"contexto\" del ultimo ngram\n","    last_context = ngrams[-1][0]\n","    # agregamos una dimension que hace las veces de batch (size=1) para hacer el forward\n","    out = last_context.unsqueeze(0)\n","    return out"],"metadata":{"id":"tNejXRk8TkJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ejemplo:\n","print(text2input(\"usamos la\", ngram_order=4))\n","print(text2input(\"\", ngram_order=4))"],"metadata":{"id":"HIIOTcIfT_cg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sample_text(model, start_text, max_length=10, ngram_order=4, greedy=False):\n","    \"\"\"Generación autorregresiva aleatoria de texto sampleando de softmax.\n","    El modelo debe ser consistente con ngram_order.\n","    \"\"\"\n","    # buscamos los input IDs segun el context size\n","    input_ = text2input(start_text, ngram_order=ngram_order)\n","    # mandamos inputs al mismo device que el modelo\n","    device = next(model.parameters()).device\n","    input_ = input_.to(device)\n","    idx_eos = token2idx[\"<eos>\"]\n","    context_size = ngram_order - 1\n","    # el resultado solo va a incluir el contexto usado + el texto nuevo\n","    idxs_result = input_.clone()\n","    with torch.inference_mode():\n","        for i in range(max_length):\n","            logits = model(input_) # logits\n","            probas = F.softmax(logits, dim=1) # probas\n","            if greedy:\n","                sampled_idx = torch.argmax(probas, dim=1).unsqueeze(1)\n","            else:\n","                # sample:\n","                sampled_idx = torch.multinomial(probas, num_samples=1)\n","            # actualizamos el resultado\n","            idxs_result = torch.cat((idxs_result, sampled_idx), dim=1)\n","            # actualizamos el input conservando solo los ultimos context_size tokens\n","            input_ = idxs_result[:,-context_size:]\n","            if sampled_idx == idx_eos:\n","                break\n","        tokens_result = [idx2token[idx.item()] for idx in idxs_result.squeeze()]\n","        return tokens_result"],"metadata":{"id":"KeMVbCy-XjQk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(texts_test[0])"],"metadata":{"id":"w0eTXvVzYcpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(0)\n","start_text = \"1 Cortar los calamares\"\n","res_ = sample_text(neural_lm, start_text, ngram_order=4, max_length=50)\n","\n","print(\" \".join(res_))"],"metadata":{"id":"GqmaBcdKYUzs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(22)\n","start_text = \"\"\n","res_ = sample_text(neural_lm, start_text, ngram_order=4, max_length=50)\n","\n","print(\" \".join(res_))"],"metadata":{"id":"ar_a_qZlZDPo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 8**: ¿por qué los textos generados son incoherentes?"],"metadata":{"id":"VGKtomy5d_u4"}},{"cell_type":"code","source":["start_text = \"\"\n","res_ = sample_text(neural_lm, start_text, ngram_order=4, max_length=50, greedy=True)\n","\n","print(\" \".join(res_))"],"metadata":{"id":"DAIkC8j22Ug_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 9**: ¿por qué el texto generado es repetitivo?"],"metadata":{"id":"h1rUgyE5dxU8"}},{"cell_type":"markdown","source":["## Evaluación\n","\n","Computamos perplexity (PPL) en test.\n","\n","* Hacemos $ \\exp(\\log PPL ) $ para evitar underflow.\n","* Vean que $\\log PPL = CrossEntropy = -avg(\\log(probas))$"],"metadata":{"id":"yYd_4p8Q9Rcs"}},{"cell_type":"code","source":["ngrams_test = []\n","for doc in tqdm(texts_test):\n","  ngrams_test.extend(doc2ngrams(doc, ngram_order=4))"],"metadata":{"id":"UvaJlCzZ4Br-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loader = DataLoader(ngrams_test, batch_size=32, shuffle=False)"],"metadata":{"id":"DlxeAZ9G4PaZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def perplexity(model, dataloader, device):\n","    with torch.no_grad():\n","        # Iteramos por batch. Vamos a ir guardando las probas de los tokens correctos en cada batch.\n","        all_log_probs_gt = torch.tensor([], device=device) # gt: ground truth\n","        for context, target in dataloader:\n","            context = context.to(device)\n","            target = target.to(device)\n","            batch_size = len(target)\n","            logits = model(context) # shape (bsz, vocab_size)\n","            log_probs = F.log_softmax(logits, dim=1) # shape (bsz, vocab_size)\n","            # log_probs_gt:\n","            log_probs_gt = log_probs[torch.arange(batch_size), target] # shape (bsz)\n","            all_log_probs_gt = torch.cat((all_log_probs_gt, log_probs_gt))\n","        # Calculamos PPL:\n","        ce = -all_log_probs_gt.mean()\n","        res = torch.exp(ce)\n","    return res.item()"],"metadata":{"id":"IsuUZCu25Gr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_ppl = perplexity(neural_lm, test_loader, device)\n","print(f\"Test PPL = {test_ppl:.3f}\")"],"metadata":{"id":"bWAjtowSLFuA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 10**: ¿El rendimiento de este modelo es mejor o peor que el ngram de la notebook \"ngramLM\"? ¿Por qué?"],"metadata":{"id":"9YtQ7F-ieFlu"}},{"cell_type":"code","source":[],"metadata":{"id":"Yqw28QzI6Tkf"},"execution_count":null,"outputs":[]}]}