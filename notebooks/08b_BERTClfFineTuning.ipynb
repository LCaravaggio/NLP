{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/08b_BERTClfFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDSQ2SpygjBj"
      },
      "source": [
        "Vamos a predecir el sentimiento de reviews de películas usando dos enfoques.\n",
        "\n",
        "1. **BERT + fine-tuning**: fine-tuneamos BERT + una capa de clasifición lineal en el dataset de reviews.\n",
        "2. **BERT \"pre-fine-tuneado\"**: usamos un modelo (BERT + clasificador) previamente entrenado para análisis de sentimiento, sin entrenar en nuestros datos.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Tarea: responder donde dice **PREGUNTA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adLynhD0gjBn"
      },
      "source": [
        "### Configuración del entorno\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrI-AEHLgjBo"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers accelerate datasets watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSTgqVh2gjBp"
      },
      "outputs": [],
      "source": [
        "%reload_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH0M270SgjBr"
      },
      "outputs": [],
      "source": [
        "%watermark -vmp transformers,datasets,torch,numpy,pandas,tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para usar GPU, arriba a la derecha seleccionar \"Change runtime type\" --> \"T4 GPU\""
      ],
      "metadata": {
        "id": "fC2quKgooVNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly0sQpe0gjBr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKkqwrY2gjBr"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Cargamos y exploramos el dataset de reviews de películas de imdb."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxybhTF_gjBr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtyJpx-6gjBs"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgZGR6MlgjBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    \"\"\"Muestra num_examples ejemplos aleatorios del dataset.\n",
        "    \"\"\"\n",
        "    indices = np.random.randint(0, len(dataset), num_examples)\n",
        "    df = pd.DataFrame(dataset[indices])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "np.random.seed(33)\n",
        "show_random_elements(dataset[\"train\"], num_examples=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN2rgvjBgjBs"
      },
      "outputs": [],
      "source": [
        "print(\"Distribucion de clases:\")\n",
        "for k in dataset.keys():\n",
        "    print(k)\n",
        "    print(pd.Series(dataset[k][\"label\"]).value_counts())\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC9MimWBgjBt"
      },
      "outputs": [],
      "source": [
        "print(\"Largo de los documentos (en palabras), deciles:\")\n",
        "for k in dataset.keys():\n",
        "    print(k)\n",
        "    largos = pd.Series(dataset[k][\"text\"]).str.split().apply(len)\n",
        "    print(np.quantile(largos, q=np.arange(0, 1.1, .1)).astype(int))\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9FyFIGlgjBt"
      },
      "outputs": [],
      "source": [
        "# Esto nos va a servir para más adelante:\n",
        "label_names = dataset[\"train\"].features[\"label\"].names\n",
        "label2id = {name: dataset[\"train\"].features[\"label\"].str2int(name) for name in label_names}\n",
        "id2label = {id: label for label, id in label2id.items()}\n",
        "\n",
        "print(label_names)\n",
        "print(id2label[0], id2label[1])\n",
        "print(label2id[\"neg\"] , label2id[\"pos\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvPRhMCmgjB2"
      },
      "source": [
        "## _Fine-tuning_ de BERT\n",
        "\n",
        "Vamos a usar BERT para extraer una representación vectorial de cada secuencia y entrenar un clasificador lineal por encima. Entrenamos _toda_ la arquitectura en simultáneo en nuestros datos. Como partimos de pesos pre-entrenados, a esto se le llama **fine-tuning**.\n",
        "\n",
        "Vamos a usar funciones de Hugging Face que van a automatizar muchas de las tareas que hicimos manualmente cuando usamos el modelo con embeddings estáticos word2vec.\n",
        "\n",
        "Empezamos cargando el tokenizador y el modelo pre-entrenado de HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOE0zjJHgjB2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
        ")\n",
        "bert_model = bert_model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 6**: ¿qué hace `.to(device)`?"
      ],
      "metadata": {
        "id": "RkxPTyLVu1Mb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v36IqcLWgjB2"
      },
      "source": [
        "El primer paso es la **tokenización**:\n",
        "\n",
        "convertir cada ejemplo en una secuencia de tokens que el modelo pueda procesar. En particular, cada ejemplo queda representado como un diccionario del tipo `{'input_ids': ..., 'attention_mask': ..., 'label': ...}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYe6yVzigjB2"
      },
      "outputs": [],
      "source": [
        "def tokenize_fn(examples):\n",
        "    \"\"\"Tokenización **sin aplicar padding** --> Lo aplicamos luego dinámicamente,\n",
        "    en cada batch de entrenamiento\n",
        "    \"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"], truncation=True, max_length=tokenizer.model_max_length\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo:\n",
        "subset_example = dataset[\"train\"][:3]\n",
        "tokenized_subset = tokenize_fn(subset_example)\n",
        "\n",
        "for k, v in tokenized_subset.items():\n",
        "    print(k)\n",
        "    print(v)\n",
        "    print(len(v))\n",
        "    print(\"Largo de cada input:\", [len(x) for x in v])\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "riQIbxANGXUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXPqGYu2gjB3"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset.map(tokenize_fn)\n",
        "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "6bcVoQ2ES7E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mLh1BIIgjB4"
      },
      "outputs": [],
      "source": [
        "print(bert_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6umYKkegjB4"
      },
      "outputs": [],
      "source": [
        "param_names = [name for name, _ in bert_model.named_parameters()]\n",
        "print(len(param_names))\n",
        "print(param_names[:5])\n",
        "print(param_names[-5:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6_oFOPFgjB3"
      },
      "source": [
        "Vamos a hacer _fine-tuning_ de todos los pesos del modelo.\n",
        "\n",
        "Alternativamente, podríamos entrenar la capa de clasificación y las últimas N capas de BERT, dejando las demás capas _congeladas_, corriendo esto:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvG1Ty2qgjB4"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # freeze todas las capas\n",
        "    for param in bert_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    # descongelar las ultimas 2 capas\n",
        "    for param in bert_model.pre_classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in bert_model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "    # y los N ultimos transformer blocks:\n",
        "    for param in bert_model.distilbert.transformer.layer[-2:].parameters():\n",
        "        param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 7**: ¿qué quiere decir \"congelar una capa\"?"
      ],
      "metadata": {
        "id": "lc9iWMNSvH6i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p03J3JqegjB5"
      },
      "source": [
        "Usamos un **data collator** de HF que se encarga de agrupar los ejemplos en batches y hacer padding dinámicamente (esto es, padding solo hasta la longitud del ejemplo más largo en cada batch)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO0wFAy-gjB5"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8na5-K9tgjB5"
      },
      "source": [
        "Hacemos una función para evaluar métricas durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs-niWxCgjB5"
      },
      "outputs": [],
      "source": [
        "from transformers import EvalPrediction\n",
        "from torch import nn\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_metrics(logits, labels):\n",
        "    \"\"\"Args:\n",
        "        logits: array shape (batch_size, num_labels)\n",
        "        labels: array shape (batch_size,)\n",
        "    \"\"\"\n",
        "    # Usamos torch para usar loss_fn, pero podriamos usar cpu y numpy\n",
        "    if not isinstance(logits, torch.Tensor):\n",
        "        logits = torch.tensor(logits)\n",
        "    if not isinstance(labels, torch.Tensor):\n",
        "        labels = torch.tensor(labels)\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    accuracy = (predictions == labels).float().mean().item()\n",
        "    cross_entropy = loss_fn(logits, labels).item()\n",
        "    return {\"accuracy\": accuracy, \"cross_entropy\": cross_entropy}\n",
        "\n",
        "def compute_metrics_for_hf(pred: EvalPrediction) -> dict:\n",
        "    \"\"\"EvalPrediction: tupla con dos elementos: predictions y label_ids\n",
        "    NOTE Trainer will put in EvalPrediction everything the model returns.\n",
        "    \"\"\"\n",
        "    logits = pred.predictions\n",
        "    labels = pred.label_ids\n",
        "    return compute_metrics(logits, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKPpvDS3gjB6"
      },
      "source": [
        "Para hacer el entrenamiento, usamos la clase `Trainer` de HF: funciona como un _wrapper_ que se encarga de hacer el loop de entrenamiento y evaluación que hicimos manualmente antes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2\n",
        "batch_size = 32\n",
        "optimization_steps = int(np.ceil(len(tokenized_dataset[\"train\"]) * n_epochs / batch_size))\n",
        "\n",
        "print(f\"N epochs: {n_epochs}\")\n",
        "print(f\"Batch size: {batch_size}\")\n",
        "print(f\"Optimization steps: {optimization_steps}\")"
      ],
      "metadata": {
        "id": "GfCEuskIUMOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEXngJDhgjB6"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"distilbert-ft-reviews\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    max_steps=optimization_steps,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    logging_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\", # el nombre de la metrica en compute_metrics()\n",
        "    push_to_hub=False,\n",
        "    seed=33,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    bert_model, args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics_for_hf,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSqcrY_lgjB6"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1aRdqAgjB7"
      },
      "outputs": [],
      "source": [
        "# Evaluar en test:\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_results)"
      ],
      "metadata": {
        "id": "0U5zGZ9vdwpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVviRSXVgjB7"
      },
      "source": [
        "## BERT _pre-fine-tuned_\n",
        "\n",
        "En lugar de hacer _fine-tuning_ de BERT en el dataset de reviews, podemos usar un modelo BERT que ya haya sido _fine-tuneado_ para resolver esta tarea, aunque sea en un dataset distinto.\n",
        "\n",
        "Esto se conoce como \"zero-shot\", y es útil cuando no tenemos datos anotados para entrenar. Es como usar un modelo \"en producción\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgQ_V4H-gjB7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_clf = pipeline(\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=device, batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencia en dataset de test:\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "\n",
        "test_outputs = []\n",
        "for output in sentiment_clf(KeyDataset(dataset[\"test\"], \"text\"), top_k=None):\n",
        "    test_outputs.append(output)\n",
        "\n",
        "# Usamos KeyDataset para trabajar con el input que nos interesa como si fuera un\n",
        "# dataset. Esto optimiza el cómputo. Ver:\n",
        "# https://huggingface.co/docs/transformers/en/pipeline_tutorial#using-pipelines-on-a-dataset"
      ],
      "metadata": {
        "id": "VaDfLVTwOFJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_outputs)"
      ],
      "metadata": {
        "id": "erlI4CgWNhtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_outputs[0]"
      ],
      "metadata": {
        "id": "nWlijmvnhPef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Queremos los logits para calcular la pérdida. Para eso, cargamos el pipeline con el argumento `function_to_apply=\"none\"`.\n",
        "\n"
      ],
      "metadata": {
        "id": "wsJhB-TRxiRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_clf = pipeline(\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=device, batch_size=32, function_to_apply=\"none\"\n",
        ")\n",
        "\n",
        "test_outputs = []\n",
        "for output in sentiment_clf(KeyDataset(dataset[\"test\"], \"text\"), top_k=None):\n",
        "    test_outputs.append(output)\n",
        "\n",
        "print(test_outputs[0])"
      ],
      "metadata": {
        "id": "rFI4bM6RyF2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colocamos los logits en un np array (n_samples x num_classes)\n",
        "test_logits = []\n",
        "\n",
        "for output in test_outputs:\n",
        "    logits = [0] * len(label_names)\n",
        "    for item in output:\n",
        "        label_ = item[\"label\"][:3].lower()\n",
        "        id_ = label2id[label_]\n",
        "        logits[id_] = item[\"score\"]\n",
        "    logits_arr = np.array(logits)\n",
        "    test_logits.append(logits_arr)\n",
        "\n",
        "test_logits = np.vstack(test_logits)"
      ],
      "metadata": {
        "id": "GtaVruU9h_lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_logits)\n",
        "print(test_logits.shape)"
      ],
      "metadata": {
        "id": "qec7QWdM26TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = dataset[\"test\"][\"label\"]\n",
        "\n",
        "print(test_labels[:5])\n",
        "print(len(test_labels))"
      ],
      "metadata": {
        "id": "H4mCdP8mf1uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ioop8ghgjB8"
      },
      "outputs": [],
      "source": [
        "compute_metrics(test_logits, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzunBkamgjB8"
      },
      "source": [
        "## Análisis de errores\n",
        "\n",
        "Suele ser útil hacer un análisis de errores de los modelos para detectar oportunidades de mejora, así como también errores en los datos.\n",
        "\n",
        "En este caso, vamos a inspeccionar los falsos positivos y negativos _más groseros_ del primer modelo i.e. los ejemplos donde la pérdida es más alta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA_zsvxPgjB8"
      },
      "outputs": [],
      "source": [
        "data_collator = trainer.data_collator\n",
        "\n",
        "def run_inference(examples, model):\n",
        "    \"\"\"Agrega a un batch la proba, prediccion y loss de cada ejemplo de examples\n",
        "    \"\"\"\n",
        "    examples = {k: v for k, v in examples.items() if k in ['label', 'input_ids', 'attention_mask']}\n",
        "    batch = data_collator(examples)\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "    labels = batch[\"labels\"].to(device)\n",
        "    with torch.inference_mode():\n",
        "        output = model(input_ids, attention_mask)\n",
        "        batch[\"proba\"] = torch.softmax(output.logits, dim=1)[:, 1]\n",
        "        batch[\"predicted_label\"] = torch.argmax(output.logits, axis=1)\n",
        "    # reduction=\"none\" --> loss por example\n",
        "    loss = torch.nn.functional.cross_entropy(output.logits, labels, reduction=\"none\")\n",
        "    batch[\"loss\"] = loss\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo:\n",
        "subset_example = tokenized_dataset[\"validation\"][:3]\n",
        "run_inference(subset_example, bert_model)"
      ],
      "metadata": {
        "id": "6-L8oqmZROwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.eval()\n",
        "errors_dataset = tokenized_dataset['validation'].map(\n",
        "    lambda examples: run_inference(examples, bert_model), batched=True, batch_size=32)"
      ],
      "metadata": {
        "id": "gLEkOOFcRtxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_df = errors_dataset.to_pandas()[['text', 'label', 'proba', 'predicted_label', 'loss']]"
      ],
      "metadata": {
        "id": "1TjkidunSCyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "tzJqVurpRJgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# falsos positivos\n",
        "errors_df.query(\"label == 0\").sort_values(\"loss\", ascending=False).head()"
      ],
      "metadata": {
        "id": "-FuiFgD8RJXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# falsos negativos\n",
        "errors_df.query(\"label == 1\").sort_values(\"loss\", ascending=False).head()"
      ],
      "metadata": {
        "id": "juaRLq6qRJQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 8**: ¿cómo se interpreta el falso negativo más \"fuerte\" de la tabla anterior?"
      ],
      "metadata": {
        "id": "lJ3ldgntvg9j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FH9_3Ev7vrdT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}