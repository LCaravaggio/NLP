{"cells":[{"cell_type":"markdown","metadata":{"id":"D7fWDtIomfY9"},"source":["Vamos a hacer análisis exploratorio de un dataset de tweets.\n","\n","Es muy importante siempre investigar acerca del dataset antes de explorarlo (e.g. leer repositorios o papers asociados, etc.). ¿Quién lo recopiló? ¿Con qué propósito? ¿Cómo se recopiló? Etc...\n","\n","-----------------------\n","\n","Tarea: responder donde dice **PREGUNTA**"]},{"cell_type":"markdown","metadata":{"id":"kCJu7BGDmfY-"},"source":["## Configuración del entorno"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKX0h0jumfY-"},"outputs":[],"source":["!pip install -qU datasets spacy nltk scikit-learn watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbJBo5-8mfY-"},"outputs":[],"source":["%reload_ext watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bELYImJdmfY_"},"outputs":[],"source":["%watermark -vmp datasets,spacy,nltk,sklearn,numpy,pandas,tqdm,matplotlib"]},{"cell_type":"markdown","metadata":{"id":"tz-5RJBamfY_"},"source":["## Carga de datos\n","\n","Vamos a cargar el dataset con la librería de Hugging Face `datasets` pero lo vamos a convertir a un DataFrame de pandas. Más adelante vamos a trabajar con `datasets` directamente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mruinPj_mfY_"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"tweet_eval\", \"emotion\")"]},{"cell_type":"markdown","metadata":{"id":"ZRYB45ZdmfZA"},"source":["¿Qué pinta tiene?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xl5q6n80mfZA"},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zH_uNg1omfZA"},"outputs":[],"source":["dataset[\"train\"].features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mNar9j8NmfZA"},"outputs":[],"source":["dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHcWyGewmfZA"},"outputs":[],"source":["# Convertimos los labels a strings por comodidad\n","int2label = dataset[\"train\"].features[\"label\"].int2str\n","dataset = dataset.map(lambda example: {\"label_str\": int2label(example[\"label\"])}, remove_columns=[\"label\"])\n","dataset = dataset.rename_column(\"label_str\", \"label\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6kkGffCmfZA"},"outputs":[],"source":["dataset[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K07HfjxXmfZB"},"outputs":[],"source":["# Convertimos a pandas por comodidad\n","import pandas as pd\n","pd.options.display.max_colwidth = 300\n","\n","dfs = {split: dataset[split].to_pandas() for split in dataset.keys()}\n","#del dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqdr_SyimfZB"},"outputs":[],"source":["dfs[\"train\"].sample(3, random_state=33)"]},{"cell_type":"markdown","metadata":{"id":"qKQ0nkHpmfZB"},"source":["## Análisis exploratorio\n","\n","Vamos a ir respondiendo preguntas que uno típicamente se hace al explorar un dataset como este, que luego se va a usar para alguna tarea de NLP."]},{"cell_type":"markdown","metadata":{"id":"km_lhrbCmfZB"},"source":["### 1. ¿Cómo es la distribución de clases?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dXhCzo4mfZB"},"outputs":[],"source":["for split, df in dfs.items():\n","    print(split)\n","    print(df[\"label\"].value_counts(normalize=False))\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"HR1qKj7KmfZB"},"source":["### 2. ¿Cómo es la longitud de los textos? ¿Hay documentos anormalmente largos o cortos?\n","\n","Para responder esto necesitamos una manera de contar palabras / tokens / unidades de texto. Es decir, necesitamos **tokenizar**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iosV4_PVmfZB"},"outputs":[],"source":["# A veces alcanza con contar espacios:\n","num_words = {}\n","for split, df in dfs.items():\n","    num_words[split] = df[\"text\"].str.count(\" \") + 1\n","\n","for split, df in dfs.items():\n","    print(split)\n","    print(num_words[split].describe())\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBOn-gUbmfZB"},"outputs":[],"source":["# O contar caracteres:\n","num_chars = {}\n","for split, df in dfs.items():\n","    num_chars[split] = df[\"text\"].str.len()\n","\n","for split, df in dfs.items():\n","    print(split)\n","    print(num_chars[split].describe())\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"aq0WpXrQmfZB"},"source":["Otras veces nos gustaría ser más cuidados a la hora de separar en palabras, por ejemplo, en inglés podríamos separar \"don't\" en \"do\" y \"n't\", o \"I'm\" en \"I\" y \"'m\".\n","\n","Para esto podemos usar tokenizadores informados por el lenguaje, como los de la librería `spacy`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMTLkr8zmfZB"},"outputs":[],"source":["# Usando spacy:\n","from spacy.lang.en import English\n","\n","nlp = English()\n","tokenizer = nlp.tokenizer\n","\n","example = dfs[\"train\"][\"text\"].iloc[0]\n","tokens_example = tokenizer(example)\n","\n","print(example)\n","print([token.text for token in tokens_example])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W76A_i56mfZC"},"outputs":[],"source":["# Usando tokenizer.pipe podemos correrlo para una lista de documentos:\n","num_tokens = {}\n","\n","for split, df in dfs.items():\n","    generator_ = tokenizer.pipe(df[\"text\"], batch_size=50)\n","    num_tokens[split] = pd.Series([len(doc) for doc in generator_])\n","\n","for split, df in dfs.items():\n","    print(split)\n","    print(num_tokens[split].describe())\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"S3m_lQTEmfZC"},"source":["Si estamos trabajando con tweets, donde los emojis y los hashtags son importantes, quizás sea mejor usar un tokenizador especializado en tweets:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NI4-vHC1mfZC"},"outputs":[],"source":["from nltk.tokenize import TweetTokenizer\n","\n","tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n","# strip_handles=False: elimina los @user\n","# reduce_len=True: reduce los caracteres repetidos a 3 e.g. \"aaaaaa\" -> \"aaa\"\n","\n","example = dfs[\"train\"][\"text\"].iloc[0]\n","tokens_example = tokenizer.tokenize(example)\n","\n","print(example)\n","print(tokens_example)\n"]},{"cell_type":"markdown","metadata":{"id":"iAuv2FzjmfZC"},"source":["**PREGUNTA 1**: ¿Por qué motivo querríamos usar `reduce_len=True`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v53Dr3bnmfZC"},"outputs":[],"source":["# Veamos cuáles son los ejemplos más largos y más cortos:\n","import textwrap\n","\n","tokenizer = TweetTokenizer(strip_handles=False, reduce_len=False)\n","\n","for split, df in dfs.items():\n","    df[\"num_tokens\"] = df[\"text\"].apply(lambda x: len(tokenizer.tokenize(x)))\n","\n","for split, df in dfs.items():\n","    short_texts = df.sort_values(\"num_tokens\", ascending=True).head(3)[\"text\"].values\n","    long_texts = df.sort_values(\"num_tokens\", ascending=False).head(3)[\"text\"].values\n","    print(\"##\", split)\n","    print(\"# Short:\")\n","    for text in short_texts:\n","        print(\"- \\t\", textwrap.fill(text, 120))\n","    print(\"# Long:\")\n","    for text in long_texts:\n","        print(\"- \\t\", textwrap.fill(text, 120))\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"od-kx7FumfZC"},"outputs":[],"source":["dfs[\"train\"].sort_values(\"num_tokens\", ascending=False).head(3)"]},{"cell_type":"markdown","metadata":{"id":"8VYkEwzqmfZC"},"source":["### 3. ¿Hay caracteres raros o inesperados?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sxZV4ZKSmfZC"},"outputs":[],"source":["# Por ejemplo, caracteres html como &amp; se pueden convertir a su forma original:\n","\n","mask = dfs[\"train\"][\"text\"].str.contains(\"&amp;\")\n","print(dfs[\"train\"][mask].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QIcXmwaomfZC"},"outputs":[],"source":["import html\n","\n","example = dfs[\"train\"][mask][\"text\"].iloc[0]\n","\n","print(example)\n","print(html.unescape(example))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZM4We9ymfZD"},"outputs":[],"source":["# O caracteres whitespace no identificados como tales:\n","mask = dfs[\"train\"][\"text\"].str.contains(\"\\\\\\\\n\") # tienen \"\\n\" literales\n","print(dfs[\"train\"][mask].shape)\n","\n","example = dfs[\"train\"][mask][\"text\"].iloc[1]\n","print(example)\n","print(example.replace(\"\\\\n\", \"\\n\"))"]},{"cell_type":"markdown","metadata":{"id":"rF4FNCx-mfZD"},"source":["### 4. ¿Hay documentos duplicados?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-R58eTCqmfZD"},"outputs":[],"source":["# buscar duplicados:\n","for split, df in dfs.items():\n","    print(split)\n","    print(df[\"text\"].duplicated().sum())\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcSS-wU9mfZD"},"outputs":[],"source":["mask = dfs[\"train\"][\"text\"].duplicated(keep=False)\n","dfs[\"train\"][mask].sort_values(\"text\")"]},{"cell_type":"markdown","metadata":{"id":"53_cO5_dmfZG"},"source":["**PREGUNTA 2**: ¿por qué motivo podrían estar duplicados estos tweets?"]},{"cell_type":"markdown","metadata":{"id":"mGwFzumQmfZG"},"source":["### 4. ¿Hay documentos raros?\n","\n","\"raro\" es un término subjetivo, pero podríamos pensar en documentos que son muy cortos, muy largos, con contenido inesperado, etc. O si hay una variable respuesta, documentos con errores de anotación.\n","\n","Una manera sencilla y bastante general de detectar documentos raros en tareas de clasificación es corriendo un modelo sencillo y viendo los documentos que más pérdida generan, i.e. los que el modelo no puede clasificar bien.\n","\n","Vamos a usar una regresión logística para esto. Más adelante vamos a analizar esto con much más detalle!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FpFyL08_mfZG"},"outputs":[],"source":["print(dfs[\"train\"].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_95lOutmfZG"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","\n","vectorizer = CountVectorizer(max_features=150)\n","clf = LogisticRegression(max_iter=1000)\n","class2int = {label: i for i, label in enumerate(dfs[\"train\"][\"label\"].unique())}\n","int2class = {i: label for label, i in class2int.items()}\n","\n","X_train = vectorizer.fit_transform(dfs[\"train\"][\"text\"])\n","y_train = dfs[\"train\"][\"label\"].map(class2int)\n","\n","clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"137dkUaQmfZG"},"source":["**PREGUNTA 3** ¿qué representa cada fila y cada columna de `X_train`?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLBgvyeYmfZG"},"outputs":[],"source":["# Ejemplos con mayor pérdida (medida como -log(probabilidad_clase_correcta)):\n","import numpy as np\n","\n","y_pred_proba = clf.predict_proba(X_train)\n","loss = -np.log(y_pred_proba[range(len(y_train)), y_train])\n","y_pred = clf.predict(X_train)\n","\n","dfs[\"train\"][\"pred\"] = [int2class[i] for i in y_pred]\n","dfs[\"train\"][\"loss\"] = loss\n","\n","dfs[\"train\"].sort_values(\"loss\", ascending=False).head(8)"]},{"cell_type":"markdown","metadata":{"id":"TOUH8RP2mfZH"},"source":["### 5. ¿Cómo es la distribución de palabras?\n","\n","Para esto es fundamental (1) preprocesar el texto y (2) tokenizarlo. La manera en la que hagamos esto depende directamente del análisis que queramos hacer.\n","\n","Cosas a definir: ¿queremos diferenciar mayúsculas y minúsculas? ¿Queremos eliminar _stopwords_? ¿Queremos eliminar puntuación? ¿Queremos lematizar o hacer stemming?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hmLiaVdqmfZH"},"outputs":[],"source":["# Empecemos usando el tokenizador de tweets de nltk.\n","# Para cada documento queremos un vector del tamaño del vocabulario con la\n","# cantidad de veces que aparece cada palabra.\n","from nltk.tokenize import TweetTokenizer\n","\n","tokenizer = TweetTokenizer(strip_handles=False, reduce_len=True)\n","vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize)\n","\n","X_train = vectorizer.fit_transform(dfs[\"train\"][\"text\"])\n","df_vocab_train = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","print(df_vocab_train.shape)\n","df_vocab_train.head()"]},{"cell_type":"markdown","metadata":{"id":"XQlN_hWQmfZH"},"source":["**PREGUNTA 4**: ¿cuántas palabras hay en el vocabulario?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oS7xrOnDmfZH"},"outputs":[],"source":["# palabras más y menos frecuentes:\n","vocab_freq = df_vocab_train.sum().sort_values(ascending=False)\n","\n","print(vocab_freq.head(10))\n","print(vocab_freq.tail(10))"]},{"cell_type":"code","source":["# Eliminando stopwords (palabras frecuentes con poca carga semántica),\n","# mayúsculas, mentions y punctuación (pero sin eliminar hashtags!):\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"Hq7tlb8UnT4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ywqko3ulmfZH"},"outputs":[],"source":["from nltk.corpus import stopwords\n","import string\n","\n","stop_words = stopwords.words(\"english\")\n","print(stop_words[:5])\n","\n","print(string.punctuation)\n","\n","def preprocess_text(text: str) -> str:\n","    \"\"\"Limpia antes de tokenizar.\"\"\"\n","    text = text.lower()\n","    text = text.replace(\"\\\\\\\\n\", \" \")\n","    text = html.unescape(text)\n","    exclude = {'#', \"'\", \"@\"}\n","    text = ''.join(char for char in text if char not in string.punctuation or char in exclude)\n","    return text\n","\n","example = \"I'm a #tweet with @user and a link: https://t.co/1234\"\n","print(example)\n","print(preprocess_text(example))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXX4cxJYmfZH"},"outputs":[],"source":["tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n","vectorizer = CountVectorizer(\n","    tokenizer=tokenizer.tokenize, stop_words=stop_words, preprocessor=preprocess_text\n",")\n","\n","X_train = vectorizer.fit_transform(dfs[\"train\"][\"text\"])\n","df_vocab_train = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","print(df_vocab_train.shape)\n","df_vocab_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTSPrSC6mfZH"},"outputs":[],"source":["vocab_freq = df_vocab_train.sum().sort_values(ascending=False)\n","print(vocab_freq.head(20))"]},{"cell_type":"markdown","metadata":{"id":"fmfrKGBCmfZH"},"source":["**PREGUNTA 5**: ¿Cómo tratarían las tildes en español para este análisis exploratorio?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPVH7hg4mfZH"},"outputs":[],"source":["# Palabras más populares de cada clase:\n","df_data = df_vocab_train.copy()\n","df_data[\"CLASE\"] = dfs[\"train\"][\"label\"]\n","# pusimos CLASE para que no se confunda con una palabra del vocabulario\n","df_data = df_data.groupby(\"CLASE\").sum().T\n","df_data.sort_values(\"joy\", ascending=False).head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kGDHdnaBmfZH"},"outputs":[],"source":["# visualizamos las palabras más populares de cada clase:\n","import matplotlib.pyplot as plt\n","\n","labels = list(df_data.columns)\n","n = 20\n","\n","for label in labels:\n","    df_plot = df_data[label].sort_values(ascending=False).head(n)\n","    df_plot.plot(kind=\"bar\", title=label, figsize=(6, 2))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wwhk141CmfZI"},"outputs":[],"source":["# Ahora, % de documentos en los que aparece cada palabra, por clase:\n","df_data = df_vocab_train.astype(bool).astype(int).copy()\n","df_data[\"CLASE\"] = dfs[\"train\"][\"label\"]\n","df_data = df_data.groupby(\"CLASE\").mean().T\n","\n","for label in labels:\n","    df_plot = df_data[label].sort_values(ascending=False).head(n)\n","    df_plot.plot(kind=\"bar\", title=label, figsize=(6, 2))\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocNvs1ZymfZI"},"outputs":[],"source":["# Esto se puede seguir mejorando...\n","# Más adelante vamos a ver cómo encontrar las palabras más _discriminativas_ entre clases"]},{"cell_type":"markdown","metadata":{"id":"FX8DQsjKmfZI"},"source":["**PREGUNTA 6**: ¿Cuándo puede ser distinto analizar la frecuencia y el % de documentos en los que aparece una palabra?\n","\n","**PREGUNTA 7**: ¿Cómo mejorar visualmente estos gráficos?"]},{"cell_type":"markdown","source":["### 6. Stemming, lematización, y regex: ejemplos de uso\n","\n","- **Stemming**: Reducción de palabras a su raíz base.\n","- **Lematización**: Transformación de palabras a su forma canónica.\n","- **Regex**: expresiones regulares para identificar patrones en el texto."],"metadata":{"id":"eEMvZodjdoLR"}},{"cell_type":"code","source":["from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","# Descargar recursos necesarios\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"metadata":{"id":"ufRjTJkNnY3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","nlp = English()\n","tokenizer = nlp.tokenizer\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","examples = [\n","    \"Speaking words of wisdom, let it be.\",\n","    \"His palms are sweaty, knees weak, arms are heavy.\",\n","    \"The mice were running through the house, searching for food.\",\n","]\n","\n","for example in examples:\n","    tokens = [token.text for token in tokenizer(example)]\n","    stemmed = [stemmer.stem(word) for word in tokens]\n","    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    print(\"Stemming\", stemmed)\n","    print(\"Lemmatized\", lemmatized)\n","    print()\n","\n","# TODO regex_words = re.findall(r'\\b\\w+mente\\b', texto)"],"metadata":{"id":"nh1X-XZ-gJ7n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**PREGUNTA 8** ¿En qué casos serviría usar stemming / lematización para un problema de clasificación? Ante la duda, ¿cómo puedo definir si sirve o no?"],"metadata":{"id":"lXl0BXIjwDOP"}},{"cell_type":"markdown","source":["Algunas expresiones regulares comunes:\n","\n","* \".\" : Matchea cualquier caracter excepto '\\n'\n","* \"^\" y \"$\": Matchean el comienzo y el final de un string\n","* \"[]\": Matchea el set de caracteres que se encuentren dentro de los corchetes (r\"l[ao]s\" machea \"las\" y \"los\")\n","* \\d: Matchea digitos; equivalente a [0-9].\n","* \\D: Matchea caracteres que NO sean digitos; equivalente a [^0-9].\n","* \\s: Matchea espacios en blanco; equivalente a [ \\t\\n\\r\\f\\v].\n","* \\S: Matchea espacios que NO esten en blanco; equivalente a [^ \\t\\n\\r\\f\\v].\n","* \\w: Matchea caracteres alfanuméricos; equivalente a [a-zA-Z0-9_].\n","* \\W: Matchea caracteres que NO sean alfanuméricos; equivalente a[^a-zA-Z0-9_].\n","* a|b: Matchea \"a\" o \"b\"\n","\n","Para repeticiones de patrones:\n","* \"+\": Matchea 1 o mas ocurrencias\n","* \"*\": Matchea 0 o mas ocurrencias\n","* \"?\": Matchea 0 o 1 ocurrencia\n","* \"{n, m}\": Matchea entre n y m ocurrencias\n","* \"\\\\\": Permite matchear caracteres especiales\n","\n","Para más info ver: https://docs.python.org/3.1/library/re.html#re-syntax"],"metadata":{"id":"0dnnktupx70t"}},{"cell_type":"code","source":["# Algunos ejemplos:\n","import re\n","\n","texto = \"las cámaras y los libros sobre laos de luis alberto\"\n","patron = r\"l[ao]s\"\n","resultados = re.findall(patron, texto)\n","print(texto)\n","print(patron)\n","print(resultados)\n","print()\n","\n","texto = \"Mi número es 12345 y tu número es 67890.\"\n","patron = r\"\\d+\"\n","resultados = re.findall(patron, texto)\n","print(texto)\n","print(patron)\n","print(resultados)\n","print()\n","\n","texto = \"123 un pasito palante maría, 123 un pasito patrás\"\n","patron = r\"\\D+\"\n","resultados = re.findall(patron, texto)\n","print(texto)\n","print(patron)\n","print(resultados)\n","print()\n","\n","# Buscamos precios compuestos por 2 o 3 dígitos, opcionalmente seguidos de un espacio y la palabra \"USD\" o \"usd\"\n","texto = \"El precio es 100USD, o tal vez 50 USD o 250usd.\"\n","patron = r\"(\\d{2,3})\\s?[USD|usd]\"\n","resultados = re.findall(patron, texto)\n","print(texto)\n","print(patron)\n","print(resultados)\n","print()"],"metadata":{"id":"4n4HSS-dwCWz"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"py311","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}