{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/03_Embeddings_PMI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATsVsal-3_IY"
      },
      "source": [
        "Medición de asociación semántica con word embeddings estáticos y con PMI.\n",
        "\n",
        "---\n",
        "\n",
        "TAREA: responder donde dice **PREGUNTA**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim watermark\n",
        "# NOTE requiere Restart Session luego de instalar!"
      ],
      "metadata": {
        "id": "7oVIWwPU4zjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jb7jxqdq3_Ih"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIeSonB3_Ik"
      },
      "outputs": [],
      "source": [
        "%watermark -udvp nltk,numpy,pandas,sklearn,gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEWwVO2T3_Im"
      },
      "source": [
        "## Embeddings pre-entrenados vs. _from scratch_\n",
        "\n",
        "Vamos a empezar descargando un set de embeddings preentrenados de gensim (alguno chico)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrSb67zO3_In"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "api.info().keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzdMGa6H3_Io"
      },
      "outputs": [],
      "source": [
        "api.info()[\"models\"].keys() # modelos disponibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtSLXN3m3_Ip"
      },
      "outputs": [],
      "source": [
        "glove_wikig = api.load('glove-wiki-gigaword-50') # GloVe Wikipedia+Gigaword dim=50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMcCAhnV3_Ip"
      },
      "outputs": [],
      "source": [
        "len(glove_wikig.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_wikig.index_to_key[:10]"
      ],
      "metadata": {
        "id": "9x9wDaVh-AAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_wikig[\"hello\"]"
      ],
      "metadata": {
        "id": "JgRWbCbX8yDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"palabra_muy_rara\" in glove_wikig, \"the\" in glove_wikig"
      ],
      "metadata": {
        "id": "5AbDET9w-DL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np.linalg.norm(glove_wikig[\"hello\"])"
      ],
      "metadata": {
        "id": "LxLatKNn94MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 1** ¿vienen normalizados los embeddings de glove_wikig? ¿qué quiere decir normalizado?"
      ],
      "metadata": {
        "id": "JpKmKcMGAAUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a entrenar embeddings en discursos presidenciales de EEUU (son corpora chiquitos, solo es ilustrativo)"
      ],
      "metadata": {
        "id": "yjD8So4M69Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-d1bvC73_Iq"
      },
      "outputs": [],
      "source": [
        "# descargamos discursos:\n",
        "import nltk\n",
        "nltk.download('inaugural')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUgEef-P3_Ir"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import inaugural\n",
        "\n",
        "print(inaugural.fileids()[-8:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whnnwUqK3_Ir"
      },
      "outputs": [],
      "source": [
        "# usamos bush hijo y obama:\n",
        "bush_corpus = inaugural.raw('2001-Bush.txt') + \"\\n\" + inaugural.raw('2005-Bush.txt')\n",
        "obama_corpus = inaugural.raw('2009-Obama.txt') + \"\\n\" + inaugural.raw('2013-Obama.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMpmAyQ73_Ir"
      },
      "source": [
        "Atención al **preprocesamiento** que decidamos usar!\n",
        "\n",
        "Si se trata de una tarea supervisada, podemos probar distintas opciones y quedarnos con la mejor."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "06LqWcBh-ZnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoTf69i_3_Is"
      },
      "outputs": [],
      "source": [
        "# Hacemos una lista de tokens para cada oracion. Las ventanas de coocurrencia\n",
        "# se van a formar dentro de los limites de las oraciones.\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from string import punctuation\n",
        "\n",
        "# Acá solamente eliminamos la puntuación y convertimos a minusc. a modo de ejemplo.\n",
        "bush_sentences = []\n",
        "for sentence in sent_tokenize(bush_corpus):\n",
        "    words_ = [word.lower() for word in word_tokenize(sentence) if word not in punctuation]\n",
        "    bush_sentences.append(words_)\n",
        "\n",
        "obama_sentences = []\n",
        "for sentence in sent_tokenize(obama_corpus):\n",
        "    words_ = [word.lower() for word in word_tokenize(sentence) if word not in punctuation]\n",
        "    obama_sentences.append(words_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 2** ¿Qué quiere decir \"ventana de coocurrencia\" en el contexto de estos embeddings?"
      ],
      "metadata": {
        "id": "WA_eyjjbATJZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdtmsRrE3_Is"
      },
      "outputs": [],
      "source": [
        "print(bush_sentences[-1])\n",
        "print(obama_sentences[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7n-rXFs3_Is"
      },
      "source": [
        "Atención a los **hiperparámetros** que usamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3fEYYH23_It"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "params = {\n",
        "    \"vector_size\": 100,\n",
        "    \"alpha\": 0.025,\n",
        "    \"window\": 10, # igual que GloVe\n",
        "    \"min_count\": 5,\n",
        "    \"max_vocab_size\": None,\n",
        "    \"sg\": 1, # 0: CBOW, 1: Skip-gram\n",
        "    \"negative\": 5,\n",
        "    \"epochs\": 2,\n",
        "    \"seed\": 33,\n",
        "    \"workers\": 2,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 3** ¿qué signfican los parámetros: min_count, negative, epochs?"
      ],
      "metadata": {
        "id": "HH35StAWAyOX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-AcrWqx3_Iu"
      },
      "outputs": [],
      "source": [
        "w2v_bush = Word2Vec(bush_sentences, **params)\n",
        "w2v_obama = Word2Vec(obama_sentences, **params)\n",
        "# Para guardar:\n",
        "# w2v_obama.save(\"obama_w2v.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMrdK6_G3_Iu"
      },
      "outputs": [],
      "source": [
        "print(len(w2v_bush.wv.index_to_key), len(w2v_obama.wv.index_to_key))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoIRQA5w3_Iv"
      },
      "outputs": [],
      "source": [
        "# algunas palabras interesantes:\n",
        "words = [\n",
        "    \"america\", \"freedom\", \"hope\", \"god\", \"american\", \"citizens\", \"democracy\",\n",
        "    \"liberty\", \"freedoms\", \"liberties\", \"rights\", \"justice\", \"equality\",\n",
        "    \"opportunity\", \"nation\", \"security\", \"peace\", \"war\",\n",
        "]\n",
        "\n",
        "for word in words:\n",
        "   if word in w2v_bush.wv and word in w2v_obama.wv:\n",
        "       print(f\"{word} is in both vocabularies\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYONn_7Q3_Iv"
      },
      "outputs": [],
      "source": [
        "# Medición de similitud con gensim:\n",
        "print(w2v_bush.wv.n_similarity([\"nation\"], [\"god\"]))\n",
        "print(w2v_obama.wv.n_similarity([\"nation\"], [\"god\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 4** ¿es correcto comparar los dos valores de similitud anteriores entre sí?"
      ],
      "metadata": {
        "id": "50HGwz8SBW4z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfL3x4nC3_Iw"
      },
      "outputs": [],
      "source": [
        "# a mano:\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def cossim(v1: np.ndarray, v2: np.ndarray) -> float:\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def get_vector(embeddings: KeyedVectors, word: str) -> np.ndarray:\n",
        "    return embeddings[word]\n",
        "\n",
        "def similarity(embeddings: KeyedVectors, word1: str, word2: str) -> float:\n",
        "    return cossim(get_vector(embeddings, word1), get_vector(embeddings, word2))\n",
        "\n",
        "print(similarity(w2v_bush.wv, \"nation\", \"god\"))\n",
        "print(similarity(w2v_bush.wv, \"god\", \"god\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iO3TchkO3_Iw"
      },
      "outputs": [],
      "source": [
        "# las palabras más similares (con gensim):\n",
        "print(w2v_bush.wv.most_similar(\"god\"))\n",
        "print(w2v_obama.wv.most_similar(\"god\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6KgrY893_Iw"
      },
      "outputs": [],
      "source": [
        "# a mano:\n",
        "def most_similar(\n",
        "        embeddings: KeyedVectors, word: str, topn: int = 10, remove_words: list = []\n",
        ") -> list:\n",
        "    word_vector = get_vector(embeddings, word)\n",
        "    words = embeddings.index_to_key\n",
        "    sims = []\n",
        "    for w in words:\n",
        "        if w not in remove_words + [word]:\n",
        "            sims.append((w, cossim(word_vector, get_vector(embeddings, w))))\n",
        "    return sorted(sims, key=lambda x: x[1], reverse=True)[:topn]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2DXejP93_Ix"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "most_similar(w2v_bush.wv, \"god\", remove_words=stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr5Mk0WB3_Ix"
      },
      "outputs": [],
      "source": [
        "most_similar(w2v_obama.wv, \"god\", remove_words=stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_EijJay3_Ix"
      },
      "outputs": [],
      "source": [
        "# en los embeddings preentrenados:\n",
        "glove_wikig.most_similar(\"god\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyajvkeo3_Iy"
      },
      "outputs": [],
      "source": [
        "glove_wikig.most_similar(\"argentina\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2LE28a33_Iy"
      },
      "outputs": [],
      "source": [
        "# podemos promediar y luego calcular similitud:\n",
        "glove_wikig.n_similarity([\"fun\", \"funny\"], [\"argentina\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwSL36im3_Iy"
      },
      "outputs": [],
      "source": [
        "# a mano:\n",
        "def similarity_multiple(model, words1: list, words2: list) -> float:\n",
        "    # promedios:\n",
        "    v1 = np.mean([get_vector(model, w) for w in words1], axis=0)\n",
        "    v2 = np.mean([get_vector(model, w) for w in words2], axis=0)\n",
        "    # similitud:\n",
        "    return cossim(v1, v2)\n",
        "\n",
        "similarity_multiple(glove_wikig, [\"fun\", \"funny\"], [\"argentina\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjPFC4xk3_Iz"
      },
      "outputs": [],
      "source": [
        "paises = [\"argentina\", \"brazil\", \"chile\", \"mexico\", \"peru\", \"uruguay\", \"venezuela\"]\n",
        "target_words = [\"fun\", \"funny\", \"happy\", \"joy\", \"happiness\", \"cheerful\", \"party\"]\n",
        "for pais in paises:\n",
        "    sim_ = similarity_multiple(glove_wikig, [pais], target_words)\n",
        "    print(f\"{pais}: {sim_:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDlB8UbX3_Iz"
      },
      "outputs": [],
      "source": [
        "# analogias\n",
        "def analogias(\n",
        "        embeddings: KeyedVectors, x1: str, x2: str, y1: str,\n",
        "        topn: int = 10, remove_words: list = []\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    \"x1\" es a \"x2\" lo que \"y1\" es a ...\n",
        "    \"\"\"\n",
        "    v_x1 = get_vector(embeddings, x1)\n",
        "    v_x2 = get_vector(embeddings, x2)\n",
        "    v_y1 = get_vector(embeddings, y1)\n",
        "    v_y2 = v_y1 + (v_x2 - v_x1)\n",
        "    # # version gensim (normaliza antes de sumar):\n",
        "    # v_x1_normalized = v_x1 / np.linalg.norm(v_x1)\n",
        "    # v_x2_normalized = v_x2 / np.linalg.norm(v_x2)\n",
        "    # v_y1_normalized = v_y1 / np.linalg.norm(v_y1)\n",
        "    # v_y2 = np.mean([v_y1_normalized, v_x2_normalized, -v_x1_normalized], axis=0)\n",
        "    words = embeddings.index_to_key\n",
        "    sims = []\n",
        "    for w in words:\n",
        "        if w not in [x1, x2, y1] + remove_words:\n",
        "            sims.append((w, cossim(v_y2, get_vector(embeddings, w))))\n",
        "    return sorted(sims, key=lambda x: x[1], reverse=True)[:topn]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analogias(glove_wikig, \"argentina\", \"tango\", \"brazil\", remove_words=stopwords)"
      ],
      "metadata": {
        "id": "6e7cVMptFdKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analogias(glove_wikig, \"cumbia\", \"argentina\", \"samba\", remove_words=stopwords)"
      ],
      "metadata": {
        "id": "z67sz7bhE4YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dSACASS3_Iz"
      },
      "outputs": [],
      "source": [
        "# con gensim:\n",
        "glove_wikig.most_similar(positive=[\"samba\", \"argentina\"], negative=[\"cumbia\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jyvkjjE3_I0"
      },
      "source": [
        "## Evaluación de embeddings\n",
        "\n",
        "¿Cómo medimos la calidad de los embeddings? Extrínsecamente, o **intrínsecamente**.\n",
        "\n",
        "Usamos código de [word-embeddings-benchmarks](https://github.com/kudkudak/word-embeddings-benchmarks/tree/master) para evaluar en SimLex999."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fetch_simlex999() -> pd.DataFrame:\n",
        "    df = pd.read_csv('https://www.dropbox.com/s/0jpa1x8vpmk3ych/EN-SIM999.txt?dl=1', sep=\"\\t\")\n",
        "    return df[['word1', 'word2', 'POS', 'SimLex999']]\n",
        "\n",
        "df_simlex = fetch_simlex999()"
      ],
      "metadata": {
        "id": "xjUbJ7PpI7W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_simlex)"
      ],
      "metadata": {
        "id": "p182XqExLj4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_simlex.sample(6)"
      ],
      "metadata": {
        "id": "hvyjrTQlOwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_similarities(embeddings: KeyedVectors, df: pd.DataFrame):\n",
        "    words1 = df[\"word1\"].tolist()\n",
        "    words2 = df[\"word2\"].tolist()\n",
        "    missing_words = 0\n",
        "    for word in words1 + words2:\n",
        "        if word not in embeddings:\n",
        "            missing_words += 1\n",
        "    if missing_words > 0:\n",
        "        print(f\"Missing {missing_words} words. Will replace them with mean vector\")\n",
        "    mean_vector = np.mean(embeddings.vectors, axis=0, keepdims=True)\n",
        "    A = np.vstack([get_vector(embeddings, w) if w in embeddings else mean_vector for w in words1])\n",
        "    B = np.vstack([get_vector(embeddings, w) if w in embeddings else mean_vector for w in words2])\n",
        "    scores = np.array([cossim(v1, v2) for v1, v2 in zip(A, B)])\n",
        "    return scores"
      ],
      "metadata": {
        "id": "Jr7oP20NOzpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_simlex[\"glove_scores\"] = compute_similarities(glove_wikig, df_simlex)\n",
        "df_simlex.head(2)"
      ],
      "metadata": {
        "id": "m5h1QswfRlkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity(glove_wikig, \"old\", \"new\")"
      ],
      "metadata": {
        "id": "Ur6ehrKwR2BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "\n",
        "scipy.stats.spearmanr(df_simlex[\"SimLex999\"], df_simlex[\"glove_scores\"]).correlation"
      ],
      "metadata": {
        "id": "wp1uD2IWRjfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 5** ¿Qué representa el valor inmediatamente anterior? ¿Cómo se interpreta? ¿Para qué sirve?"
      ],
      "metadata": {
        "id": "eD58yX0NCKr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_obama_scores = compute_similarities(w2v_obama.wv, df_simlex)\n",
        "scipy.stats.spearmanr(df_simlex[\"SimLex999\"], w2v_obama_scores).correlation"
      ],
      "metadata": {
        "id": "x3prp2i7SXoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhn8HRB13_I5"
      },
      "source": [
        "**PREGUNTA 6** ¿Tiene sentido evaluar los embeddings entrenados en los discursos presidenciales en estos benchmarks?\n",
        "\n",
        "**PREGUNTA 7** ¿Por qué usaríamos nuestros propios embeddings en lugar de usar preentrenados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu-90o_k3_I5"
      },
      "source": [
        "## PMI\n",
        "\n",
        "Vamos a usar GloVe para computar las coocurrencias porque es  mucho más rápido que hacerlo con una función de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRmn3ySl3_I5"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/stanfordnlp/GloVe.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!cd GloVe && make"
      ],
      "metadata": {
        "id": "wHgCoi2NSqVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos a usar todos los discursos para tener mas datos:\n",
        "files = inaugural.fileids()\n",
        "presidents_corpus = \"\"\n",
        "for f in files:\n",
        "    presidents_corpus = presidents_corpus + \"\\n\" + inaugural.raw(f)\n",
        "presidents_sentences = []\n",
        "for sentence in sent_tokenize(presidents_corpus):\n",
        "    words_ = [word.lower() for word in word_tokenize(sentence) if word not in punctuation]\n",
        "    presidents_sentences.append(words_)"
      ],
      "metadata": {
        "id": "94ruL2Y4k7wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# guardamos corpus con una oracion por linea\n",
        "with open('sentences.txt', 'w') as f:\n",
        "    for sentence in presidents_sentences:\n",
        "        f.write(' '.join(sentence) + '\\n')"
      ],
      "metadata": {
        "id": "fWsVmbTrTMdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generamos el vocab\n",
        "!GloVe/build/vocab_count -min-count 5 -verbose 0 < sentences.txt > vocab.txt"
      ],
      "metadata": {
        "id": "5YaOqyq4Sw4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -2 vocab.txt"
      ],
      "metadata": {
        "id": "hUsDyWq-UQNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# leemos el vocab como dict:\n",
        "str2count = {}\n",
        "with open(\"vocab.txt\", \"r\") as f:\n",
        "    for line in f:\n",
        "        word, count = line.split()\n",
        "        str2count[word] = int(count)\n",
        "\n",
        "str2idx = dict(zip(str2count.keys(), range(len(str2count))))"
      ],
      "metadata": {
        "id": "NpRXvCoaUT-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str2count[\"american\"], str2idx[\"the\"]"
      ],
      "metadata": {
        "id": "c4bzH2M2Ursw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJz0H7IS3_I6"
      },
      "outputs": [],
      "source": [
        "# generamos un .bin con las coocurrencias\n",
        "!GloVe/build/cooccur -vocab-file vocab.txt -verbose 0 -window-size 10 -distance-weighting 0 < sentences.txt > coocs.bin\n",
        "# ventanas: +-10 sin ponderar por distancia al centro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cuestiones tecnicas no importantes para leer coocs como sparse matrix\n",
        "import array\n",
        "from ctypes import Structure, c_int, c_double, sizeof\n",
        "from os import path\n",
        "from scipy import sparse\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CREC(Structure):\n",
        "    \"\"\"c++ class to read triples (idx, idx, cooc) from GloVe binary file\n",
        "    \"\"\"\n",
        "    _fields_ = [('idx1', c_int),\n",
        "                ('idx2', c_int),\n",
        "                ('value', c_double)]\n",
        "\n",
        "\n",
        "class IncrementalCOOMatrix:\n",
        "    \"\"\"class to create scipy.sparse.coo_matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, dtype=np.double):\n",
        "        self.dtype = dtype\n",
        "        self.shape = shape\n",
        "        self.rows = array.array('i')\n",
        "        self.cols = array.array('i')\n",
        "        self.data = array.array('d')\n",
        "\n",
        "    def append(self, i, j, v):\n",
        "        m, n = self.shape\n",
        "        if (i >= m or j >= n):\n",
        "            raise Exception('Index out of bounds')\n",
        "        self.rows.append(i)\n",
        "        self.cols.append(j)\n",
        "        self.data.append(v)\n",
        "\n",
        "    def tocoo(self):\n",
        "        rows = np.frombuffer(self.rows, dtype=np.int32)\n",
        "        cols = np.frombuffer(self.cols, dtype=np.int32)\n",
        "        data = np.frombuffer(self.data, dtype=self.dtype)\n",
        "        return sparse.coo_matrix((data, (rows, cols)), shape=self.shape)\n",
        "\n",
        "\n",
        "def build_cooc_matrix(str2idx, cooc_file):\n",
        "    \"\"\"\n",
        "    Build full coocurrence matrix from cooc. data in binary glove file and glove vocab text file\n",
        "    Row and column indices are numeric indices from vocab_file\n",
        "    There must be (i,j) for every (j,i) such that C[i,j]=C[j,i]\n",
        "    \"\"\"\n",
        "    vocab_size = len(str2idx)  # vocab size (largest word index)\n",
        "    size_crec = sizeof(CREC)  # crec: structura de coocucrrencia en Glove\n",
        "    C = IncrementalCOOMatrix((vocab_size, vocab_size))\n",
        "    K = path.getsize(cooc_file) / size_crec # total de coocurrencias\n",
        "    pbar = tqdm(total=K)\n",
        "    # open bin file and store coocs in C\n",
        "    with open(cooc_file, 'rb') as f:\n",
        "        # read and overwrite into cr while there is data\n",
        "        cr = CREC()\n",
        "        while (f.readinto(cr) == size_crec):\n",
        "            C.append(cr.idx1-1, cr.idx2-1, cr.value) # porque glove empieza en 1\n",
        "            pbar.update(1)\n",
        "    pbar.close()\n",
        "    return C.tocoo().tocsr()"
      ],
      "metadata": {
        "id": "EA21SH4LVBio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 8** ¿qué quiere decir que la matriz de coocurrencias es rala / sparse?"
      ],
      "metadata": {
        "id": "aHWfohN3DW18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora si!\n",
        "cooc_matrix = build_cooc_matrix(str2idx, \"coocs.bin\")\n",
        "\n",
        "def get_cooc(w1: str, w2: str, str2idx: dict = str2idx, cooc_matrix=cooc_matrix) -> float:\n",
        "    if w1 not in str2idx:\n",
        "        print(f\"{w1} not in vocab\")\n",
        "        return 0.\n",
        "    if w2 not in str2idx:\n",
        "        print(f\"{w2} not in vocab\")\n",
        "        return 0.\n",
        "    idx1 = str2idx[w1]\n",
        "    idx2 = str2idx[w2]\n",
        "    return cooc_matrix[idx1, idx2]"
      ],
      "metadata": {
        "id": "l5iURRi3afrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pares = [\n",
        "    (\"the\", \"the\"), (\"of\", \"the\"), (\"the\", \"of\"), (\"united\", \"states\"), (\"americans\", \"war\")\n",
        "]\n",
        "for par in pares:\n",
        "    print(par, get_cooc(*par))"
      ],
      "metadata": {
        "id": "luTvb45ClsNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 9** ¿cómo se interpreta el valor `('the', 'the') 26396.0`?"
      ],
      "metadata": {
        "id": "GTRd_CYyDFRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coocs. con una palabra\n",
        "idx = str2idx[\"god\"]\n",
        "coocs = cooc_matrix[idx, :].toarray()[0]\n",
        "coocs_dict = dict(zip(str2idx.keys(), coocs))\n",
        "sorted(coocs_dict.items(), key=lambda x: x[1], reverse=True)[:20]"
      ],
      "metadata": {
        "id": "s5qNuf-ScBPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pmi(words_w: list, words_c: list, stridx=str2idx, cooc_matrix=cooc_matrix):\n",
        "    \"\"\"Un PMI por cada palabra en W con respecto a las palabras en C\"\"\"\n",
        "    idx_w = [str2idx[w] for w in words_w if w in stridx]\n",
        "    idx_c = [str2idx[w] for w in words_c if w in str2idx]\n",
        "    total_count = cooc_matrix.sum()\n",
        "    count_c = cooc_matrix[idx_c, :].sum()\n",
        "    counts_w = cooc_matrix.sum(axis=0)[:,idx_w]\n",
        "    counts_w_c = cooc_matrix[idx_c,:][:,idx_w].sum(axis=0)\n",
        "    return np.array(pmi(counts_w_c, counts_w, count_c, total_count)).flatten()\n",
        "\n",
        "\n",
        "def pmi(counts_wc, counts_w, count_c, count_tot):\n",
        "    \"\"\"\n",
        "    PMI for given word counts of lists of words W and C. It works vectorized accross W\n",
        "    if needed.\n",
        "    Param:\n",
        "        - counts_wc: co-ocurrence array between C and W\n",
        "        - counts_w: co-ocurrence array for W words\n",
        "        - count_c: co-ocurrence count C\n",
        "        - count_tot: total co-occurrence count\n",
        "    \"\"\"\n",
        "    numerador = counts_wc * count_tot\n",
        "    denominador = counts_w * count_c\n",
        "    res = np.log(numerador / denominador)\n",
        "    return res"
      ],
      "metadata": {
        "id": "dY_tn8tUY9hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pmi([\"god\", \"america\"], [\"almighty\"])"
      ],
      "metadata": {
        "id": "vkeHqH5tf6Bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pmi([\"united\", \"country\"], [\"states\"])"
      ],
      "metadata": {
        "id": "K1nxxt8vmX5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hNeCkxn3_I6"
      },
      "source": [
        "**PREGUNTA 10** ¿En términos generales, qué diferencia hay en el _tipo de asociaciones_ que captura PMI vs. la similitud entre embeddings?\n",
        "\n",
        "OJO: estos PMI surgen de un corpus distinto que los embeddings anteriores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar(glove_wikig, \"country\")"
      ],
      "metadata": {
        "id": "uM1_3gEmmf3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = list(str2idx.keys())\n",
        "pmis = get_pmi(all_words, [\"country\"])\n",
        "pmis_dict = dict(zip(all_words, pmis))\n",
        "sorted(pmis_dict.items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "i672WHb3mvH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Veweiul3_I6"
      },
      "outputs": [],
      "source": [
        "print(glove_wikig.n_similarity([\"very\"], [\"good\"]))\n",
        "print(glove_wikig.n_similarity([\"bad\"], [\"good\"]))\n",
        "print(glove_wikig.n_similarity([\"watermelon\"], [\"good\"]))\n",
        "print(glove_wikig.n_similarity([\"black\"], [\"white\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 11** ¿Por qué es relativamente alta la asociación entre \"bad\" y \"good\" medida con embeddings?"
      ],
      "metadata": {
        "id": "6UBU9ogBD4jZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETdqHYLa3_I6"
      },
      "source": [
        "## Bonus track\n",
        "\n",
        "Identificacion de collocations con nPMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icC2GofJ3_I6"
      },
      "outputs": [],
      "source": [
        "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
        "\n",
        "collocations_model = Phrases(\n",
        "    sentences=obama_sentences,\n",
        "    min_count=3, threshold=.8, # valor umbral de NPMI\n",
        "    scoring='npmi', connector_words=ENGLISH_CONNECTOR_WORDS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collocations_model.export_phrases()"
      ],
      "metadata": {
        "id": "6j1S_jOe_gJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tRlNMzk3_I7"
      },
      "outputs": [],
      "source": [
        "sentences_with_collocations = collocations_model[obama_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ca_65vM3_I7"
      },
      "outputs": [],
      "source": [
        "print(sentences_with_collocations[0])\n",
        "print(sentences_with_collocations[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uW3VK7jw3_I7"
      },
      "outputs": [],
      "source": [
        "# ahora podemos entrenar embeddings considerando las collocations como tokens:\n",
        "# w2v_model = Word2Vec(sentences_with_collocations, ...)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QRaI422CpWWx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}