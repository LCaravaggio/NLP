{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/word2vec_clf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"QDSQ2SpygjBj"},"source":["Vamos a clasificar el sentimiento de reviews de películas con **Word2Vec**, en particular, con una red feed-forward que toma los embeddings de las palabras Word2Vec como entradas.\n","\n","---\n","\n","Tarea: responder donde dice **PREGUNTA**"]},{"cell_type":"markdown","metadata":{"id":"adLynhD0gjBn"},"source":["### Configuración del entorno"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrI-AEHLgjBo"},"outputs":[],"source":["!pip install -qU datasets gensim numpy spacy watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lSTgqVh2gjBp"},"outputs":[],"source":["%reload_ext watermark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sH0M270SgjBr"},"outputs":[],"source":["%watermark -vmp datasets,gensim,spacy,torch,numpy,pandas,tqdm"]},{"cell_type":"markdown","source":["Para usar GPU, arriba a la derecha seleccionar \"Change runtime type\" --> \"T4 GPU\".\n","\n","Es un buena idea desarrollar con CPU, y usar GPU para la corrida final, para que Google no nos limite el uso. En esta notebook vamos a trabajar con pocos datos y un modelo chico, por lo cual no es imprescindible usar GPU."],"metadata":{"id":"fC2quKgooVNc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ly0sQpe0gjBr"},"outputs":[],"source":["import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"VKkqwrY2gjBr"},"source":["## Dataset\n","\n","Cargamos y exploramos el dataset de [reviews de películas de Rotten Tomatoes](https://huggingface.co/datasets/rotten_tomatoes)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxybhTF_gjBr"},"outputs":[],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"rotten_tomatoes\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TtyJpx-6gjBs"},"outputs":[],"source":["dataset"]},{"cell_type":"code","source":["label_names = dataset[\"train\"].features[\"label\"].names\n","print(label_names)"],"metadata":{"id":"8sRLr5osO4Iz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgZGR6MlgjBs"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import datasets\n","from IPython.display import display, HTML\n","\n","def show_random_elements(dataset, num_examples=10):\n","    \"\"\"Muestra num_examples ejemplos aleatorios del dataset.\n","    \"\"\"\n","    indices = np.random.randint(0, len(dataset), num_examples)\n","    df = pd.DataFrame(dataset[indices])\n","    for column, typ in dataset.features.items():\n","        if isinstance(typ, datasets.ClassLabel):\n","            df[column] = df[column].transform(lambda i: typ.names[i])\n","    display(HTML(df.to_html()))\n","\n","np.random.seed(33)\n","show_random_elements(dataset[\"train\"], num_examples=6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zN2rgvjBgjBs"},"outputs":[],"source":["print(\"Distribucion de clases:\")\n","for k in dataset.keys():\n","    print(k)\n","    labels = dataset[k][\"label\"]\n","    print(pd.Series(labels).value_counts())\n","    print(\"-\"*70)"]},{"cell_type":"markdown","source":["**PREGUNTA 1**: ¿para qué sirve mirar la distribución de clases?"],"metadata":{"id":"UzutedrR__Gz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RC9MimWBgjBt"},"outputs":[],"source":["print(\"Largo de los documentos (en palabras), deciles:\")\n","for k in dataset.keys():\n","    print(k)\n","    textos = dataset[k][\"text\"]\n","    largos = pd.Series(textos).str.split().apply(len)\n","    # print(largos.describe())\n","    print(np.quantile(largos, q=np.arange(0, 1.1, .1)).astype(int))\n","    print(\"-\"*70)"]},{"cell_type":"markdown","metadata":{"id":"60Jw72ufgjBt"},"source":["## Embeddings word2vec\n","\n","Vamos a extraer embeddings de las palabras con Word2Vec y entrenar una red feed-forward con una capa oculta.\n","\n","Usamos `gensim` para extraer los embeddings y `pytorch` para entrenar la red. Luego, cuando usemos transformers vamos a usar funciones de la librería `transformers` de HF que ayudan a automatizar algunas tareas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgxJLX6_gjBu"},"outputs":[],"source":["import gensim.downloader as api\n","\n","api.info().keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27dR1huKgjBu"},"outputs":[],"source":["# modelos disponibles:\n","api.info()[\"models\"].keys()"]},{"cell_type":"markdown","metadata":{"id":"xFzllnRggjBu"},"source":["Cargamos los embeddings pre-entrenados, y analizamos el vocabulario y los embeddings de las palabras.\n","\n","**PREGUNTA 2**: ¿qué diferencias hay _grosso modo_ entre estos modelos disponibles? ¿qué quiere decir que son pre-entrenados?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kpjcd1QKgjBv"},"outputs":[],"source":["# puede demorar unos 10 minutos...\n","wv_model = api.load(\"word2vec-google-news-300\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c65vP5RtgjBv"},"outputs":[],"source":["print(\"Tamaño del vocabulario:\")\n","print(len(wv_model.index_to_key))\n","\n","print(\"Palabras más frecuentes:\")\n","print(wv_model.index_to_key[:20])\n","\n","print(\"Palabras menos frecuentes:\")\n","print(wv_model.index_to_key[-20:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FRSzaLTgjBv"},"outputs":[],"source":["tokens = [\"the\", \"The\", \".\", \",\", \"and\", \"xeneize\", \"<unk>\", \"<pad>\"]\n","\n","for token in tokens:\n","    print(f\"{token:10s} in wv vocab: {token in wv_model}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"drenURt1gjBv"},"outputs":[],"source":["# Extraemos el vector de una palabra:\n","vec = wv_model[\"hello\"]\n","print(vec.shape)\n","print(wv_model.vector_size)\n","print(vec[:5])"]},{"cell_type":"markdown","source":["Podemos usar la similitud coseno para medir la cercanía entre palabras."],"metadata":{"id":"HTVZhoMeUAII"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4VWZ4DhgjBw"},"outputs":[],"source":["import numpy as np\n","\n","def cossim(v1, v2):\n","    \"\"\"Calcula la similitud coseno entre dos vectores v1 y v2.\n","    Args:\n","        v1: vector 1\n","        v2: vector 2\n","    \"\"\"\n","    producto_interno = np.dot(v1, v2)\n","    norma_v1 = np.linalg.norm(v1)\n","    norma_v2 = np.linalg.norm(v2)\n","    return producto_interno / (norma_v1 * norma_v2)\n","\n","\n","print(\"Prueba:\")\n","vec_banana = wv_model[\"banana\"]\n","vec_apple = wv_model[\"apple\"]\n","print(f\"{cossim(vec_banana, vec_apple) = :.4f}\")"]},{"cell_type":"markdown","source":["**PREGUNTA 3**: elegir una palabra en inglés ($w$), y otras tres que estén _a priori_ semánticamente asociadas en orden decreciente según la fuerza de la asociación ($x_1$,$x_2$,$x_3$). Calcular la similitud coseno entre $w$ y $x_i$ y verificar que la similitud en el espacio de los embeddings correlaciona con la similitud semántica _a priori_."],"metadata":{"id":"IcbdHTx6sDkj"}},{"cell_type":"code","source":["# w = ...\n","# x1 = ...\n","# x2 = ...\n","# x3 = ...\n","\n","# print(f\"{cossim(..., ...) = :.4f}\")\n","# print(f\"{cossim(..., ...) = :.4f}\")\n","# print(f\"{cossim(..., ...) = :.4f}\")"],"metadata":{"id":"R6ybsgbpsBwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ou7wYuvsgjBw"},"outputs":[],"source":["def most_similar_by_vector(vec, top=None) -> dict:\n","    \"\"\"Hallar las palabras más similares al vector vec.\n","    Args:\n","        vec: vector\n","        top: número de palabras más similares a devolver\n","    Imita el comportamiento de model.similar_by_vector(vec, topn=top)\n","    Implementación ingenua que no paraleliza con multiplicación de matrices.\n","    \"\"\"\n","    similarities = {}\n","    for w in wv_model.index_to_key:\n","        similarities[w] = cossim(wv_model[w], vec)\n","    sorted_similarities = dict(sorted(similarities.items(), key=lambda item: item[1], reverse=True))\n","    if top is not None:\n","        return dict(list(sorted_similarities.items())[:top])\n","    else:\n","        return sorted_similarities\n","\n","def most_similar(word, top=None) -> list:\n","    \"\"\"Hallar las palabras más similares a una palabra dada.\n","    Args:\n","        word: palabra\n","        top: número de palabras más similares a devolver\n","    Imita a model.most_similar(word, topn=top)\n","    \"\"\"\n","    similarities = most_similar_by_vector(wv_model[word], top=None)\n","    similarities.pop(word, None)\n","    if top is not None:\n","        return list(similarities.items())[:top]\n","    else:\n","        return list(similarities.items())"]},{"cell_type":"code","source":["np.random.seed(33)\n","random_vec = np.random.normal(size=wv_model.vector_size)\n","print(most_similar_by_vector(random_vec, top=5))"],"metadata":{"id":"7iz0lyd9U0oc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(wv_model.most_similar(\"banana\", topn=5))"],"metadata":{"id":"5n20Y7zkwg54"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbORtxg9gjBw"},"outputs":[],"source":["print(most_similar(\"banana\" , top=5))"]},{"cell_type":"markdown","source":["**PREGUNTA 4**: ¿0.3 es una similitud alta o baja? ¿Y 0.7? ¿Y 0.9? ¿De qué depende esto?"],"metadata":{"id":"lkGCjk431WRk"}},{"cell_type":"markdown","source":[">La idea es la siguiente:\n",">\n",">1. Hacer un **vocabulario** con las palabras del dataset de train. Usamos un corte por frecuencia para determinar cuándo una palabra es desconocida (token especial \"\\<unk\\>\"). También agregamos un token especial \"\\<pad\\>\" para rellenar las secuencias en el armado de los batches.\n",">2. **Tokenizamos** las reviews y las convertimos en secuencias de índices. Cada índice representa una palabra dentro del vocabulario.\n",">3. Armamos una **matriz de embeddings** de dimensión (n_vocab, embedding_dim) donde cada fila es el embedding de una palabra del vocabulario. Si la palabra está en el vocab de word2vec, usamos ese embedding; caso contrario, inicializamos aleatoriamente.\n",">4. Armamos **batches** de secuencias de índices con **padding** i.e. rellenando las secuencias más cortas con el token \"\\<pad\\>\" para que todas tengan la misma longitud dentro del batch.\n",">5. Definimos una **red de clasificación binaria** que toma como entrada un batch de secuencias de índices de dimensión (batch_size, max_seq_len) y devuelve un tensor de probabilidades de dimensión (batch_size, 2).\n",">6. **Entrenamos** el modelo de clasificación lineal reajustando los embeddings y evaluamos su rendimiento."],"metadata":{"id":"1yBEap_R-SAf"}},{"cell_type":"markdown","metadata":{"id":"3UtHARt2gjBw"},"source":["## Construcción del vocabulario y tokenización\n","\n","Para pasar de documentos a tokens y construir el vocabulario, usamos simplemente `split()` de Python. Vamos a considerar como parte del vocabulario todas las palabras que ocurran al menos dos veces."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJqn9d6jgjBx"},"outputs":[],"source":["def create_vocab(dataset, min_frec=2):\n","    \"\"\"Crea un vocabulario a partir de un dataset de Hugging Face.\n","    Args:\n","        dataset: una partición de un dataset de Hugging Face\n","    Returns:\n","        Dos diccionarios: token2idx (palabra -> índice) y idx2token (índice -> palabra)\n","    \"\"\"\n","    str2count = {}\n","    for example in dataset[\"train\"]:\n","        for token in example[\"text\"].split():\n","            str2count[token] = str2count.get(token, 0) + 1\n","    # filtrar por min_frec:\n","    str2count = {token: count for token, count in str2count.items() if count >= min_frec}\n","    # ordenar de mayor a menor frecuencia:\n","    str2count = dict(sorted(str2count.items(), key=lambda x: x[1], reverse=True))\n","    # Mapeamos cada token a un índice distinto\n","    token2idx = {token: idx for idx, token in enumerate(str2count)}\n","    # Agregamos \"<unk>\" y \"<pad>\" al vocab:\n","    token2idx[\"<unk>\"] = len(str2count)\n","    token2idx[\"<pad>\"] = len(str2count) + 1\n","    # \"Invertir\" el diccionario:\n","    idx2token = {idx: token for idx, token in enumerate(token2idx)}\n","    return token2idx, idx2token\n","\n","\n","token2idx, idx2token = create_vocab(dataset)"]},{"cell_type":"code","source":["print(f\"vocab_size = {len(token2idx)}\")\n","print(f\"1ros tokens = {list(token2idx.keys())[:10]}\")\n","print(f\"úlimos tokens = {list(token2idx.keys())[-10:]}\")"],"metadata":{"id":"kcypF0fz72aM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizamos los documentos de todo el dataset (convertimos los strings en listas de índices)."],"metadata":{"id":"-7bHPpP3_vQD"}},{"cell_type":"code","source":["# Tokenizamos todos los splits del dataset:\n","def tokenize(example):\n","    \"\"\"Procesa un example de un dataset de Hugging Face, tokenizando el texto\n","    y convirtiendo tokens a ids. Agrega una columna \"input_ids\" al example con\n","    un array de enteros.\n","    \"\"\"\n","    token_ids = [token2idx.get(token, token2idx[\"<unk>\"]) for token in example[\"text\"].split()]\n","    return {\"input_ids\": np.array(token_ids, dtype=np.int64)}\n","\n","dataset = dataset.map(tokenize)\n","dataset"],"metadata":{"id":"7X6DpqBaD8jS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# en cada split:\n","# Calculamos la proporción de \"<unk>\" y de tokens que no están en w2v vocab.\n","for k in dataset.keys():\n","    print(k)\n","    all_input_ids = np.hstack(dataset[k][\"input_ids\"])\n","    all_tokens = [idx2token[idx] for idx in all_input_ids]\n","    token_counts = pd.Series(all_input_ids).value_counts(normalize=True)\n","    unk_count = token_counts.get(token2idx[\"<unk>\"], 0)\n","    print(f\"{unk_count = :.2%}\")\n","    not_in_wv_count = np.mean([token not in wv_model for token in all_tokens])\n","    print(f\"{not_in_wv_count = :.2%}\")\n","    print(\"-\"*70)"],"metadata":{"id":"9MGI951wEnDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Veamos un ejemplo:\n","example = dataset[\"validation\"][0]\n","for token, id_ in zip(example[\"text\"].split(), example[\"input_ids\"]):\n","    in_wv = token in wv_model\n","    print(f\"{token:16s} --> {id_:6d} (in wv: {in_wv})\")"],"metadata":{"id":"Kgj2F_lrEfJo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAQ2_rasgjBx"},"source":["## Matriz de embeddings\n","\n","Preparamos la matriz de embeddings. Si un token está en el vocab de w2v, inicializamos con ese vector. Caso contrario, usamos inicialización random. Para \"\\<pad\\>\" usamos 0s, y para \"\\<unk\\>\", el promedio de todos los embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yah60WkVgjBy"},"outputs":[],"source":["def create_embedding_matrix(wv_model, token2idx):\n","    \"\"\"Crea una matriz de embeddings de dimensión (vocab_size, embedding_dim)\n","    donde cada fila es el embedding de una palabra del vocabulario.\n","    Args:\n","        wv_model: modelo word2vec\n","        token2idx: vocab (diccionario que mapea tokens a índices)\n","    \"\"\"\n","    vocab_size = len(token2idx)\n","    embedding_dim = wv_model.vector_size\n","    embedding_matrix = torch.randn(vocab_size, embedding_dim) # random normal(0, 1)\n","    for token, idx in token2idx.items():\n","        if token in wv_model:\n","            embedding_matrix[idx] = torch.tensor(wv_model[token])\n","    embedding_matrix[token2idx[\"<pad>\"]] = torch.zeros(embedding_dim)\n","    embedding_matrix[token2idx[\"<unk>\"]] = embedding_matrix.mean(dim=0)\n","    return embedding_matrix\n","\n","\n","torch.manual_seed(33)\n","embedding_matrix = create_embedding_matrix(wv_model, token2idx)\n","print(\"Primeros valores de la matriz de embeddings:\")\n","print(embedding_matrix[:5, :5])"]},{"cell_type":"markdown","source":["**PREGUNTA 5**: ¿qué dimensiones tiene la matriz de embeddings? ¿Qué representan las filas y las columnas del array anterior?"],"metadata":{"id":"yNhXhHZx3OZL"}},{"cell_type":"markdown","metadata":{"id":"q2euSqSMgjBy"},"source":["## Armado de _batches_\n","\n","Armamos los batches para entrenar y para evaluar el modelo. Para esto\n","\n","* Usamos la clase `DataLoader` de PyTorch. En cada iteración, el `DataLoader` nos devuelve un batch de ejemplos.\n","* Definimos una función `collate_fn` que se encarga de preparar los batches. En este caso, se encarga de rellenar las secuencias con el token \"\\<pad\\>\" para que todas tengan la misma longitud dentro de un batch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQCv3dDhgjBy"},"outputs":[],"source":["from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","\n","def collate_fn(examples):\n","    \"\"\"Función que se pasa a DataLoader para que se encargue de hacer padding\n","    de las secuencias.\n","    Devuelve un diccionario con los tensores de las secuencias de texto,\n","    las longitudes de las secuencias y las etiquetas. Necesitamos las longitudes\n","    para poder hacer pooling en la capa de embeddings de manera correcta.\n","    \"\"\"\n","    input_ids = [example[\"input_ids\"] for example in examples]\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=token2idx[\"<pad>\"])\n","    # batch_first=True: devuelve tensor con batch_size como 1ra dim.\n","    inputs_lengths = torch.tensor([len(example[\"input_ids\"]) for example in examples], dtype=torch.long)\n","    labels = torch.tensor([example[\"label\"] for example in examples], dtype=torch.long)\n","    return {\"input_ids\": input_ids, \"inputs_lengths\": inputs_lengths, \"labels\": labels}"]},{"cell_type":"code","source":["batch_size = 32\n","dataset.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n","\n","train_loader = DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n","val_loader = DataLoader(dataset[\"validation\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n","test_loader = DataLoader(dataset[\"test\"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"],"metadata":{"id":"EskCrQNAN4SY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVJ_V6pQgjBy"},"outputs":[],"source":["# Veamos los primeros dos batches de entrenamiento:\n","torch.manual_seed(33)\n","for i, data in enumerate(train_loader):\n","    print(f\"batch {i}\")\n","    print(f\"Shapes = {[s.shape for s in data.values()]}\")\n","    print(data)\n","    if i == 1:\n","        break"]},{"cell_type":"markdown","source":["Definimos la red neuronal."],"metadata":{"id":"c7RxrazHduse"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UhGXCCTgjBz"},"outputs":[],"source":["from torch import nn\n","\n","class FFNModel(nn.Module):\n","    \"\"\"Red Feed-Forward de dos capas (hidden layer + output layer).\n","    \"\"\"\n","    def __init__(self, vocab_size, embedding_dim, pad_idx, hidden_size, num_labels,\n","                 embedding_matrix=None, freeze_embeddings=False):\n","        \"\"\"Args:\n","            vocab_size: tamaño del vocabulario\n","            embedding_dim: dimensión de los embeddings\n","            pad_idx: índice del token \"<pad>\"\n","            hidden_size: dimensión de la capa oculta\n","            num_labels: número de clases de salida\n","            embedding_matrix: matriz de embeddings pre-entrenadas\n","            freeze_embeddings: si True, no se entrena la matriz de embeddings\n","        \"\"\"\n","        super().__init__()\n","        if embedding_matrix is not None:\n","            self.embedding = nn.Embedding.from_pretrained(\n","                embedding_matrix, padding_idx=pad_idx,\n","                freeze=freeze_embeddings\n","            )\n","        else:\n","            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.fc = nn.Linear(embedding_dim, hidden_size)\n","        self.clf = nn.Linear(hidden_size, num_labels)\n","\n","    def forward(self, text, text_lengths):\n","        \"\"\"Dimensiones de los tensores:\n","            text: (batch_size, seq_len)\n","            text_lengths: (batch_size,)\n","        Returns:\n","            Un tensor de **logits** (batch_size, num_labels)\n","        \"\"\"\n","        embeddings = self.embedding(text) # (batch_size, seq_len, embedding_dim)\n","        pooled_embeddings = embeddings.sum(dim=1) / text_lengths.unsqueeze(1) # (batch_size, embedding_dim)\n","        z = torch.relu(self.fc(pooled_embeddings)) # (batch_size, hidden_size)\n","        logits = self.clf(z) # (batch_size, num_labels)\n","        return logits"]},{"cell_type":"markdown","source":["**PREGUNTA 6**: ¿cómo hacemos este modelo para procesar textos de distinta longitud?\n","\n","**PREGUNTA 7**: ¿qué hace esto `embeddings.sum(dim=1) / text_lengths.unsqueeze(1)`?\n","\n","**PREGUNTA 8**: ¿qué va a hacer el modelo si aparece un token nuevo/desconocido (OOV) en la inferencia? ¿Y si es un token conocido pero que no tiene embedding w2v?"],"metadata":{"id":"Zb3dHNV46lhk"}},{"cell_type":"markdown","source":["## Entrenamiento"],"metadata":{"id":"FnJf3IFqAbSq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"roKSwM98gjBz"},"outputs":[],"source":["ffn_model = FFNModel(\n","    vocab_size=len(token2idx),\n","    embedding_dim=wv_model.vector_size,\n","    pad_idx=token2idx[\"<pad>\"],\n","    hidden_size=128,\n","    num_labels=dataset[\"train\"].features[\"label\"].num_classes,\n","    embedding_matrix=embedding_matrix,\n","    freeze_embeddings=False\n",")"]},{"cell_type":"markdown","metadata":{"id":"_gB4mIQ5gjBz"},"source":["Definimos `CrossEntropyLoss` como función de pérdida (está diseñada para trabajar con los logits de salida del modelo, _no_ las probabilidades), y el optimizador Adam, y una función para computar las métricas de evaluación que nos interesan."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2bQUfIAMgjB0"},"outputs":[],"source":["from torch import optim\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(ffn_model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd_TWMyWgjB0"},"outputs":[],"source":["def compute_metrics(logits, labels):\n","    \"\"\"Args:\n","        logits: array shape (batch_size, num_labels)\n","        labels: array shape (batch_size,)\n","    \"\"\"\n","    # Usamos torch para usar loss_fn, pero podriamos usar cpu y numpy\n","    if not isinstance(logits, torch.Tensor):\n","        logits = torch.tensor(logits)\n","    if not isinstance(labels, torch.Tensor):\n","        labels = torch.tensor(labels)\n","    predictions = torch.argmax(logits, dim=-1)\n","    accuracy = (predictions == labels).float().mean().item()\n","    cross_entropy = loss_fn(logits, labels).item()\n","    return {\"accuracy\": accuracy, \"cross_entropy\": cross_entropy}\n","\n","# por ejemplo:\n","predicciones_ = [[1.2, -1.2], [-.7, .7], [.9, -.9]] # 3 muestras, dos logits por muestra\n","labels_ = [0, 1, 0] # 3 muestras, 1 label por muestra\n","compute_metrics(predicciones_, labels_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDDkS5NSgjB0"},"outputs":[],"source":["ffn_model = ffn_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OU83-acdgjB1"},"outputs":[],"source":["# El loop de entrenamiento:\n","from tqdm import tqdm\n","\n","def validate(model, loader):\n","    \"\"\"Función de evaluación en dataset de validación para ser ejecutada\n","    cada N steps de entrenamiento.\n","    Args:\n","        model: modelo\n","        loader: loader de validación\n","    Returns:\n","        Un diccionario con las métricas (nombre -> valor)\n","    \"\"\"\n","    all_labels = []\n","    all_logits = []\n","    model.eval()\n","    with torch.inference_mode():\n","        for batch in loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            inputs_lengths = batch[\"inputs_lengths\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            logits = model(input_ids, inputs_lengths)\n","            all_labels.append(labels)\n","            all_logits.append(logits)\n","    all_labels = torch.cat(all_labels)\n","    all_logits = torch.cat(all_logits)\n","    metrics = compute_metrics(all_logits, all_labels)\n","    return metrics\n","\n","\n","def train_epoch(model, optimizer, train_loader, val_loader, best_val_acc, eval_steps=50):\n","    \"\"\"Entrenar el modelo un epoch.\n","    Args:\n","        model: modelo\n","        optimizer: optimizador\n","        train_loader: loader de entrenamiento\n","        val_loader: loader de validación\n","        best_val_acc: mejor valor de accuracy en validación\n","        eval_steps: número de steps entre evaluaciones\n","    Returns:\n","        Mejor valor de accuracy en validación\n","    \"\"\"\n","    model.train()\n","    steps_done = 0\n","    total_loss = 0\n","    n_steps = len(train_loader)\n","    for batch in tqdm(train_loader, total=n_steps, position=0):\n","        input_ids = batch[\"input_ids\"].to(device)\n","        inputs_lengths = batch[\"inputs_lengths\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        optimizer.zero_grad()\n","        logits = model(input_ids, inputs_lengths)\n","        batch_loss = loss_fn(logits, labels)\n","        batch_loss.backward()\n","        optimizer.step()\n","        total_loss += batch_loss.item()\n","        steps_done += 1\n","        if steps_done % eval_steps == 0:\n","            metrics = validate(model, val_loader)\n","            train_loss = total_loss / steps_done\n","            print(f\"\\n[steps={steps_done}] train_loss: {train_loss:.4f}, val_acc: {metrics['accuracy']:.4f}\")\n","            if metrics[\"accuracy\"] > best_val_acc:\n","                best_val_acc = metrics[\"accuracy\"]\n","                print(f\">>> New best accuracy: {best_val_acc:.4f}\")\n","                torch.save(model.state_dict(), \"best_model.pt\")\n","            model.train()\n","    return best_val_acc\n","\n","\n","def train(model, optimizer, train_loader, val_loader, n_epochs):\n","    \"\"\"Entrena el modelo durante n_epochs.\n","    Args:\n","        model: modelo\n","        optimizer: optimizador\n","        train_loader: loader de entrenamiento\n","        val_loader: loader de validación\n","        n_epochs: número de epochs\n","    \"\"\"\n","    best_val_acc = -999\n","    for epoch in range(n_epochs):\n","        print(f\"Epoch {epoch} / {n_epochs}\")\n","        best_val_acc = train_epoch(\n","            model, optimizer, train_loader, val_loader, best_val_acc=best_val_acc\n","        )\n","        print(\"-\"*70)"]},{"cell_type":"markdown","source":["**PREGUNTA 9**: ¿para qué sirve `torch.inference_mode()`? ¿para qué sirve `model.eval()`?"],"metadata":{"id":"2ya801tD5V8T"}},{"cell_type":"code","source":["n_epochs = 3\n","optimization_steps = len(train_loader) * n_epochs\n","print(f\"epochs = {n_epochs}, total steps = {optimization_steps}\")"],"metadata":{"id":"w9w0co2ofVZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M15wwqKGgjB1"},"outputs":[],"source":["torch.manual_seed(33)\n","train(ffn_model, optimizer, train_loader, val_loader, n_epochs)"]},{"cell_type":"markdown","source":["## Evaluación en test\n","\n","Cargamos el mejor modelo y lo evaluamos en el set de test.\n"],"metadata":{"id":"1lNSnmv-ARWR"}},{"cell_type":"code","source":["best_model = FFNModel(\n","    vocab_size=len(token2idx),\n","    embedding_dim=wv_model.vector_size,\n","    pad_idx=token2idx[\"<pad>\"],\n","    hidden_size=128,\n","    num_labels=dataset[\"train\"].features[\"label\"].num_classes,\n","    embedding_matrix=embedding_matrix\n",")\n","best_model.load_state_dict(torch.load(\"best_model.pt\"))\n","best_model = best_model.to(device)"],"metadata":{"id":"R2DwTGdjmcOw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5G8tGMjAgjB1"},"outputs":[],"source":["def eval_model(model, loader):\n","    \"\"\"Evalúa el modelo en un dataset dado.\n","    Args:\n","        model: modelo\n","        loader: loader del dataset de evaluación\n","    Returns:\n","        Un diccionario con las métricas (nombre -> valor)\n","    \"\"\"\n","    model.eval()\n","    metrics = validate(model, loader)\n","    return metrics\n","\n","print(\"Resultados del mejor modelo en los datos de test:\")\n","test_metrics = eval_model(best_model, test_loader)\n","for k, v in test_metrics.items():\n","    print(f\"{k:16s}= {v:.4f}\")"]},{"cell_type":"markdown","source":["**PREGUNTA 10**: ¿cómo podemos saber cuán buenos son los resultados?"],"metadata":{"id":"PbbvCRXx5pXf"}},{"cell_type":"code","source":[],"metadata":{"id":"PynM2xm35xf0"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}