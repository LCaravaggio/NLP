{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/07_CausalLMFinetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a hacer fine-tuning de un LM causal con [**GPT-2**](https://huggingface.co/docs/transformers/model_doc/gpt2):\n",
        "\n",
        "* Es un LM (causal) de transformers\n",
        "* Datos de entrenamiento: _WebText_ (scraping de links que salen de reddit con al menos 3 upvotes)\n",
        "* Tokenizador: subword tokenization con BPE (Byte Pair Encoding)\n",
        "\n",
        "Aunque en realidad vamos a usar una versión _destilada_: **distilled-GPT2**.\n",
        "\n",
        "_Knowledge distillation_ es un proceso que entrena una versión reducida de un modelo más grande al que se intenta imitar, con el objetivo de acelerar el procesamiento y el finetuning en tareas específicas, sacrificando poca performance (ver https://arxiv.org/pdf/1910.01108v4.pdf y https://arxiv.org/pdf/2006.05525.pdf).\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Tarea: responder donde dice **PREGUNTA**"
      ],
      "metadata": {
        "id": "XdHoV9NWEYWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno"
      ],
      "metadata": {
        "id": "vjRefaGBoI20"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "!pip install -qU torch datasets transformers watermark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "TriIEyrSShEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp torch,transformers,datasets,pandas,numpy"
      ],
      "metadata": {
        "id": "HhtVteVkSVY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para usar GPU, arriba a la derecha seleccionar \"Change runtime type\" --> \"T4 GPU\"."
      ],
      "metadata": {
        "id": "fC2quKgooVNc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly0sQpe0gjBr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_n9OWV3l-Q"
      },
      "source": [
        "## Data\n",
        "\n",
        "Cargamos [reviews de yelp](https://huggingface.co/datasets/yelp_review_full). Vamos a usar solo algunos ejemplos para trabajar más rápido.\n",
        "\n",
        "Para cargar un dataset propio ver https://huggingface.co/docs/datasets/loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2ZRs1cL3l-R"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BgnA0GBcX72"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS1lJ50NWnNk"
      },
      "outputs": [],
      "source": [
        "print(*dataset[\"train\"].features.items(), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8n4-z4dfNTt"
      },
      "outputs": [],
      "source": [
        "# 5k train, 2k validation, 5k test\n",
        "from datasets import DatasetDict\n",
        "\n",
        "small_dataset = DatasetDict(\n",
        "    train=dataset[\"train\"].shuffle(seed=33).select(range(0, 5_000)),\n",
        "    val=dataset[\"train\"].shuffle(seed=33).select(range(10_000, 12_000)),\n",
        "    test=dataset[\"test\"].shuffle(seed=33).select(range(5_000)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 1**: ¿Por qué podríamos necesitar tres sets aún si no tuneamos hiperparámetros?"
      ],
      "metadata": {
        "id": "ZeGsDsz9w5Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "kNkldiNYTNWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(example):\n",
        "    \"\"\"Corrige caracteres raros segun la doc de yelp\n",
        "    \"\"\"\n",
        "    texto = re.sub(r'\\\\n', '\\n', example[\"text\"]) # real newlines\n",
        "    texto = re.sub(r'\\\\\"', '\"', texto) # comillas de verdad\n",
        "    example[\"text\"] = texto\n",
        "    return example"
      ],
      "metadata": {
        "id": "mGt4HgDcgtW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset = small_dataset.map(clean_text)"
      ],
      "metadata": {
        "id": "RJL1jaYyf59d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "WXzDCnjijY7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEA1ju653l-p"
      },
      "source": [
        "## Tokenización y modelo\n",
        "\n",
        "El max_length admitido por el modelo es 1024 pero esto puede consumir mucha memoria. Entonces vamos a trabajar con un max_length de 128 tokens.\n",
        "\n",
        "En particular, vamos a partir cada documento en pedazos de 128 tokens. Vamos a tener algunos pedazos con menos de 128 porque hay documentos que no llegan a esta cantidad, y también por los pedazos que queden al final de documentos largos.\n",
        "\n",
        "Para poder hacer un procesamiento en batches vamos a necesitar _padding_: completar con un token especial hasta llegar al max_length o a la máxima longitud del batch.\n",
        "\n",
        "Una alternativa es truncar los documentos con más de 128 tokens pero si tenemos muchos documentos largos esto puede implicar tirar mucha información.\n",
        "\n",
        "Vamos a cargar el tokenizador y los pesos de un modelo pre-entrenado: a esto se le llama **checkpoint**. En este caso, la arquitectura es GPT-2 Distilled, mientras que el checkpoint (los pesos específicos) se llama `distilgpt2`.\n",
        "\n",
        "Vamos a cargar tokenizer y modelo con `AutoClass`es que permiten cargar checkpoints de cualquier arquitectura rápidamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WGBCO343l-q"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAYlS40Z3l-v"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length # Hay solo model_max_length embeddings de posicion"
      ],
      "metadata": {
        "id": "yDxjK49Hsuj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context_length = tokenizer.model_max_length\n",
        "context_length = 128"
      ],
      "metadata": {
        "id": "Hd210WXnIHq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos cómo funciona la tokenización en 3 ejemplos\n",
        "ejemplos = small_dataset[\"train\"][:3]\n",
        "ejemplos"
      ],
      "metadata": {
        "id": "ZwOCgzIVIeQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_ = tokenizer(\n",
        "    ejemplos[\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True, # tokeniza doc y lo parte en pedazos\n",
        "    return_length=True, # computa length de cada doc\n",
        ")"
      ],
      "metadata": {
        "id": "RJ0OJ2zXkT0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# como ouput obtenemos token_ids y attention_mask\n",
        "# por el momento solo vamos a usar token_ids\n",
        "outputs_"
      ],
      "metadata": {
        "id": "3kKAozrvNrYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cantidad de chunks: {len(outputs_['input_ids'])}\")\n",
        "print(f\"Tokens en cada chunk: {(outputs_['length'])}\")\n",
        "print(f\"Mapping chunk-doc: {outputs_['overflow_to_sample_mapping']}\")"
      ],
      "metadata": {
        "id": "-ifX03WTlZGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# con tokenize() obtenemos la separación en subwords\n",
        "tokens_ = tokenizer.tokenize(ejemplos[\"text\"][0])\n",
        "print(tokens_)"
      ],
      "metadata": {
        "id": "B6kMlO1ANLEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# el tokenizer de gpt2 trata a los espacios como parte de las palabras,\n",
        "# entonces codifica distinto a las palabras en el medio vs el principio de la\n",
        "# secuencia\n",
        "# https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer\n",
        "\n",
        "print(tokenizer.tokenize(\"Love this place\"))\n",
        "print(tokenizer(\"Love this place\")['input_ids'])\n",
        "print(tokenizer.tokenize(\" Love this place\"))\n",
        "print(tokenizer(\" Love this place\")['input_ids'])"
      ],
      "metadata": {
        "id": "6WS1iebfOPqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(example):\n",
        "    \"\"\"Tokeniza `text` de examples de un dataset.\n",
        "    Returns only input_ids.\n",
        "    \"\"\"\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    return {\"input_ids\": outputs[\"input_ids\"]}"
      ],
      "metadata": {
        "id": "uI_t441IRFk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos la tokenizacion en batches y 4 procesos para acelerar la corrida\n",
        "    # descartamos el resto de columnas\n",
        "tokenized_dataset = small_dataset.map(\n",
        "    tokenize_fn, batched=True, num_proc=4,\n",
        "    remove_columns=small_dataset[\"train\"].column_names)\n",
        "\n",
        "# NOTE: si queremos conservar mas columnas, tenemos que generar la misma\n",
        "# cantidad de datos que en el output (esta tokenizacion genera mas samples\n",
        "# que la cantidad inicial de examples)"
      ],
      "metadata": {
        "id": "LQRKPv1qPGJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset"
      ],
      "metadata": {
        "id": "Wnzj6XbpoPIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "uOD9phXhP-pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 2**: ¿qué representa cada _row_ de tokenized_dataset?"
      ],
      "metadata": {
        "id": "rq4TIiF0qiSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el modelo\n",
        "    # Usamos el EOS token as PAD token to avoid warnings (GPT2 does not have a PAD token)\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint, pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "nvit8xFYSZK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
        "# numel: number of elements in tensor\n",
        "\n",
        "# gpt3 tiene 175B params, gpt4 tiene 1T..."
      ],
      "metadata": {
        "id": "Np2ITqLySOD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "axD5FaXUmkm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEmeQ7Xm3l_H"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Un \"collator\" es una función que forma batches de datos.\n",
        "\n",
        "Vamos a usar un \"collator\" que arma batches de ejemplos con padding. `DataCollatorForLanguageModeling` está diseñado específicamente para language models.\n",
        "\n",
        "En particular se encarga de:\n",
        "\n",
        "* armar los targets del modelo (los tokens desplazados) _on the fly_ durante el entrenamiento sin duplicar los input_ids.\n",
        "* Agregar padding donde corresponda\n",
        "\n",
        "Usamos `mlm=False` para usar **Causal Language Modeling** en lugar de Masked Language Modeling.\n",
        "\n",
        "Podemos loguear métricas durante el entrenamiento con tensorboard, wandb, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# el padding se hace con el EOS token\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "HwoiDTP6U457"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un ejemplo con un batch de 3 docs\n",
        "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(3)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ],
      "metadata": {
        "id": "w6GxNu8NU4uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hay padding:\n",
        "out[\"input_ids\"][1]"
      ],
      "metadata": {
        "id": "J5OorNSLVdWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention mask para no hacer attention sobre pad_tokens:\n",
        "out[\"attention_mask\"][1]"
      ],
      "metadata": {
        "id": "r0j4su0LVdN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos solo el nombre del modelo para el nuevo nombre (no el usuario)\n",
        "pretrained_model_name = model_checkpoint.split(\"/\")[-1]\n",
        "finetuned_model_name = f\"{pretrained_model_name}-finetuned-yelp\"\n",
        "print(finetuned_model_name)"
      ],
      "metadata": {
        "id": "CXI-S4Rxilm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vamos a usar wandb, copiamos API key de https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "-FZLWKTS_7K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wandb login"
      ],
      "metadata": {
        "id": "LxBx0coy7vx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"WANDB_PROJECT\"] = project_name"
      ],
      "metadata": {
        "id": "RfadvKi47pxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos los parametros del entrenamiento\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    finetuned_model_name,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.1, # forma de regularizacion (restringe el tamaño de updates de SGD)\n",
        "    warmup_ratio=0.1, # # warmup evita divergencia de loss en primeros steps (10%)\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    do_eval=True, # eval en validation set\n",
        "    gradient_accumulation_steps=1, # acumula gradientes por N steps --> update cada N*32 samples\n",
        "    # sirve cuando batches grandes no entran en memoria y tenemos muchos samples\n",
        "    eval_strategy=\"steps\", # eval en validation set\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True, # conserva mejor modelo segun eval loss\n",
        "    save_total_limit=2, # save max 2 models including best one\n",
        "    save_steps=50, # checkpoint model every N steps\n",
        "    logging_dir='./logs', # logging\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    fp16=True, # float16 en training (only on CUDA)\n",
        "    push_to_hub=False,\n",
        "#    report_to=\"wandb\",  # enable logging to W&B\n",
        "   report_to=\"none\",\n",
        "    save_safetensors=False # por un bug\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"], #.select(range(0, 128)),\n",
        "    eval_dataset=tokenized_dataset[\"val\"], #.select(range(0, 128)),\n",
        ")"
      ],
      "metadata": {
        "id": "7HAQ9gpajSRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 3**: ¿qué es el parámetro de learning_rate?"
      ],
      "metadata": {
        "id": "wqFTXdh3rZXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_UzidtN_y1p"
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./logs # para wandb/tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbSwEhQ63l_L"
      },
      "outputs": [],
      "source": [
        "#%reload_ext tensorboard\n",
        "#%tensorboard --logdir logs\n",
        "\n",
        "# para wandb/tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos!\n",
        "train_output = trainer.train()"
      ],
      "metadata": {
        "id": "lbtIwaS_RhK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# para guardar el modelo:\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "mHTwgvO2YqNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMD2qhItpeA-"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output"
      ],
      "metadata": {
        "id": "EBBGs9-XDUPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# volvemos a calcular loss en train porque train_output.training_loss\n",
        "# se calcula con criterio distinto a trainer.evaluate()\n",
        "train_results = trainer.evaluate(tokenized_dataset[\"train\"])\n",
        "val_results = trainer.evaluate()\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])"
      ],
      "metadata": {
        "id": "S6mr2CoiAgdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_results"
      ],
      "metadata": {
        "id": "zBm9ODF7QOV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_results"
      ],
      "metadata": {
        "id": "gAHxowUgA3LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"Perplexity:\")\n",
        "print(f\"Train: {np.exp(train_results['eval_loss']):.2f}\")\n",
        "print(f\"Validation: {np.exp(val_results['eval_loss']):.2f}\")\n",
        "print(f\"Test: {np.exp(test_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "QVVvdO0K_7IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparamos con el GPT2 no fine-tuneado\n",
        "    # un poco hackoso, instanciamos un trainer pero no vamos a entrenar\n",
        "    # es solo para replicar exactamente la evaluacion anterior, sería\n",
        "    # mejor armar una funcion adhoc\n",
        "model_original = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint, pad_token_id=tokenizer.eos_token_id)\n",
        "trainer_aux = Trainer(\n",
        "    model=model_original,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"], #.select(range(0, 128)),\n",
        "    eval_dataset=tokenized_dataset[\"test\"], #.select(range(0, 128)),\n",
        ")"
      ],
      "metadata": {
        "id": "rFif8fQuRzta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_original = trainer_aux.evaluate(tokenized_dataset[\"test\"])"
      ],
      "metadata": {
        "id": "2RuxGVwfSD1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perplexity (no fine-tuning):\")\n",
        "print(f\"Test: {np.exp(test_results_original['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "j_GOu4xYSq_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 4** ¿por qué la versión fine-tuned tiene menos perplexity que sin fine-tuning?"
      ],
      "metadata": {
        "id": "w1t3YJ4LvIfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation"
      ],
      "metadata": {
        "id": "wO1L68EDAdCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "V8jvbnVOzS9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    prompt=None, max_length=100, greedy=True, model=model, tokenizer=tokenizer, device=device\n",
        "):\n",
        "    \"\"\"Generar texto con sampling (greedy=False) o greedy search (greedy=True)\n",
        "\n",
        "    prompt=None stands for beggining of sequence.\n",
        "\n",
        "    NOTE si bien parece que GPT2 puede generar a partir de BOS token, la\n",
        "    documentacion es poco clara. Ademas hicimos nuestro finetuning sin BOS token.\n",
        "    Entonces solo vamos a usar la funcion pasandole un contexto.\n",
        "\n",
        "    Ver:\n",
        "    https://github.com/huggingface/transformers/issues/3311#issuecomment-601264426\n",
        "    https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/generate_unconditional_samples.py#L60\n",
        "    \"\"\"\n",
        "    do_sample = False if greedy else True\n",
        "    # model.eval() to set dropout and batch normalization layers to evaluation mode before running inference\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        if prompt:\n",
        "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "            outputs = model.generate(input_ids, do_sample=do_sample, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "        else:\n",
        "            outputs = model.generate(do_sample=do_sample, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "    # pad_token_id=tokenizer.eos_token_id to suppress warning\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "NbP7wI0Z0Pgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_ = generate('I loved \"El Topo\" because')\n",
        "print(res_[0])"
      ],
      "metadata": {
        "id": "mjsKO7Ko6gTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 5**: ¿con un mismo prompt vamos a obtener siempre la misma generación?"
      ],
      "metadata": {
        "id": "r0C1m_E7tVUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(33)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "id": "SVlHuwhF8HAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "id": "Ov2GTNXvZHxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(33)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False, model=model_original)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "id": "OQM2iv73ZRB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 6** ¿por qué el formato y contenido del texto generado con el modelo sin fine-tuning es tan distinto al modelo fine-tuned?"
      ],
      "metadata": {
        "id": "2piMCRRLwVh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(23)\n",
        "res_ = generate('I hated the cake from \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "id": "95qrlI56_XMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('It was the worst day ever because', greedy=False)"
      ],
      "metadata": {
        "id": "tvFXZGrxHAOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "* [Causal LM from sratch](https://huggingface.co/course/chapter7/6?#training-a-causal-language-model-from-scratch)\n",
        "\n",
        "* [LM finetuning](https://github.com/huggingface/notebooks/blob/6ca682955173cc9d36ffa431ddda505a048cbe80/examples/language_modeling.ipynb)\n",
        "\n",
        "* [Customized training](https://huggingface.co/course/chapter3/4#a-full-training)\n",
        "\n",
        "* [Text generation](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)\n",
        "\n",
        "* [Scripts para entrenar y finetunear modelos](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
        "\n",
        "* [Sobre GPT-2](https://huggingface.co/gpt2)\n",
        "\n",
        "* [Autoclasses](https://huggingface.co/docs/transformers/autoclass_tutorial)\n",
        "\n",
        "* [Hugging Face + wandb](https://docs.wandb.ai/guides/integrations/huggingface) (no logré hacerlo andar bien en colab 😞)\n",
        "\n",
        "* [Howard & Gugger (2020) - Deep learning for coders with fastai and PyTorch](https://dl.ebooksworld.ir/books/Deep.Learning.for.Coders.with.fastai.and.PyTorch.Howard.Gugger.OReilly.9781492045526.EBooksWorld.ir.pdf) -- temas generales de fine-tuning y DL"
      ],
      "metadata": {
        "id": "W8HQLLP9D3n1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCfJdyG_7FFY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}