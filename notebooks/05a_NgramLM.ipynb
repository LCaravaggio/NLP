{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/05a_NgramLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a entrenar un modelo de lenguaje ngram sobre un corpus de recetas de cocina con la librería `nltk`."
      ],
      "metadata": {
        "id": "q2y-krf3hJYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno"
      ],
      "metadata": {
        "id": "wfGGhMMse9Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets spacy nltk watermark"
      ],
      "metadata": {
        "id": "U3PdmF1YercR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "Z-FyVrzxerUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext watermark"
      ],
      "metadata": {
        "id": "jH1A0ex8erLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vmp datasets,spacy,nltk,numpy,pandas,tqdm"
      ],
      "metadata": {
        "id": "_KkeUlwMiXFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Vamos a usar un [corpus de recetas de SomosNLP](https://huggingface.co/datasets/somosnlp/RecetasDeLaAbuela)."
      ],
      "metadata": {
        "id": "pN9ZrHk5h6mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"somosnlp/RecetasDeLaAbuela\", \"version_1\")"
      ],
      "metadata": {
        "id": "gzVdx6xVh6Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos la estructura:\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "xMpZStLajfTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conservamos pais = \"ESP\":\n",
        "dataset = dataset.filter(lambda x: x[\"Pais\"] == \"ESP\")"
      ],
      "metadata": {
        "id": "alRfTgMcjnez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un ejemplo al azar:\n",
        "dataset[\"train\"][300]"
      ],
      "metadata": {
        "id": "Z7rgvwpCrY_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A veces los textos son listas no parseadas como tales.\n",
        "# En tal caso, hacemos un join de la lista.\n",
        "import re\n",
        "\n",
        "def preprocess(example):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    if example[\"Pasos\"].startswith(\"[\"):\n",
        "        pasos_list = eval(example[\"Pasos\"].encode('unicode_escape'))\n",
        "        example[\"Pasos\"] = \" \".join(pasos_list)\n",
        "    # Eliminamos whitespace duplicado:\n",
        "    example[\"Pasos\"] = re.sub(r'\\s+', ' ', example[\"Pasos\"])\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(preprocess)"
      ],
      "metadata": {
        "id": "lQkKTsirfOFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][300]"
      ],
      "metadata": {
        "id": "LXVr7n_XfN8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos un partición train/test y achicamos (solo para trabajar mas rapido). Y conservamos solo el texto de las recetas."
      ],
      "metadata": {
        "id": "o_XXBbwrfVyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle(seed=33)"
      ],
      "metadata": {
        "id": "hqSAV2K-fN1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_train = dataset[\"train\"].select(range(0, 4_000))[\"Pasos\"]\n",
        "texts_test = dataset[\"train\"].select(range(4_000, 8_000))[\"Pasos\"]"
      ],
      "metadata": {
        "id": "0jYzjqzHfYJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "print(textwrap.fill(texts_train[33], 100))"
      ],
      "metadata": {
        "id": "5pvDeXwAfYAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización y \"entrenamiento\" del LM\n",
        "\n",
        "* Usamos el tokenizer para español de `spacy`.\n",
        "* Consideramos como parte del vocabulario todas las palabras que ocurran al menos dos veces en train. Usamos `nltk` para definir el vocab y detectar los \"\\<unk\\>\" en test.\n",
        "* Hacemos padding con BOS y EOS tokens.\n",
        "* Debemos tener en cuenta que `nltk` espera que cada documento sea una lista de strings.\n",
        "\n",
        "Usamos el LM más sencillo, el MLE (Maximum Likelihood Estimator), con 4-gramas."
      ],
      "metadata": {
        "id": "gOR6scsQjyay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer con reglas de puntacion, contracciones, etc:\n",
        "import spacy\n",
        "\n",
        "tokenizer = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "M6-oSyEvnXFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos un ejemplo:\n",
        "doc = tokenizer(texts_train[0])\n",
        "print(doc.text)\n",
        "for i, token in enumerate(doc):\n",
        "    print(token.text)\n",
        "    if i > 15:\n",
        "        break"
      ],
      "metadata": {
        "id": "6citqe3bn5cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizamos en train y test, sin marcar los UNK todavía:\n",
        "import tqdm\n",
        "\n",
        "def tokenize(doc, ngram_order=4):\n",
        "    \"\"\"Tokeniza un documento y agrega BOS y EOS tokens.\n",
        "    \"\"\"\n",
        "    tokens = [token.text for token in tokenizer(doc)]\n",
        "    tokens = [\"<bos>\"] * (ngram_order - 1) + tokens + [\"<eos>\"]\n",
        "    return tokens\n",
        "\n",
        "tokenized_train = []\n",
        "for doc in tqdm.tqdm(texts_train):\n",
        "    tokenized_train.append(tokenize(doc))\n",
        "\n",
        "tokenized_test = []\n",
        "for doc in tqdm.tqdm(texts_test):\n",
        "    tokenized_test.append(tokenize(doc))"
      ],
      "metadata": {
        "id": "SaAuKpculave"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Un poco de hacking de nltk para evitar que haga padding con la misma cantidad de tokens\n",
        "# a izq y derecha\n",
        "from functools import partial\n",
        "from itertools import chain\n",
        "\n",
        "from nltk.util import everygrams, pad_sequence\n",
        "\n",
        "flatten = chain.from_iterable\n",
        "pad_both_ends = partial(\n",
        "    pad_sequence,\n",
        "    pad_left=True,\n",
        "    left_pad_symbol=\"<s>\",\n",
        "    pad_right=True,\n",
        "    right_pad_symbol=\"</s>\",\n",
        ")\n",
        "\n",
        "def padded_everygram_pipeline(order, text):\n",
        "    \"\"\"Modificación de https://www.nltk.org/_modules/nltk/lm/preprocessing.html\n",
        "    para que no haga padding\n",
        "    \"\"\"\n",
        "    padding_fn = partial(pad_both_ends, n=0)\n",
        "    return (\n",
        "        (everygrams(list(padding_fn(sent)), max_len=order) for sent in text),\n",
        "        flatten(map(padding_fn, text)),\n",
        "    )\n",
        "\n",
        "train, train_flat = padded_everygram_pipeline(4, tokenized_train)"
      ],
      "metadata": {
        "id": "dSyq3j2JkGEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora sí armamos el vocab\n",
        "from nltk.lm import Vocabulary\n",
        "\n",
        "# cutoff de freq>=2 para el vocab:\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)"
      ],
      "metadata": {
        "id": "JLD6ARh9kFpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab size:\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "nrsSk76ssSCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens más y menos frecuentes:\n",
        "print(sorted(vocab.counts, key=vocab.counts.get, reverse=True)[:5])\n",
        "print(sorted(vocab.counts, key=vocab.counts.get)[:5])"
      ],
      "metadata": {
        "id": "_ThmBc3SkPTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens ordenados alfabeticamente:\n",
        "print(sorted(vocab.counts)[:5])\n",
        "print(sorted(vocab.counts, reverse=True)[:5])"
      ],
      "metadata": {
        "id": "ImdNkKfBgAVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens con frec 1 \"no están en el vocab\" (pero podemos consultar su frec.)\n",
        "print(vocab[\"el\"], \"el\" in vocab)\n",
        "print(vocab[\"digestivo\"], \"digestivo\" in vocab)\n",
        "print(vocab[\" \"], \" \" in vocab)\n",
        "print(vocab[\"riquelme\"], \"riquelme\" in vocab)"
      ],
      "metadata": {
        "id": "tJx-uESxgARN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ejemplos de sequencia tokenizada:\n",
        "print(vocab.lookup(tokenized_train[33][:10]))\n",
        "print(vocab.lookup([\"un\", \"té\", \"digestivo\", \".\"]))"
      ],
      "metadata": {
        "id": "e9dJo8VvgANL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos el modelo con el ngram order y el vocab\n",
        "from nltk.lm import MLE\n",
        "\n",
        "lm = MLE(4, vocabulary=vocab)"
      ],
      "metadata": {
        "id": "HeQ76Ynr-G6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm.fit(train)"
      ],
      "metadata": {
        "id": "p5gvEgQ03LvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.counts)"
      ],
      "metadata": {
        "id": "k4rH7plLVuBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unigram counts\n",
        "lm.counts['la']"
      ],
      "metadata": {
        "id": "YJSdKlF5Vtu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram counts\n",
        "print(lm.counts[['en']][\"la\"])\n",
        "print(lm.counts[['la']][\"en\"])\n",
        "print(lm.counts[['el']][\"<UNK>\"])"
      ],
      "metadata": {
        "id": "849ZrN1QWgxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram y 4gram counts\n",
        "print(lm.counts[[\"con\", \"la\"]][\"cuchara\"])\n",
        "print(lm.counts[[\"y\", \"con\", \"la\"]][\"cuchara\"])"
      ],
      "metadata": {
        "id": "gMbVItd_W9yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lo mas frecuente despues de un ngrama dado:\n",
        "ngram_example = [\"con\", \"la\"]\n",
        "sorted(lm.counts[ngram_example].items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "id": "RWdQOqQ2XgIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilidad de un token luego de un ngrama:\n",
        "ngram_example = [\"con\", \"la\", \"salsa\"]\n",
        "print(lm.score(\"rosa\", ngram_example))"
      ],
      "metadata": {
        "id": "wYCpuypxYvaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos logscore para evitar underflow:\n",
        "import numpy as np\n",
        "\n",
        "ngram_example = [\"con\", \"la\", \"salsa\"]\n",
        "print(lm.logscore(\"rosa\", ngram_example))\n",
        "print(np.log2(lm.score(\"rosa\", ngram_example)))"
      ],
      "metadata": {
        "id": "7TGwR1JwY6nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación\n",
        "\n",
        "Medimos perplexity en el dataset de test."
      ],
      "metadata": {
        "id": "POfAiKjGHu8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_test = tokenized_test[33]\n",
        "print(example_test)\n",
        "print(lm.vocab.lookup(example_test))"
      ],
      "metadata": {
        "id": "56EnCPZ0ZvH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "def perplexity(docs, lm, ngram_order=3) -> float:\n",
        "    \"\"\"docs: lista de listas de tokens (con BOS y EOS)\n",
        "    \"\"\"\n",
        "    ngrams_flat = []\n",
        "    for doc in docs:\n",
        "        ngrams_ = ngrams(doc, ngram_order)\n",
        "        ngrams_flat.extend(list(ngrams_))\n",
        "    return lm.perplexity(ngrams_flat)"
      ],
      "metadata": {
        "id": "K9GInDjFeF5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ppl_train = perplexity(tokenized_train, lm)\n",
        "print(f\"Perplexity en train: {ppl_train:.4f}\")"
      ],
      "metadata": {
        "id": "Oqh39H7ihxB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ppl_test = perplexity(tokenized_test, lm)\n",
        "print(f\"Perplexity en test: {ppl_test:.4f}\")\n",
        "# qué pasó??"
      ],
      "metadata": {
        "id": "ZiT4a1ZbeX-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Necesitamos smoothing / backoff / interpolation para computar perplexity en test!\n",
        "\n",
        "Usamos add-k smoothing (aka Lidstone smoothing, gamma=k)"
      ],
      "metadata": {
        "id": "xl89hc5wwupP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Lidstone\n",
        "\n",
        "train, train_flat = padded_everygram_pipeline(4, tokenized_train)\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)\n",
        "lm_smoothed = Lidstone(order=4, vocabulary=vocab, gamma=.01)"
      ],
      "metadata": {
        "id": "vt7yslv1lvo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm_smoothed.fit(train)"
      ],
      "metadata": {
        "id": "Kc0A7B2enuXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppl_test_smoothed = perplexity(tokenized_test, lm_smoothed)\n",
        "print(f\"Perplexity en test: {ppl_test_smoothed:.4f}\")"
      ],
      "metadata": {
        "id": "3t2Px-Y2oARm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de texto\n",
        "\n",
        "Generamos texto sampleando iterativamente del LM."
      ],
      "metadata": {
        "id": "ddjpr1b0xD2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_generados = lm_smoothed.generate(\n",
        "    30, text_seed=[\"<bos>\", \"<bos>\", \"<bos>\"], random_seed=33)\n",
        "print(\" \".join(tokens_generados))"
      ],
      "metadata": {
        "id": "8Eu2KQHoo0bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "tokens_generados = lm_smoothed.generate(\n",
        "    120, text_seed=[\"<bos>\", \"<bos>\", \"1\"], random_seed=33)\n",
        "receta_generada = \" \".join(tokens_generados)\n",
        "print(textwrap.fill(receta_generada, 100))"
      ],
      "metadata": {
        "id": "Y8p8zhsuB1Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZMXy52FA9-o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}