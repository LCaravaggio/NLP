{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOln9kjg6Dzcly4LH2hn7yE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/06a_PytorchTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tarea: responder donde dice **PREGUNTA**"
      ],
      "metadata": {
        "id": "aqSPt0zZ7f4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ¿Qué es pytorch?"
      ],
      "metadata": {
        "id": "W-jAsdiBARGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PyTorch](https://pytorch.org/) es una librería de deep learning de código abierto basada en Python.\n",
        "\n",
        "Tiene tres componentes principales:\n",
        "\n",
        "* Es una librería de **tensores**, similar a numpy, pero con soporte para GPU.\n",
        "\n",
        "* Es un **motor de diferenciación automática** (autograd): calcula gradientes de funciones automáticamente, con el objetivo de simplificar el _backpropagation_ y la optimización de modelos.\n",
        "\n",
        "* Es una librería de **deep learning**: tiene módulos para diseñar y entrenar redes neuronales profundas--modelos preentrenados, funciones de pérdida, optimizadores, etc."
      ],
      "metadata": {
        "id": "XiH3JGRK2P6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalación"
      ],
      "metadata": {
        "id": "H26weXpmAb8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch se puede instalar como cualquier otra librería de Python.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Q8YF2TEL3rTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch watermark\n",
        "# usamos watermark para hacer un print de las versiones que usamos"
      ],
      "metadata": {
        "id": "PU-uoyvYAbZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "EwoBaLYxBkqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp torch"
      ],
      "metadata": {
        "id": "1mJkYMmiBve3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar GPU:\n",
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "3da1buRcBtev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si devuelve False, significa que no hay GPU compatible o no es reconocida.\n",
        "\n",
        "Aunque no es obligatorio tener GPU para usar PyTorch, **acelera enormemente** las operaciones matriciales que usamos en la inferencia y entrenamiento.\n",
        "\n",
        "En google colab, usar `Runtime > Change runtime type` para seleccionar una GPU."
      ],
      "metadata": {
        "id": "AbIPlU8J4lkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------\n",
        "\n",
        "Al ser una librería grande con soporte para GPU, la instalación puede requerir algunas consideraciones adicionales si la hacemos en otro entorno.\n",
        "\n",
        "En particular, pytorch con GPU requiere **CUDA**, un API desarrollada por NVIDIA que permite aprovechar la GPU no solo para gráficos, sino también para cómputo general.\n",
        "\n",
        "La instalación de CUDA puede ser problemática. Fuera de colab, puede ser útil crear un virtual environment para manejar esto. Por ejemplo de conda:\n",
        "\n",
        "```bash\n",
        "conda create --name my-env python=3.11 -y &&\n",
        "conda activate my-env &&\n",
        "conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia -y\n",
        "# pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126\n",
        "```\n",
        "\n",
        "También pueden ver en [la página oficial](https://pytorch.org) el comando adecuado según el sistema operativo y versión de CUDA.\n",
        "\n",
        "Respecto de la versión de Python: muchas librerías no soportan de inmediato la versión más reciente de Python, así que se recomienda usar una versión una o dos ediciones anteriores (por ejemplo, si la última es 3.13, usar 3.11 o 3.12). En este caso usamos la que viene por defecto en colab."
      ],
      "metadata": {
        "id": "AVg8LGra4IxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensores"
      ],
      "metadata": {
        "id": "mU8v4QpjApTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los **tensores** son un objeto matemático que **generaliza vectores y matrices** a dimensiones más altas. El orden o **rango** de los tensores indica el número de dimensiones.\n",
        "\n",
        "* Un escalar es un tensor de rango 0\n",
        "* Un vector es un tensor de rango 1\n",
        "* Una matriz es un tensor de rango 2\n",
        "* Tensores de orden superior se nombran como tensores 3D, 4D, etc.\n",
        "\n",
        "En computación, los tensores funcionan como **contenedores de datos multidimensionales**, donde cada dimensión representa una característica distinta.\n",
        "\n",
        "Con PyTorch podemos crearlos, manipularlos y hacer cálculos de forma eficiente. Funcionan como los _arrays_ de numpy, pero con funcionalidades adicionales útiles para hacer deep learning: **diferenciación automática**, y **soporte para GPU**."
      ],
      "metadata": {
        "id": "uxzfWPTv653h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de tensores\n",
        "\n",
        "tensor0d = torch.tensor(1)\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "tensor2d = torch.tensor([[1, 2], [3, 4]])\n",
        "tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])"
      ],
      "metadata": {
        "id": "E5VbcAbNAQpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipos de datos\n",
        "\n",
        "tensor1d = torch.tensor([1, 2, 3])\n",
        "print(tensor1d.dtype)\n",
        "\n",
        "floatvec = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "id": "Lu-Nfv49B69e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "np_array = np.array([1.0, 2.0, 3.0])\n",
        "print(np_array.dtype)"
      ],
      "metadata": {
        "id": "VJCxDsBZBDlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch usa por defecto float32 en lugar de float64 como numpy porque está optimizado para GPUs, donde **float32** es más rápido y consume menos memoria, y esta precisión es suficiente para hacer deep learning."
      ],
      "metadata": {
        "id": "BHrzTrYFBALA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar la precisión:\n",
        "\n",
        "floatvec = tensor1d.to(torch.float32)\n",
        "print(floatvec.dtype)"
      ],
      "metadata": {
        "id": "OozKBXTuB64e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de un tensor\n",
        "tensor2d = torch.tensor([[1, 2, 3],\n",
        "                         [4, 5, 6]])\n",
        "tensor2d"
      ],
      "metadata": {
        "id": "j-LZi0kxB6zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Consultar la forma (shape)\n",
        "\n",
        "print(tensor2d.shape)\n"
      ],
      "metadata": {
        "id": "ndpqHdYLCDtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 1** ¿Cuál es la diferencia entre los atributos .shape y .ndim?\n"
      ],
      "metadata": {
        "id": "Q7mqfMwEBVCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cambiar la forma\n",
        "\n",
        "tensor2d.view(3, 2)"
      ],
      "metadata": {
        "id": "yW1EavSHCDrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trasponer:\n",
        "\n",
        "tensor2d.T"
      ],
      "metadata": {
        "id": "yjJwaVNsCDo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiplicar matrices:\n",
        "\n",
        "print(tensor2d.matmul(tensor2d.T))\n",
        "print(tensor2d @ tensor2d.T)"
      ],
      "metadata": {
        "id": "ZLhPz19jCDmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Muchas veces pyotorch ofrece varias formas de hacer lo mismo, porque combina convenciones del antiguo Torch y de numpy. e.g. reshape y view, size y shape."
      ],
      "metadata": {
        "id": "UHKjyVTF9EKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelos y diferenciación automática"
      ],
      "metadata": {
        "id": "ShbPe2zRA2df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El motor de diferenciación automática de pytorch, **autograd**, se usa para calcular gradientes automáticamente en grafos computacionales.\n",
        "\n",
        "Un [**grafo computacional**](https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/) es un grafo dirigido que representa operaciones matemáticas. En deep learning, un grafo computacional representa la secuencia de **cálculos que se hacen para obtener la salida de una red neuronal**. Este grafo sirve para calcular los gradientes usados en **backpropagation**, el algoritmo de entrenamiento de redes neuronales.\n"
      ],
      "metadata": {
        "id": "Ef2vEb1f9k-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por ejemplo, en una regresión logística, que puede verse como una red neuronal de una sola capa, los cálculos pueden representarse como un grafo: la entrada o feature se multiplica por un peso, se suma un sesgo, se aplica una función de activación (sigmoide) y luego se compara con la etiqueta verdadera para calcular la pérdida."
      ],
      "metadata": {
        "id": "xUJbz8nG-5H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: regresión logística con un feature de entrada y una salida\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "y = torch.tensor([1.0])  # true label\n",
        "x1 = torch.tensor([1.1]) # entrada (feature)\n",
        "w1 = torch.tensor([2.2]) # weight\n",
        "b = torch.tensor([0.0])  # bias\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)     # activación y output\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "iz5U29NNAQmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation** aplica la regla de la cadena de derivadas \"de derecha a izquierda\" en el grafo, comenzando en la capa de salida y la pérdida, y yendo hacia atrás hasta la entrada. Esto permite calcular **cómo cambia la pérdida con respecto a cada parámetro** (pesos y sesgos), que a su vez sirve para **actualizar los parámetros** y mejorar el modelo durante el entrenamiento.\n",
        "\n",
        "* Las **derivadas parciales** miden cómo cambia una función respecto a una de sus variables.\n",
        "* Un **gradiente** es un vector que contiene todas las derivadas parciales de una función multivariable.\n",
        "* La **regla de la cadena** permite combinar estas derivadas en el grafo para obtener los gradientes.\n",
        "\n",
        "Si alguno de los nodos terminales que usamos tiene el atributo `requires_grad=True`, entonces se construye internamente un grafo computacional en segundo plano. Luego, al llamar a la función `grad`, podemos calcular el gradiente de la función de pérdida con respecto a cualquier parámetro del modelo."
      ],
      "metadata": {
        "id": "d1nvf_Tw_OuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.autograd import grad\n",
        "\n",
        "y = torch.tensor([1.0])\n",
        "x1 = torch.tensor([1.1])\n",
        "w1 = torch.tensor([2.2], requires_grad=True)\n",
        "b = torch.tensor([0.0], requires_grad=True)\n",
        "\n",
        "z = x1 * w1 + b\n",
        "a = torch.sigmoid(z)\n",
        "\n",
        "loss = F.binary_cross_entropy(a, y)\n",
        "\n",
        "grad_L_w1 = grad(loss, w1, retain_graph=True)\n",
        "grad_L_b = grad(loss, b, retain_graph=True)"
      ],
      "metadata": {
        "id": "P0cwrT36CdFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 2**: ¿cuáles son los parámetros o pesos en el ejemplo inmediatamente anterior?"
      ],
      "metadata": {
        "id": "VEM-60TuCWbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(grad_L_w1)\n",
        "print(grad_L_b)"
      ],
      "metadata": {
        "id": "_8R5bagqCeYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 3**: ¿qué representan los dos valores inmediatamente anteriores?"
      ],
      "metadata": {
        "id": "c212OmFjCl1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por defecto, PyTorch destruye el grafo computacional después de calcular los gradientes para liberar memoria. Si vamos a reutilizar ese grafo, podemos usar `retain_graph=True` para que se mantenga en memoria.\n",
        "\n",
        "En la práctica, no hace falta usar `grad` de manera \"manual\"--pytorch tiene herramientas de más alto nivel para automatizar este proceso. Podemos usar el **método `.backward()`** sobre la pérdida para calcular automáticamente los gradientes de todos los nodos hoja del grafo, almacenándolos en los atributos `.grad` de los tensores correspondientes."
      ],
      "metadata": {
        "id": "am2GhoNPBNW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w1.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "GLyjy8CgCfuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En resumen: **autograd** registra todas las operaciones con tensores, construye automáticamente el grafo computacional en segundo plano y, cuando llamamos al método `.backward()` sobre la pérdida, calcula automáticamente los gradientes de todos los parámetros involucrados. No necesitamos calcular derivadas ni gradientes a mano--pytorch se encarga de todo de forma automática."
      ],
      "metadata": {
        "id": "24M0jrCLAqxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep learning"
      ],
      "metadata": {
        "id": "SYJNTP3PA66H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al implementar una red neuronal en pytorch, normalmente instanciamos una subclase de la clase  `torch.nn.Module` para definir nuestra propia arquitectura personalizada. `Module` facilita la construcción y entrenamiento de modelos, por ejemplo, encapsulando capas y operaciones, y llevando un seguimiento de los pesos del modelo.\n",
        "\n",
        "Dentro de la subclase, definimos las capas de la red en el método `__init__` y especificamos cómo interactúan en el método **`forward`**. Este método define cómo los datos de entrada pasan a través de la red y se combinan en un grafo computacional."
      ],
      "metadata": {
        "id": "raCK8MCU4ATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de un perceptrón multicapa (MLP):\n",
        "\n",
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, num_inputs, num_outputs):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(num_inputs, 30),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(30, 20),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(20, num_outputs),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.layers(x)\n",
        "        return logits\n",
        "\n",
        "# Sequential() no es obligatorio pero ayuda a evitar código repetido cuando hay muchas operaciones\n",
        "# porque podemos llamar directamente a layers()"
      ],
      "metadata": {
        "id": "kv5DEjopAQj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una instancia de la clase\n",
        "model = NeuralNetwork(50, 3)"
      ],
      "metadata": {
        "id": "cbQsnY4nCjeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 4**: ¿Cuántas capas ocultas y de salida tiene este modelo?"
      ],
      "metadata": {
        "id": "CZkrEOGUDc5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resumen de la estructura del modelo:\n",
        "print(model)"
      ],
      "metadata": {
        "id": "lej5KZGrCmr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cantidad de parámetros del modelo\n",
        "num_params = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        ")\n",
        "print(\"Cantidad de parámetros entrenables:\", num_params)\n",
        "\n"
      ],
      "metadata": {
        "id": "cI5NwX9pCjb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 5** ¿qué significa que un parámetro es entrenable?"
      ],
      "metadata": {
        "id": "tBMINcy-Dj3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Acceder a una matriz de pesos específica:\n",
        "print(model.layers[0].weight)\n",
        "print(model.layers[0].weight.shape)"
      ],
      "metadata": {
        "id": "8r7-YnUUCqSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inicialización de pesos reproducible:\n",
        "torch.manual_seed(123)\n",
        "\n",
        "model = NeuralNetwork(50, 3)\n",
        "print(model.layers[0].weight)"
      ],
      "metadata": {
        "id": "q-i-tBHoCuLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 6**: ¿por qué se suelen inicializar de manera aleatoria los pesos de una red?"
      ],
      "metadata": {
        "id": "DiMR1rKkDrhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Método forward:\n",
        "torch.manual_seed(123)\n",
        "\n",
        "X = torch.rand((1, 50)) # entrada ficticia\n",
        "out = model(X) # forward\n",
        "print(out)"
      ],
      "metadata": {
        "id": "V85DONNLCveX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`grad_fn=...` indica la última función usada para calcular una variable en el grafo computacional.\n",
        "\n",
        "`<AddmmBackward0>` significa que el tensor se creó mediante una multiplicación de matrices (mm) y suma (add). Pytorch usa esta información cuando calcula gradientes durante la retropropagación."
      ],
      "metadata": {
        "id": "sMpcOit36usj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si solo queremos usar una red para hacer predicciones, no hace falta construir el grafo computacional para seguir los gradientes--esto implica cálculos innecesarios y consumo de memoria.\n",
        "\n",
        "Para desactivar estas operaciones en la **inferencia**, usamos el context manager `torch.no_grad()` o `torch.inference_mode()` para ahorrar memoria y cómputo."
      ],
      "metadata": {
        "id": "wR_2tLks7EX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = model(X)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "QZrXHrJVCxyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En problemas de clasificación, solemos programar los modelos para que devuelvan directamente los **logits** (las salidas de la última capa) sin pasarlas por una función de activación como softmax o sigmoidea.\n",
        "\n",
        "Esto es porque las funciones de pérdida de pytorch ya combinan la operación softmax con la cross entropy por eficiencia y estabilidad numérica.\n",
        "\n",
        "Entonces, para calcular probabilidades de pertenencia a clases para las predicciones, tenemos que usar explícitamente la función softmax."
      ],
      "metadata": {
        "id": "-61wpVG27McD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    out = torch.softmax(model(X), dim=1)\n",
        "print(out)"
      ],
      "metadata": {
        "id": "Vm_X0mrXCzm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 7**: ¿Cuál es la diferencia entre softmax y sigmoidea?"
      ],
      "metadata": {
        "id": "5MjoQEbEEMMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Procesamiento de datos\n",
        "\n",
        "Para entrenar modelos necesitamos maneras de cargar y procesar los datos. Hay tres componentes fundamentales para esto:\n",
        "\n",
        "* La clase `Dataset` para instanciar objetos que definen cómo se carga cada registro\n",
        "* La clase `DataLoader` para manejar cómo se mezclan los datos y se agrupan en tandas o **batches**\n",
        "* Una función `collate` que aplica procesamiento a los batches e.g. tokenización, padding, etc."
      ],
      "metadata": {
        "id": "LDRCG3VdBBaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Supongamos que ya tenemos features y labels listos para entrenar y evaluar\n",
        "X_train = torch.tensor([\n",
        "    [-1.2, 3.1],\n",
        "    [-0.9, 2.9],\n",
        "    [-0.5, 2.6],\n",
        "    [2.3, -1.1],\n",
        "    [2.7, -1.5]\n",
        "])\n",
        "y_train = torch.tensor([0, 0, 0, 1, 1])\n",
        "\n",
        "X_test = torch.tensor([\n",
        "    [-0.8, 2.8],\n",
        "    [2.6, -1.6],\n",
        "])\n",
        "y_test = torch.tensor([0, 1])\n",
        "\n",
        "# las etiquetas deben comenzar con 0, y el valor máximo no debe exceder el número de salidas menos 1"
      ],
      "metadata": {
        "id": "C4ARB5AgAQhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los tres componentes principales de un **`Dataset`** son:\n",
        "\n",
        "* En `__init__` configuramos atributos a los que podemos acceder más adelante en `__getitem__` y `__len__`. Estos atributos podrían ser rutas de archivos, objetos de archivos, conectores de bases de datos, etc. En este caso, como tenemos tensores en memoria, simplemente asignamos `X` y `y` a estos atributos.\n",
        "* En `__getitem__` definimos cómo devolver exactamente un elemento del dataset mediante un índice. Esto significa devolver los features y la etiqueta de clase de un solo ejemplo.\n",
        "* En `__len__` definimos cómo obtener la longitud del dataset."
      ],
      "metadata": {
        "id": "EiL0DPoqepNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ToyDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        one_x = self.features[index]\n",
        "        one_y = self.labels[index]\n",
        "        return one_x, one_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "train_ds = ToyDataset(X_train, y_train)\n",
        "test_ds = ToyDataset(X_test, y_test)"
      ],
      "metadata": {
        "id": "7lvhTnigC5MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "id": "Vg6SuvRVC-oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la clase **`DataLoader`** para tomar muestras e iterar sobre el dataset.\n",
        "\n",
        "El loader recorre todo el dataset de entrenamiento visitando cada ejemplo exactamente una vez. Esto se conoce como un **epoch** de entrenamiento.\n",
        "\n",
        "Si fijamos la semilla del generador aleatorio, obtenemos siempre el mismo orden de las muestras en la primera ejecución. Si iteramos una segunda vez sobre el dataset, el orden cambia--esto evita que las redes neuronales caigan en ciclos repetitivos durante el entrenamiento."
      ],
      "metadata": {
        "id": "juNJ8LbyfK0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_ds = ToyDataset(X_test, y_test)\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "f4EYlgQRC__m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ],
      "metadata": {
        "id": "mhGV2E78DCkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como usamos batch_size=2, el tercer batch solo contiene un ejemplo porque tenemos 5 ejemplos en total. En la práctica, tener un último batch más chico puede afectar la convergencia del entrenamiento--podemos eliminar el último batch de cada epoch con `drop_last=True`."
      ],
      "metadata": {
        "id": "3EFBry2afgfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset=train_ds,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    drop_last=True\n",
        ")"
      ],
      "metadata": {
        "id": "7Yg06FxDDEFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {idx+1}:\", x, y)"
      ],
      "metadata": {
        "id": "F5PCBsqMDFXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con `num_workers` podemos paralelizar la carga y el preprocesamiento de datos:\n",
        "\n",
        "* Cuando `num_workers=0`, la carga de datos ocurre en el proceso principal. Esto puede generar cuellos de botella durante el entrenamiento de modelos grandes en GPU porque la CPU debe cargar/preprocesar los datos mientras la GPU queda inactiva esperando.\n",
        "* Con `num_workers>0`, se lanzan varios procesos en paralelo para cargar los datos, permitiendo que el proceso principal se concentre en entrenar el modelo y aprovechar mejor los recursos. El loader puede ir preparando los siguientes batches en segundo plano.\n",
        "\n",
        "La configuración ideal depende del hardware y código específicos--`num_workers=4` es un buen punto de partida para datos y/o modelos grandes."
      ],
      "metadata": {
        "id": "FHdsBN_Tgzzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, la **`collate` function** (función de ensamblado) define cómo se combinan las muestras individuales de un dataset en un batch dentro del DataLoader. Es decir, el collator se encarga de cómo se ve cada batch cuando llega al entrenamiento.\n",
        "\n",
        "Por defecto, pytorch usa `default_collate`, que simplemente apila los tensores (stack) para formar un batch.\n",
        "\n",
        "En algunos casos los datos necesitan cierto preprocesamiento. Por ejemplo, si las muestras tienen longitudes variables podemos aplicar padding u otra transformación antes de pasar los datos al modelo.\n",
        "\n",
        "Veamos un ejemplo de juguete en el que rellenamos muestras de distinta longitud con 0s hasta que todas tengan la misma longitud dentro del lote."
      ],
      "metadata": {
        "id": "ixd6Imf3hb8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Dataset de ejemplo con secuencias de distinta longitud\n",
        "class VarLenDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.data = [\n",
        "            torch.tensor([1, 2, 3]),\n",
        "            torch.tensor([4, 5]),\n",
        "            torch.tensor([6]),\n",
        "            torch.tensor([7, 8, 9, 10])\n",
        "        ]\n",
        "        self.labels = torch.tensor([0, 1, 0, 1])  # etiquetas binarias\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Collator personalizado con padding\n",
        "def pad_collator(batch):\n",
        "    # batch = [(seq1, label1), (seq2, label2), ...]\n",
        "    sequences = [item[0] for item in batch]\n",
        "    labels = torch.tensor([item[1] for item in batch])\n",
        "    # Rellenar las secuencias al largo máximo del lote (padding)\n",
        "    padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
        "    # batch_first pone como primera dimensión el batch_size\n",
        "    return padded, labels\n",
        "\n",
        "dataset = VarLenDataset()\n",
        "loader = DataLoader(dataset, batch_size=2, collate_fn=pad_collator, shuffle=True)\n",
        "\n",
        "for idx, (x, y) in enumerate(loader):\n",
        "    print(f\"Batch {idx+1}:\")\n",
        "    print(\"Secuencias con padding:\\n\", x)\n",
        "    print(\"Labels:\", y, \"\\n\")"
      ],
      "metadata": {
        "id": "EsKByKksdeFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "RdEMbb1MBC7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un loop de entrenamiento clásico.\n",
        "\n",
        "* Usamos un **optimizador** de descenso de gradiente estocástico (SGD) con una **tasa de aprendizaje** (lr) de 0.5. La tasa de aprendizaje es un hiperparámetro i.e. en la práctica debemos ajustarla observando la pérdida.\n",
        "* El **número de épocas** es otro hiperparámetro a elegir.\n",
        "* A veces usamos un tercer conjunto de datos, de **validación**, para encontrar la configuración óptima de hiperparámetros. Normalmente usamos el set de validación muchas veces, cosa que no debemos hacer con el de test.\n",
        "* `model.train()` y `model.eval()` se usan para poner el modelo en **modo entrenamiento** y **modo evaluación**. Esto es necesario para componentes que se comportan de manera diferente durante el entrenamiento y la inferencia, como dropout o batch normalization. En este casos usarlos es redundante, pero es una buena práctica incluirlos siempre.\n",
        "* Pasamos los logits directamente a la **función de pérdida** cross_entropy, que ya aplica la función softmax internamente.\n",
        "* Al llamar **`loss.backward()`** se calculan los gradientes en el grafo computacional-\n",
        "* El método `optimizer.step()` usa los gradientes para **actualizar los parámetros del modelo** en pos de minimizar la pérdida. En el caso de SGD, esto significa multiplicar los gradientes por la tasa de aprendizaje y restarlo a los parámetros.\n",
        "* Es importante incluir una llamada a `optimizer.zero_grad()` en cada actualización para **reiniciar los gradientes** a cero--de lo contrario, los gradientes se acumulan, lo cual puede ser no deseado."
      ],
      "metadata": {
        "id": "USbBEcQq4FBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        logits = model(features)\n",
        "\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()\n",
        "    # Evaluación opcional acá"
      ],
      "metadata": {
        "id": "GAA2hDUHAQfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_train)\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "peorxkTFDLvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener las probabilidades\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "probas = torch.softmax(outputs, dim=1)\n",
        "print(probas)"
      ],
      "metadata": {
        "id": "CJnxCK3zDLr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener predicciones de etiquetas\n",
        "predictions = torch.argmax(probas, dim=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "wU77uULjDLo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.argmax(outputs, dim=1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "Iy69cuzwDOC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 8**: ¿Por qué da el mismo resultado hacer el argmax() sobre las probabilidades o los logits?"
      ],
      "metadata": {
        "id": "o2ZuKZlWG2f0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy en los datos de entrenamiento\n",
        "print(predictions == y_train)\n",
        "print(torch.sum(predictions == y_train))\n",
        "print(torch.mean((predictions == y_train).float()))"
      ],
      "metadata": {
        "id": "EccfpIU7DSk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementado como función para escalar a datasets de cualquier tamaño:\n",
        "\n",
        "def compute_accuracy(model, dataloader):\n",
        "\n",
        "    model = model.eval()\n",
        "    correct = 0.0\n",
        "    total_examples = 0\n",
        "\n",
        "    for idx, (features, labels) in enumerate(dataloader):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(features)\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        compare = labels == predictions\n",
        "        correct += torch.sum(compare)\n",
        "        total_examples += len(compare)\n",
        "\n",
        "    return (correct / total_examples).item()"
      ],
      "metadata": {
        "id": "9Ry2XrIbDSie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(model, train_loader)"
      ],
      "metadata": {
        "id": "HrGAZMKsDZMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_accuracy(model, test_loader)"
      ],
      "metadata": {
        "id": "db0NBZzEDaQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El `state_dict` de un modelo es un diccionario que mapea cada capa del modelo a sus parámetros entrenables. Para **guardar un modelo**, guardamos esto.\n"
      ],
      "metadata": {
        "id": "og0s3jY0mM11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"model.pt\")\n",
        "# \"model.pt\" es un nombre de archivo arbitrario\n",
        "# Podemos usar cualquier nombre y extensión -- .pth y .pt son las convenciones más comunes."
      ],
      "metadata": {
        "id": "jtHNEH_aDbc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con `torch.load(...)` leemos el archivo y reconstruimos el diccionario que contiene los parámetros del modelo, mientras que `model.load_state_dict()` aplica estos parámetros al modelo, restaurando su estado en el momento en que lo guardamos.\n",
        "\n",
        "Necesitamos una **instancia del modelo en memoria** para aplicar los parámetros guardados; i.e., la arquitectura `NeuralNetwork(2, 2)` debe coincidir exactamente con el modelo original guardado."
      ],
      "metadata": {
        "id": "JIkggzDemYy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(2, 2) # needs to match the original model exactly\n",
        "model.load_state_dict(torch.load(\"model.pt\", weights_only=True))"
      ],
      "metadata": {
        "id": "BoitSrWXDdGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de GPU"
      ],
      "metadata": {
        "id": "0uxuaeNzBLun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En pytorch, un **device** es el dispositivo donde ocurren los cálculos y están los datos. CPU y GPU son los device que solemos usar.\n",
        "\n",
        "Por defecto, pytorch usa CPU. Podemos usar el método `.to()` para mandar tensores a una GPU y hacer las operaciones ahí."
      ],
      "metadata": {
        "id": "n8KKueVgqbhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = torch.tensor([1., 2., 3.])\n",
        "tensor_2 = torch.tensor([4., 5., 6.])\n",
        "\n",
        "print(tensor_1 + tensor_2)"
      ],
      "metadata": {
        "id": "WTT-DWo0DgP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = tensor_1.to(\"cuda\")\n",
        "tensor_2 = tensor_2.to(\"cuda\")\n",
        "\n",
        "print(tensor_1 + tensor_2)"
      ],
      "metadata": {
        "id": "RXpxHIWQDgM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`device='cuda:0'` significa que los tensores están en la primera GPU. Si la máquina tiene varias GPUs, podemos especificar cuál GPU usar.\n",
        "\n",
        "Si los tensores están en devices distintos, rompe:"
      ],
      "metadata": {
        "id": "KUi8sxd2rIcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_1 = tensor_1.to(\"cpu\")\n",
        "print(tensor_1 + tensor_2)"
      ],
      "metadata": {
        "id": "XloQIdIwDgKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenando con GPU:"
      ],
      "metadata": {
        "id": "uCgqmYMArcke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = NeuralNetwork(num_inputs=2, num_outputs=2)\n",
        "\n",
        "# Definir device:\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Mandar modelo a device:\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
        "\n",
        "        # Mandar datos a device:\n",
        "        features, labels = features.to(device), labels.to(device)\n",
        "        logits = model(features)\n",
        "        loss = F.cross_entropy(logits, labels) # Loss function\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
        "              f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
        "              f\" | Train Loss: {loss:.2f}\")\n",
        "\n",
        "    model.eval()"
      ],
      "metadata": {
        "id": "NwTYSC9MDnwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso seguramente no haya mucha diferencia -- la **aceleración es abismal cuando usamos modelos grandes**.\n",
        "\n",
        "Cuando los modelos son demasiado grandes, se suelen usar múltiples GPUs en lugar de una sola. Eso es un poco difícil de aplicar en el contexto de una notebook."
      ],
      "metadata": {
        "id": "ifZMlOYxsGXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recursos:\n",
        "\n",
        "* https://sebastianraschka.com/teaching/pytorch-1h\n",
        "* https://docs.pytorch.org/tutorials"
      ],
      "metadata": {
        "id": "FNVNdN5fBRY3"
      }
    }
  ]
}