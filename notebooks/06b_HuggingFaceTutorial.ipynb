{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/06b_HuggingFaceTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38f3ff0",
      "metadata": {
        "id": "e38f3ff0"
      },
      "source": [
        "[**Hugging Face**](https://huggingface.co/docs) es un ecosistema de librerías que permite a desarrolladores y científicos compartir y usar recursos open-source de machine learning. Es particularmente popular en el campo del NLP.\n",
        "\n",
        "Vamos a hacer un _overview_ de los principales componentes de Hugging Face: tokenizadores, modelos, datasets y pipelines.\n",
        "\n",
        "Muchos de estos componentes son interfaces de alto nivel que por debajo usan pytorch.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Tarea: responder donde dice **PREGUNTA**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620f9801",
      "metadata": {
        "id": "620f9801"
      },
      "source": [
        "## Configuración del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9EhWoZef-X8u",
      "metadata": {
        "collapsed": true,
        "id": "9EhWoZef-X8u"
      },
      "outputs": [],
      "source": [
        "!pip install -qU transformers datasets watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NY0z9Izv-S5O",
      "metadata": {
        "id": "NY0z9Izv-S5O"
      },
      "outputs": [],
      "source": [
        "%load_ext watermark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gS0LHQ0j-XJ4",
      "metadata": {
        "id": "gS0LHQ0j-XJ4"
      },
      "outputs": [],
      "source": [
        "%watermark -vmp transformers,datasets,torch,numpy,pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43FLbwgz-X83",
      "metadata": {
        "id": "43FLbwgz-X83"
      },
      "source": [
        "## Tokenizadores\n",
        "\n",
        "Los modelos preentrenados se desarrollan junto con **tokenizadores**: toman strings sin procesar y devuelven diccionarios con los **inputs del modelo**.\n",
        "\n",
        "Cada **token** es un número entero que se corresponde con una **palabra en el vocabulario** del modelo.\n",
        "\n",
        "Con `AutoTokenizer` podemos cargar **tokenizers pre-entrenados**. Para ver cómo entrenar un tokenizador con BPE desde cero, ver: https://huggingface.co/learn/nlp-course/en/chapter6/8?fw=pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pu6L0lWG-X83",
      "metadata": {
        "id": "Pu6L0lWG-X83",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "nH_R0fzZ5-2l"
      },
      "id": "nH_R0fzZ5-2l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zrPzbBhR-X84",
      "metadata": {
        "id": "zrPzbBhR-X84"
      },
      "outputs": [],
      "source": [
        "input_str = \"These pretzels are making me thirsty!\"\n",
        "tokenized_input = tokenizer(input_str)\n",
        "\n",
        "print(\"> Tokenizer input:\")\n",
        "print(input_str)\n",
        "print(\"-\"*70)\n",
        "print(\"> Tokenizer output:\")\n",
        "print(tokenized_input)\n",
        "print(\"-\"*70)\n",
        "print(\"> Tokenizer output (input IDs):\")\n",
        "print(tokenized_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63pW6IfEGX43",
      "metadata": {
        "id": "63pW6IfEGX43"
      },
      "source": [
        "Veamos lo que sucede por debajo paso a paso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E_8C6L2G-X85",
      "metadata": {
        "id": "E_8C6L2G-X85"
      },
      "outputs": [],
      "source": [
        "def tokenize_step_by_step(input_str):\n",
        "    input_tokens = tokenizer.tokenize(input_str)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "    cls = [tokenizer.cls_token_id]\n",
        "    sep = [tokenizer.sep_token_id]\n",
        "    input_ids_special_tokens = cls + input_ids + sep\n",
        "    decoded_str = tokenizer.decode(input_ids_special_tokens)\n",
        "    print(\"input:                  \", input_str)\n",
        "    print(\"tokenize:               \", input_tokens)\n",
        "    print(\"convert_tokens_to_ids:  \", input_ids)\n",
        "    print(\"add special tokens:     \", input_ids_special_tokens)\n",
        "    print(\"-\"*70)\n",
        "    print(\"decode (IDs to strings):\", decoded_str)\n",
        "\n",
        "tokenize_step_by_step(input_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deaa04d9",
      "metadata": {
        "id": "deaa04d9"
      },
      "source": [
        "**PREGUNTA 9** ¿Qué cambia cuando pasamos una oración en castellano o árabe vs inglés? ¿Por qué?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a2598c",
      "metadata": {
        "id": "78a2598c"
      },
      "outputs": [],
      "source": [
        "x = \"Quiero aprender a usar modelos de lenguaje\"\n",
        "\n",
        "tokenize_step_by_step(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"أريد أن أتعلم استخدام نماذج اللغة\"\n",
        "\n",
        "tokenize_step_by_step(x)"
      ],
      "metadata": {
        "id": "iwIm5ThM7GYa"
      },
      "id": "iwIm5ThM7GYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "h8sQ_r6BMWNc",
      "metadata": {
        "id": "h8sQ_r6BMWNc"
      },
      "source": [
        "Cuando entrenamos modelos o hacemos inferencia, vamos a querer:\n",
        "\n",
        "* trabajar con **_batches_**, pasando muchas secuencias simultáneamente como input\n",
        "* trabajar con **tensores** de PyTorch, no con listas\n",
        "\n",
        "**PREGUNTA 10** ¿para qué sirve usar batches?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7121a02d",
      "metadata": {
        "id": "7121a02d"
      },
      "outputs": [],
      "source": [
        "input_strings = [\n",
        "    \"These pretzels are making me thirsty!\",\n",
        "    \"I am speechless! I am without speech.\",\n",
        "    \"No more soup for you!\",\n",
        "    \"I'm a wealthy industrialist and philanthropist and a bicyclist.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ed25c60",
      "metadata": {
        "id": "4ed25c60"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(\n",
        "    input_strings, return_tensors=\"pt\", padding='longest', truncation=True,\n",
        "    max_length=tokenizer.model_max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bdf2842",
      "metadata": {
        "id": "7bdf2842"
      },
      "source": [
        "Lo que hicimos recién es:\n",
        "\n",
        "- Tokenizar todas las frases\n",
        "- Devolver los tensores en formato PyTorch (\"pt\") en un tensor _rectangular_\n",
        "- Truncar las frases más largas para que no excedan el tamaño máximo admitido por el modelo\n",
        "- Rellenar con _padding_ hasta el máximo largo del batch para que todas las entradas tengan la misma longitud\n",
        "\n",
        "Cuando entrenamos modelos o hacemos inferencia, a veces es recomendable tokenizar cada batch on-the-fly en lugar de pretokenizar -- esto permite experimentar más rápido con muchos datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MlCFICig925P",
      "metadata": {
        "id": "MlCFICig925P"
      },
      "outputs": [],
      "source": [
        "print(f\"Max model length: {tokenizer.model_max_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4897dab",
      "metadata": {
        "id": "f4897dab"
      },
      "outputs": [],
      "source": [
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "print(f\"Pad token ID: {tokenizer.pad_token_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXhvA6CJMlEC",
      "metadata": {
        "id": "YXhvA6CJMlEC"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(\n",
        "    input_strings, return_tensors=\"pt\", padding='longest', truncation=True,\n",
        "    max_length=tokenizer.model_max_length)\n",
        "\n",
        "print(\"Batch encode:\")\n",
        "print([f\"{k}: {v.shape}\" for k, v in model_inputs.items()])\n",
        "print(model_inputs[\"input_ids\"])\n",
        "print(model_inputs[\"attention_mask\"])\n",
        "print(\"-\"*70)\n",
        "print(\"Batch decode:\")\n",
        "print(*tokenizer.batch_decode(model_inputs.input_ids, skip_special_tokens=False), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6juLjnNt-X87",
      "metadata": {
        "id": "6juLjnNt-X87"
      },
      "source": [
        "## Modelos\n",
        "\n",
        "Los modelos suelen tener un **body** y **head**.\n",
        "\n",
        "* El \"body\" son los **pesos preentrenados** que devuelven una representación de la secuencia de input.\n",
        "* El \"head\" son los pesos adicionales que dependen de la **tarea específica** que estamos resolviendo.\n",
        "\n",
        "Con las clases `AutoModel...` podemos cargar un modelo preentrenado y agregarle un head específico para nuestra tarea.\n",
        "\n",
        "```\n",
        "AutoModel # (solo hidden states, sin head)\n",
        "AutoModelForCausalLM\n",
        "AutoModelForMaskedLM\n",
        "AutoModelForSequenceClassification\n",
        "AutoModelForTokenClassification\n",
        "# etc\n",
        "```\n",
        "\n",
        "Vamos a cargar un BERT \"destilado\" para hacer **clasificación binaria de secuencias**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RXm1K2sF-X88",
      "metadata": {
        "id": "RXm1K2sF-X88",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-cased', num_labels=2, id2label={0: \"A\", 1: \"B\"}, label2id={\"A\": 0, \"B\": 1})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1opFV7Vi-X88",
      "metadata": {
        "id": "1opFV7Vi-X88"
      },
      "source": [
        "Esto quiere decir que estamos agregando una **capa de clasificación** con **dos salidas** al final del modelo preentrenado.\n",
        "\n",
        "El warning nos dice que los pesos de esta capa todavía no fueron entrenados. Es decir, necesitamos hacer fine-tuning sobre un dataset específico para que tenga sentido usarlo.\n",
        "\n",
        "**_Solo a modo ilustrativo_**, vamos a hacer **inferencia** sobre una frase de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f865f4b4",
      "metadata": {
        "id": "f865f4b4"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccca7be5",
      "metadata": {
        "id": "ccca7be5"
      },
      "outputs": [],
      "source": [
        "param_names = [n for n, p in model.named_parameters()]\n",
        "\n",
        "print(f\"# de 'capas': {len(param_names)}\")\n",
        "print(param_names[:3])\n",
        "print(param_names[-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TDZ72k-U-X89",
      "metadata": {
        "id": "TDZ72k-U-X89"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input_str = \"These pretzels are making me thirsty!\"\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "model.eval() # eval mode: desactiva componentes random, como dropout\n",
        "with torch.inference_mode(): # inference mode: desactiva cómputo de gradientes\n",
        "    model_outputs = model(**model_inputs)\n",
        "\n",
        "print(\"Inputs:\")\n",
        "print(model_inputs)\n",
        "print(\"-\"*70)\n",
        "print(\"Outputs:\")\n",
        "print(model_outputs)\n",
        "print(f\"Logits: {model_outputs.logits}\")\n",
        "print(f\"Probabilidades: {torch.softmax(model_outputs.logits, dim=1)}\")\n",
        "pred = torch.argmax(model_outputs.logits).item()\n",
        "print(f\"Predicción: {model.config.id2label[pred]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205f77fd",
      "metadata": {
        "id": "205f77fd"
      },
      "source": [
        "Si tuviésemos labels, podemos usar pytorch para entrenar i.e. actualizar los pesos del modelo para optimizar la loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-E1ibEHWTu_G",
      "metadata": {
        "id": "-E1ibEHWTu_G"
      },
      "outputs": [],
      "source": [
        "input_str = \"These pretzels are making me thirsty!\"\n",
        "\n",
        "model_inputs = tokenizer(input_str, return_tensors=\"pt\")\n",
        "model.train()\n",
        "model_outputs = model(**model_inputs)\n",
        "label = torch.tensor([1])\n",
        "loss = torch.nn.functional.cross_entropy(model_outputs.logits, label)\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "loss.backward() # Computa gradientes\n",
        "# optimizer.step() # Si quisieramos actualizar los pesos con un optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si los labels están en el input, podemos obtener la loss automáticamente:"
      ],
      "metadata": {
        "id": "XNcmN2KX_qPy"
      },
      "id": "XNcmN2KX_qPy"
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs['labels'] = torch.tensor([1]) # label de ejemplo\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    model_outputs = model(**model_inputs)\n",
        "\n",
        "print(f\"Logits: {model_outputs.logits}\")\n",
        "print(f\"Probabilidades: {torch.softmax(model_outputs.logits, dim=1)}\")\n",
        "print(f\"Loss: {model_outputs.loss:.4f}\")"
      ],
      "metadata": {
        "id": "aCpJoFgb_w_8"
      },
      "id": "aCpJoFgb_w_8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 11** ¿por qué puede diferir la loss en las dos celdas anteriores?"
      ],
      "metadata": {
        "id": "-W-5FGZpwaSj"
      },
      "id": "-W-5FGZpwaSj"
    },
    {
      "cell_type": "markdown",
      "id": "kMSLugHFWsE4",
      "metadata": {
        "id": "kMSLugHFWsE4"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "HF también tiene [datasets](https://huggingface.co/datasets) open-source que podemos usar para entrenar y evaluar nuestros modelos.\n",
        "\n",
        "Hay [muchas funcionalidades](https://huggingface.co/docs/datasets/process) para leer y modificar la estructura y contenido de un dataset (e.g. streaming, split de datos, reordenar filas, cambiar nombres de columnas, eliminar columnas, transformar ejemplos, concatenar datasets, etc.)\n",
        "\n",
        "Vamos a cargar un dataset de [reviews de películas](https://huggingface.co/datasets/rotten_tomatoes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cqawKxuibaId",
      "metadata": {
        "id": "cqawKxuibaId"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"rotten_tomatoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w8Uj3SfSdNbh",
      "metadata": {
        "id": "w8Uj3SfSdNbh"
      },
      "outputs": [],
      "source": [
        "# filas y columnas\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].features.items()"
      ],
      "metadata": {
        "id": "RHiIUNrcl0Rm"
      },
      "id": "RHiIUNrcl0Rm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oLu6387RUGwV",
      "metadata": {
        "id": "oLu6387RUGwV"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][3]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# En general es útil armar una función para mapear de IDs a labels de la variable respuesta\n",
        "label_names = dataset[\"train\"].features[\"label\"].names\n",
        "label2id = {name: dataset[\"train\"].features[\"label\"].str2int(name) for name in label_names}\n",
        "id2label = {id: label for label, id in label2id.items()}\n",
        "\n",
        "id_example = dataset[\"train\"][3][\"label\"]\n",
        "print(f\"Label ID: {id_example}\")\n",
        "print(f\"Label: {id2label[id_example]}\")"
      ],
      "metadata": {
        "id": "7pNYnSQNl69G"
      },
      "id": "7pNYnSQNl69G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "jzkwMXRadZrM",
      "metadata": {
        "id": "jzkwMXRadZrM",
        "outputId": "69317cb1-f7da-4d3f-e301-ba029307b504"
      },
      "source": [
        "A modo de ejemplo, vamos a limpiar cualquier caracter HTML que pueda haber en las reviews y truncar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xmqKp2p5W6eO",
      "metadata": {
        "id": "xmqKp2p5W6eO"
      },
      "outputs": [],
      "source": [
        "import html\n",
        "\n",
        "def truncate(examples, max_length=50):\n",
        "    \"\"\"Recibe un diccionario con los nombres de las columnas como keys\n",
        "    Como lo vamos a aplicar en batches, cada value del dict es una lista con los\n",
        "    valores de esa columna\n",
        "    \"\"\"\n",
        "    return {\n",
        "        'text': [html.unescape(text[:max_length]) for text in examples['text']],\n",
        "        # 'label': ... # si quisieramos modificar el label\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ejemplo:\n",
        "truncate(dataset[\"train\"][:4])"
      ],
      "metadata": {
        "id": "sdnNMZyYB71Q"
      },
      "id": "sdnNMZyYB71Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UK0tgU9mdMMY",
      "metadata": {
        "id": "UK0tgU9mdMMY"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: truncate(x, max_length=50), batched=True)\n",
        "# batch_size default es 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRU54B_LW6eP",
      "metadata": {
        "id": "nRU54B_LW6eP"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-obwpZ9OW6eP",
      "metadata": {
        "id": "-obwpZ9OW6eP"
      },
      "outputs": [],
      "source": [
        "dataset['train'][3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tHI3KuNZ-X8w",
      "metadata": {
        "id": "tHI3KuNZ-X8w"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "Hay tareas estándar de NLP para las que ya hay **modelos preentrenados y fine-tuneados**. HF los disponibiliza a través de la interfaz de [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "Por ejemplo para **clasificación de sentimiento**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gOj5ODS0-X8x",
      "metadata": {
        "collapsed": true,
        "id": "gOj5ODS0-X8x"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analysis = pipeline(\n",
        "    \"sentiment-analysis\", model=\"siebert/sentiment-roberta-large-english\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Vkio5ttF7rb",
      "metadata": {
        "id": "5Vkio5ttF7rb"
      },
      "outputs": [],
      "source": [
        "sentiment_analysis(\"Change is inevitable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd9d76b",
      "metadata": {
        "id": "7fd9d76b"
      },
      "outputs": [],
      "source": [
        "sentiment_analysis(\"Change is inevitable\", top_k=None) # devuelve los scores de todas las clases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LQKGFDNEgeRv",
      "metadata": {
        "id": "LQKGFDNEgeRv"
      },
      "outputs": [],
      "source": [
        "input_strings = [\n",
        "    \"These pretzels are making me thirsty!\",\n",
        "    \"I am speechless! I am without speech.\",\n",
        "    \"No more soup for you!\",\n",
        "    \"I'm a wealthy industrialist and philanthropist and a bicyclist.\"\n",
        "]\n",
        "outputs = sentiment_analysis(input_strings)\n",
        "\n",
        "for i, output in enumerate(outputs):\n",
        "    print(f\"Input: {input_strings[i]}\")\n",
        "    print(f\"Sentiment: {output['label']}, score: {output['score']:.4f}\")\n",
        "    print(\"-\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptc0BViy-X80"
      },
      "source": [
        "O para [NER](https://huggingface.co/dslim/bert-base-NER):"
      ],
      "id": "Ptc0BViy-X80"
    },
    {
      "cell_type": "code",
      "source": [
        "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\")"
      ],
      "metadata": {
        "id": "zKt3xKpsh-q_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "id": "zKt3xKpsh-q_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner.model.config.id2label)"
      ],
      "metadata": {
        "id": "atCqhzTdmR_K"
      },
      "execution_count": null,
      "outputs": [],
      "id": "atCqhzTdmR_K"
    },
    {
      "cell_type": "code",
      "source": [
        "ner_string = (\n",
        "    \"In Mendoza, José de San Martín met with representatives of the Supreme Court\"\n",
        "    \" of Argentina after the Boca Juniors victory in the Copa Libertadores, while\"\n",
        "    \" the Ministry of Culture, J. Mendoza, highlighted the influence of tango as a UNESCO\"\n",
        "    \" Intangible Cultural Heritage in San Martín, Buenos Aires.\"\n",
        ")"
      ],
      "metadata": {
        "id": "rY-w9m2Yixxx"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rY-w9m2Yixxx"
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = ner(ner_string)\n",
        "for entity in outputs:\n",
        "    print(entity)"
      ],
      "metadata": {
        "id": "mHg6KpNQic04"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mHg6KpNQic04"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}