{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/notebooks/08a_BERTEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a analizar los embeddings que devuelve BERT.\n",
        "\n",
        "-----------------------\n",
        "\n",
        "Tarea: responder donde dice **PREGUNTA**"
      ],
      "metadata": {
        "id": "fx_t8wcyxAGC"
      },
      "id": "fx_t8wcyxAGC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del entorno"
      ],
      "metadata": {
        "id": "3xq35ODSxIG6"
      },
      "id": "3xq35ODSxIG6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9EhWoZef-X8u"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets bertviz watermark"
      ],
      "id": "9EhWoZef-X8u"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "from bertviz import head_view, model_view\n",
        "from bertviz.neuron_view import show\n",
        "from scipy.spatial.distance import cosine"
      ],
      "metadata": {
        "id": "Bnjvs8fi-KAv"
      },
      "id": "Bnjvs8fi-KAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "NY0z9Izv-S5O"
      },
      "id": "NY0z9Izv-S5O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp transformers,datasets,pandas,numpy"
      ],
      "metadata": {
        "id": "gS0LHQ0j-XJ4"
      },
      "id": "gS0LHQ0j-XJ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qué mirás, BERT?\n",
        "\n",
        "Como vimos, BERT fue entrenado para Masked Language Modeling (_aka_ [fill-mask](https://huggingface.co/tasks/fill-mask) en HF).\n",
        "\n",
        "Vamos a ver cómo le va en eso.\n"
      ],
      "metadata": {
        "id": "e1gV0l3aXY4d"
      },
      "id": "e1gV0l3aXY4d"
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "bert = AutoModelForMaskedLM.from_pretrained(\n",
        "    \"bert-base-cased\", output_attentions=True, output_hidden_states=True)"
      ],
      "metadata": {
        "id": "oDXL_Gt9nlPl"
      },
      "id": "oDXL_Gt9nlPl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_tokenizer.mask_token) # <mask>"
      ],
      "metadata": {
        "id": "rTRaFgxgo_CQ"
      },
      "id": "rTRaFgxgo_CQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_mlm = [\n",
        "    \"These [MASK] are making me thirsty!\",\n",
        "    \"These pretzels are making me [MASK]!\",\n",
        "    \"These songs are making me [MASK]!\",\n",
        "    \"I am [MASK]! I am without speech.\",\n",
        "    \"[MASK] more soup for you! NEXT!\",\n",
        "    \"I'm a [MASK] industrialist and philanthropist and a bicyclist.\"\n",
        "]"
      ],
      "metadata": {
        "id": "m_pttdxDo6Cd"
      },
      "id": "m_pttdxDo6Cd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask(input_str):\n",
        "    \"\"\"Tomamos el camino largo en lugar de usar pipeline\n",
        "    \"\"\"\n",
        "    inputs = bert_tokenizer(input_str, return_tensors=\"pt\")\n",
        "    mask_index = np.where(inputs['input_ids'] == bert_tokenizer.mask_token_id)\n",
        "    # .eval() to set dropout and batch normalization layers to evaluation mode\n",
        "    bert.eval()\n",
        "    outputs = bert(**inputs)\n",
        "    top_5_predictions = torch.softmax(outputs.logits[mask_index], dim=1).topk(5)\n",
        "    for i in range(5):\n",
        "        token = bert_tokenizer.decode(top_5_predictions.indices[0, i])\n",
        "        prob = top_5_predictions.values[0, i]\n",
        "        print(f\" {i+1}) {token:<20} {prob:.3f}\")"
      ],
      "metadata": {
        "id": "9iAzFe2eqARw"
      },
      "id": "9iAzFe2eqARw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in input_mlm:\n",
        "    print(x)\n",
        "    out = predict_mask(x)\n",
        "    print(\"-\"*70)"
      ],
      "metadata": {
        "id": "NWHJ6A-HquHn"
      },
      "id": "NWHJ6A-HquHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 1** ¿Cómo genera el modelo las probabilidades de la palabra enmascarada?"
      ],
      "metadata": {
        "id": "YNu_cqTFzMNP"
      },
      "id": "YNu_cqTFzMNP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos analizar los hidden states y los attention scores.\n",
        "\n",
        "Para esto está muy bueno [BertViz](https://github.com/jessevig/bertviz) pero también lo podemos hacer a mano."
      ],
      "metadata": {
        "id": "FlEgQQneu7om"
      },
      "id": "FlEgQQneu7om"
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert)"
      ],
      "metadata": {
        "id": "p10DcuZ3v6go"
      },
      "id": "p10DcuZ3v6go",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos consultar todos los pesos del modelo con:\n",
        "state_dict = bert.state_dict()\n",
        "list(state_dict.keys())[:5]\n",
        "\n",
        "# o con named_parameters()"
      ],
      "metadata": {
        "id": "BJ5sXacoxRai"
      },
      "id": "BJ5sXacoxRai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embeddings tensor shape:\")\n",
        "print(state_dict[\"bert.embeddings.word_embeddings.weight\"].shape)\n",
        "print(\"Position embeddings tensor shape:\")\n",
        "print(state_dict[\"bert.embeddings.position_embeddings.weight\"].shape)"
      ],
      "metadata": {
        "id": "nM8bReZgyyZ2"
      },
      "id": "nM8bReZgyyZ2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 2** ¿Cómo se interpretan las dimensiones de los dos tensores anteriores?"
      ],
      "metadata": {
        "id": "6Q44NvMY0GYw"
      },
      "id": "6Q44NvMY0GYw"
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = '\"I voted for Obama because he was most aligned with my values\", Mary said.'"
      ],
      "metadata": {
        "id": "dK9ryADsveBt"
      },
      "id": "dK9ryADsveBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = bert_tokenizer(input_str, return_tensors=\"pt\")\n",
        "bert.eval()\n",
        "with torch.inference_mode():\n",
        "    model_output = bert(**model_inputs)"
      ],
      "metadata": {
        "id": "FdsINvyOuPoX"
      },
      "id": "FdsINvyOuPoX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# hidden states = {len(model_output.hidden_states)}\")\n",
        "# initial embeddings + 12 transf. blocks"
      ],
      "metadata": {
        "id": "e3dOUf_OzSuT"
      },
      "id": "e3dOUf_OzSuT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 3** ¿Qué información tiene el último hidden state del modelo?"
      ],
      "metadata": {
        "id": "bzrH1jMGzgda"
      },
      "id": "bzrH1jMGzgda"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of each hidden state:\")\n",
        "print(model_output.hidden_states[1].shape) # (bsz, tokens, dim)"
      ],
      "metadata": {
        "id": "rUB9vebhzTld"
      },
      "id": "rUB9vebhzTld",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of each attention tensor:\")\n",
        "print(model_output.attentions[0].shape) # (bsz, head, query_word, key_word)"
      ],
      "metadata": {
        "id": "mPdyCYt5zuY3"
      },
      "id": "mPdyCYt5zuY3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 4** ¿Cómo se interpreta el tamaño del tensor anterior?"
      ],
      "metadata": {
        "id": "aEgEjcJX0mKT"
      },
      "id": "aEgEjcJX0mKT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos cómo extraer los contextual word embeddings (CWE) -- sin el [feature extractor de HF](https://huggingface.co/tasks/feature-extraction)."
      ],
      "metadata": {
        "id": "XwiFf49SLx5A"
      },
      "id": "XwiFf49SLx5A"
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(model_output.hidden_states))\n",
        "print(model_output.hidden_states[0].shape)"
      ],
      "metadata": {
        "id": "9x7TBK7B2CUS"
      },
      "id": "9x7TBK7B2CUS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cwes(model_output):\n",
        "    \"\"\"Contextual embeddings como la suma de last 4 layers\n",
        "    \"\"\"\n",
        "    # stack los 13 states en un solo tensor\n",
        "    embeddings = torch.stack(model_output.hidden_states, dim=0)\n",
        "    #print(embeddings.shape)\n",
        "    # drop dimension de batches:\n",
        "    embeddings = torch.squeeze(embeddings, dim=1)\n",
        "    #print(embeddings.shape)\n",
        "    # sum last 4 layers\n",
        "    embeddings = embeddings[-4:].sum(dim=0)\n",
        "    #print(embeddings.shape)\n",
        "    return embeddings\n",
        "\n",
        "def extract_bert_cwe(input_str, target_word):\n",
        "    \"\"\"Extract BERT CWE of a specific token in input_str\n",
        "    \"\"\"\n",
        "    model_inputs = bert_tokenizer(input_str, return_tensors=\"pt\")\n",
        "    target_position = model_inputs.tokens().index(target_word)\n",
        "    bert.eval()\n",
        "    with torch.inference_mode():\n",
        "        model_output = bert(**model_inputs)\n",
        "    embedding = get_cwes(model_output)[target_position]\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "qmu5JKr02CWy"
      },
      "id": "qmu5JKr02CWy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analicemos la similitud de \"values\" en cada contexto\n",
        "input_strings = [\n",
        "    '\"I voted for Obama because he was most aligned with my values\", Mary said.',\n",
        "    'Find the values of x and y in x+y=8',\n",
        "    'I believe in the values of liberal democracy.',\n",
        "]"
      ],
      "metadata": {
        "id": "BUA3mvL0A0tx"
      },
      "id": "BUA3mvL0A0tx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = []\n",
        "for input_ in input_strings:\n",
        "    emb_ = extract_bert_cwe(input_, \"values\")\n",
        "    word_embeddings.append(emb_)"
      ],
      "metadata": {
        "id": "83w-AAavI2lf"
      },
      "id": "83w-AAavI2lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_embeddings)"
      ],
      "metadata": {
        "id": "BAN3DmzTvFui"
      },
      "id": "BAN3DmzTvFui",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_ = torch.cosine_similarity(word_embeddings[0], word_embeddings[1], dim=0).item()\n",
        "print(f'Cosine sim. entre \"values\" de')\n",
        "print(f\"  {input_strings[0]}\")\n",
        "print(f\"  {input_strings[1]}\")\n",
        "print(f\"{cos_:.4f}\")"
      ],
      "metadata": {
        "id": "Np72xct5Cr_K"
      },
      "id": "Np72xct5Cr_K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_ = torch.cosine_similarity(word_embeddings[0], word_embeddings[2], dim=0).item()\n",
        "print(f'Cosine sim. entre \"values\" de')\n",
        "print(f\"  {input_strings[0]}\")\n",
        "print(f\"  {input_strings[2]}\")\n",
        "print(f\"{cos_:.4f}\")"
      ],
      "metadata": {
        "id": "9Bnn7WGIAs31"
      },
      "id": "9Bnn7WGIAs31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREGUNTA 5** ¿Por qué obtenemos un valor más alto en el segundo caso que en el primero?"
      ],
      "metadata": {
        "id": "IzRWlhN01C3T"
      },
      "id": "IzRWlhN01C3T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "De acuerdo a [What Does BERT Look At? (Clark et al, 2019)](https://arxiv.org/abs/1906.04341) las correferencias tienden a estar captadas en los heads 4-5.\n"
      ],
      "metadata": {
        "id": "nvQpag1T96jX"
      },
      "id": "nvQpag1T96jX"
    },
    {
      "cell_type": "code",
      "source": [
        "# attention from one token (left) to another (right)\n",
        "tokens = bert_tokenizer.convert_ids_to_tokens(model_inputs.input_ids[0])\n",
        "head_view(model_output.attentions, tokens)"
      ],
      "metadata": {
        "id": "lZyU3ibS13cW"
      },
      "id": "lZyU3ibS13cW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">¿Por qué `[SEP]` recibe tanta atención?\n",
        ">\n",
        ">Una hipótesis es que funciona como un default cuando no aplica una función de un head (por ej, si un head representa objetos directos que prestan atención a verbos, tal vez los sustantivos en este head prestan atención a [SEP]).\n",
        ">\n",
        ">En definitiva, para hacer análisis, a veces conviene no tener en cuenta este token.\n",
        ">\n",
        ">Ver https://arxiv.org/pdf/1906.04341.pdf."
      ],
      "metadata": {
        "id": "zFVGn4RLO9fL"
      },
      "id": "zFVGn4RLO9fL"
    },
    {
      "cell_type": "code",
      "source": [
        "# BertViz show() está buenisimo pero no funciona bien para cualquier modelo\n",
        "# ver https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing#scrollTo=-QnRteSLP0Hm"
      ],
      "metadata": {
        "id": "NxeaeP_O6TIb"
      },
      "id": "NxeaeP_O6TIb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "Generales:\n",
        "\n",
        "* [HuggingFace Docs](https://huggingface.co/docs/transformers/index)\n",
        "* [HuggingFace Course](https://huggingface.co/course/)\n",
        "* [HuggingFace Book](https://transformersbook.com/) (Tunstall et al, 2022)\n",
        "\n",
        "Específicas:\n",
        "\n",
        "* HuggingFace tutorial de [Stanford CS224n](http://web.stanford.edu/class/cs224n/)\n",
        "* [Entrenar tu propio tokenizer](https://huggingface.co/docs/tokenizers/quicktour)\n",
        "* [Cargar tu propio dataset](https://huggingface.co/docs/datasets/loading)\n",
        "* [Streaming de large datasets](https://huggingface.co/course/chapter5/4?fw=pt)\n",
        "* [HF pipeline overview](https://huggingface.co/course/chapter2/2?fw=pt)\n"
      ],
      "metadata": {
        "id": "ZADhLirc9iCT"
      },
      "id": "ZADhLirc9iCT"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sr9rPER29vbE"
      },
      "id": "Sr9rPER29vbE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}