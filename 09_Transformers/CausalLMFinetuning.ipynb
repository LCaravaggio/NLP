{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/09_Transformers/LMFinetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning en language modeling\n",
        "\n",
        "Vamos a usar [**GPT-2**](https://huggingface.co/docs/transformers/model_doc/gpt2):\n",
        "\n",
        "* Es un LM (causal) de transformers\n",
        "* Datos de entrenamiento: _WebText_ (scraping de links que salen de reddit con al menos 3 upvotes)\n",
        "* Tokenizador: subword tokenization con BPE (Byte Pair Encoding)\n",
        "\n",
        "Aunque en realidad vamos a usar una versión _destilada_: **distilled-GPT2**.\n",
        "\n",
        "_Knowledge distillation_ es un proceso que entrena una versión reducida de un modelo más grande al que se intenta imitar, con el objetivo de acelerar el procesamiento y el finetuning en tareas específicas, sacrificando poca performance (ver https://arxiv.org/pdf/1910.01108v4.pdf y https://arxiv.org/pdf/2006.05525.pdf).\n"
      ],
      "metadata": {
        "id": "XdHoV9NWEYWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MOsHUjgdIrIW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets==2.19.0 transformers==4.40.1 watermark # wandb\n",
        "#!pip install datasets==2.14.5 transformers==4.34.0 accelerate==0.23.0 watermark\n",
        "#!pip install datasets==2.19.0 transformers==4.40.1 accelerate==0.29.3 watermark # wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KMo0Waw5TOpp"
      },
      "outputs": [],
      "source": [
        "import re, os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "#import wandb\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from IPython.display import display, HTML\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling,\n",
        "    Trainer, TrainingArguments\n",
        ")\n",
        "from transformers.trainer_callback import PrinterCallback\n",
        "from transformers.utils.notebook import NotebookProgressCallback"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "TriIEyrSShEY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp transformers,datasets,pandas,numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtVteVkSVY5",
        "outputId": "42edbcb4-7246-4691-a275-58df0e1f03fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "transformers: 4.40.1\n",
            "datasets    : 2.19.0\n",
            "pandas      : 2.0.3\n",
            "numpy       : 1.25.2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r_n9OWV3l-Q"
      },
      "source": [
        "## Data\n",
        "\n",
        "Cargamos [reviews de yelp](https://huggingface.co/datasets/yelp_review_full). Vamos a usar solo algunos ejemplos para trabajar más rápido.\n",
        "\n",
        "Para cargar un dataset propio ver https://huggingface.co/docs/datasets/loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n2ZRs1cL3l-R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e43e1ab-8499-43c9-e1f2-071ea49527f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BgnA0GBcX72",
        "outputId": "6e85c01c-9e15-43d6-9a3e-90a79787611a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 650000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS1lJ50NWnNk",
        "outputId": "badf6abb-4ceb-413a-9b25-2b483dd98ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('label', ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None))\n",
            "('text', Value(dtype='string', id=None))\n"
          ]
        }
      ],
      "source": [
        "print(*dataset[\"train\"].features.items(), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v8n4-z4dfNTt"
      },
      "outputs": [],
      "source": [
        "# 10k train, 2k validation, 5k test\n",
        "small_dataset = DatasetDict(\n",
        "    train=dataset[\"train\"].shuffle(seed=33).select(range(0, 10_000)),\n",
        "    val=dataset[\"train\"].shuffle(seed=33).select(range(10_000, 12_000)),\n",
        "    test=dataset[\"test\"].shuffle(seed=33).select(range(5_000)),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Por qué podríamos necesitar tres sets?"
      ],
      "metadata": {
        "id": "ZeGsDsz9w5Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNkldiNYTNWr",
        "outputId": "18471144-627b-400f-e1c6-bb7ff10c8dc0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Love this place. Stayed in February for 4 days and after spending 4 days at Red Rock, I decided I had to write a review to compliment the service and amenities of this hotel\\n\\nPluses: \\nClean, nice front desk staff, great kitchenette, spacious, quiet, no smell of smoke because the casinos are down the walkway at the MGM, nice little bar for late night drinks (better if it was open later), good breakfast at the sandwich shop downstairs, convenient casino/restaurants/shopping at the MGM but none of the noise and seediness because the Signature is set apart. Also, reasonably priced even booking through the hotel.\\n\\nMinuses:\\nWould be nice to have a little store with kitchen/breakfast essentials in the hotel to make good use of the kitchenette.\\n\\nI just stayed at the Red Rock where the front desk service was abysmal. When you only have 4 days at a hotel, it makes a big difference when the staff smile and make you feel welcome and try to address concerns efficiently and effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(example):\n",
        "    \"\"\"Corrige caracteres raros segun la doc de yelp\n",
        "    \"\"\"\n",
        "    texto = re.sub(r'\\\\n', '\\n', example[\"text\"]) # real newlines\n",
        "    texto = re.sub(r'\\\\\"', '\"', texto) # comillas de verdad\n",
        "    example[\"text\"] = texto\n",
        "    return example"
      ],
      "metadata": {
        "id": "mGt4HgDcgtW0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset = small_dataset.map(clean_text)"
      ],
      "metadata": {
        "id": "RJL1jaYyf59d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(small_dataset[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXzDCnjijY7V",
        "outputId": "b955fbf9-731e-49a6-dfc1-99c790ca86ae"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Love this place. Stayed in February for 4 days and after spending 4 days at Red Rock, I decided I had to write a review to compliment the service and amenities of this hotel\n",
            "\n",
            "Pluses: \n",
            "Clean, nice front desk staff, great kitchenette, spacious, quiet, no smell of smoke because the casinos are down the walkway at the MGM, nice little bar for late night drinks (better if it was open later), good breakfast at the sandwich shop downstairs, convenient casino/restaurants/shopping at the MGM but none of the noise and seediness because the Signature is set apart. Also, reasonably priced even booking through the hotel.\n",
            "\n",
            "Minuses:\n",
            "Would be nice to have a little store with kitchen/breakfast essentials in the hotel to make good use of the kitchenette.\n",
            "\n",
            "I just stayed at the Red Rock where the front desk service was abysmal. When you only have 4 days at a hotel, it makes a big difference when the staff smile and make you feel welcome and try to address concerns efficiently and effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEA1ju653l-p"
      },
      "source": [
        "## Tokenización y modelo\n",
        "\n",
        "El max_length admitido por el modelo es 1024 pero esto puede consumir muchísima memoria. Entonces vamos a trabajar con un max_length de 128 tokens.\n",
        "\n",
        "En particular, vamos a partir cada documento en pedazos de 128 tokens. Vamos a tener algunos pedazos con menos de 128 porque hay documentos que no llegan a esta cantidad, y también por los pedazos que queden al final de documentos largos.\n",
        "\n",
        "Para poder hacer un procesamiento en batches vamos a necesitar _padding_: completar con un token especial hasta llegar al max_length o a la máxima longitud del batch.\n",
        "\n",
        "Una alternativa es truncar los documentos con más de 128 tokens pero si tenemos muchos documentos largos esto puede implicar tirar mucha información.\n",
        "\n",
        "Vamos a cargar el tokenizador y los pesos de un modelo pre-entrenado: a esto se le llama **checkpoint**. En este caso, la arquitectura es GPT-2 Distilled, mientras que el checkpoint (los pesos específicos) se llama `distilgpt2`.\n",
        "\n",
        "Vamos a cargar tokenizer y modelo con `AutoClass`es que permiten cargar checkpoints de cualquier arquitectura rápidamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-WGBCO343l-q"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"distilgpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iAYlS40Z3l-v"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# https://huggingface.co/docs/transformers/main_classes/tokenizer#tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length # Hay solo model_max_length embeddings de posicion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDxjK49Hsuj0",
        "outputId": "60e211e2-e48c-49a6-d931-5a31d7279e73"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context_length = tokenizer.model_max_length\n",
        "context_length = 128"
      ],
      "metadata": {
        "id": "Hd210WXnIHq5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos cómo funciona la tokenización en 3 ejemplos\n",
        "ejemplos = small_dataset[\"train\"][:3]\n",
        "ejemplos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwOCgzIVIeQR",
        "outputId": "b0876735-89c7-466b-da7f-79b0b0fed85f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': [4, 2, 1],\n",
              " 'text': ['Love this place. Stayed in February for 4 days and after spending 4 days at Red Rock, I decided I had to write a review to compliment the service and amenities of this hotel\\n\\nPluses: \\nClean, nice front desk staff, great kitchenette, spacious, quiet, no smell of smoke because the casinos are down the walkway at the MGM, nice little bar for late night drinks (better if it was open later), good breakfast at the sandwich shop downstairs, convenient casino/restaurants/shopping at the MGM but none of the noise and seediness because the Signature is set apart. Also, reasonably priced even booking through the hotel.\\n\\nMinuses:\\nWould be nice to have a little store with kitchen/breakfast essentials in the hotel to make good use of the kitchenette.\\n\\nI just stayed at the Red Rock where the front desk service was abysmal. When you only have 4 days at a hotel, it makes a big difference when the staff smile and make you feel welcome and try to address concerns efficiently and effectively.',\n",
              "  \"I just wanted to add an update for sound quality for the bands... sometimes it's really bad and sometimes it can't be beat....  \\n\\nAgain, the friendliness and service are always right on- great atmosphere.\",\n",
              "  \"A friend told me about this place, and how it was the best burger place in Waterloo. He oversold it. I had the deep fried bacon with bacon mayo as an appetizer and the Dead Ringer burger. \\n\\nThe deep fried bacon was really overdone. The coating on the bacon was really airy and flakey, although really crispy. Apparently it wasn't supposed to be like that, so I'm definitely disappointed with the consistency of the restaurant. The bacon mayo didn't add much to the deep fried bacon. As for the burger, it was also a letdown. It consisted of a fried onion ring with smoked brisket and BBQ sauce. I decided to get an elk patty as well. The burger was on the dry side. The patty was slightly overcooked there definitely wasn't enough BBQ sauce. I also didn't get much gaminess from the elk. Since it was my first time having elk, I'm unsure if it was supposed to taste as it did, but I felt in this case, a regular beef patty would have sufficed.\\n\\nIt would have been nice to get a more well executed version of the dishes. I'm sure it would have been better. But because of the consistency of food I got this time, I'm not sure I'll return. We'll see how it goes...\"]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_ = tokenizer(\n",
        "    ejemplos[\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True, # tokeniza doc y lo parte en pedazos\n",
        "    return_length=True, # computa length de cada doc\n",
        ")"
      ],
      "metadata": {
        "id": "RJ0OJ2zXkT0p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# como ouput obtenemos token_ids y attention_mask\n",
        "# por el momento solo vamos a usar token_ids\n",
        "outputs_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kKAozrvNrYx",
        "outputId": "1d7b291a-5100-4c20-9da4-baa62b9b7595"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[18565, 428, 1295, 13, 16160, 276, 287, 3945, 329, 604, 1528, 290, 706, 4581, 604, 1528, 379, 2297, 4631, 11, 314, 3066, 314, 550, 284, 3551, 257, 2423, 284, 19370, 262, 2139, 290, 35468, 286, 428, 7541, 198, 198, 3646, 2664, 25, 220, 198, 32657, 11, 3621, 2166, 6915, 3085, 11, 1049, 9592, 5857, 11, 40894, 11, 5897, 11, 645, 8508, 286, 7523, 780, 262, 39855, 389, 866, 262, 2513, 1014, 379, 262, 49182, 11, 3621, 1310, 2318, 329, 2739, 1755, 11758, 357, 27903, 611, 340, 373, 1280, 1568, 828, 922, 12607, 379, 262, 20433, 6128, 34624, 11, 11282, 21507, 14, 2118, 2899, 1187, 14, 1477, 33307, 379, 262, 49182, 475, 4844, 286, 262, 7838, 290, 9403, 1272, 780, 262, 34894, 318, 900, 5475, 13, 4418, 11, 13025], [19744, 772, 25452, 832, 262, 7541, 13, 198, 198, 9452, 2664, 25, 198, 17353, 307, 3621, 284, 423, 257, 1310, 3650, 351, 9592, 14, 9032, 7217, 41954, 287, 262, 7541, 284, 787, 922, 779, 286, 262, 9592, 5857, 13, 198, 198, 40, 655, 9658, 379, 262, 2297, 4631, 810, 262, 2166, 6915, 2139, 373, 450, 893, 7617, 13, 1649, 345, 691, 423, 604, 1528, 379, 257, 7541, 11, 340, 1838, 257, 1263, 3580, 618, 262, 3085, 8212, 290, 787, 345, 1254, 7062, 290, 1949, 284, 2209, 4786, 18306, 290, 6840, 13], [40, 655, 2227, 284, 751, 281, 4296, 329, 2128, 3081, 329, 262, 11760, 986, 3360, 340, 338, 1107, 2089, 290, 3360, 340, 460, 470, 307, 4405, 1106, 220, 220, 198, 198, 15316, 11, 262, 1545, 26061, 290, 2139, 389, 1464, 826, 319, 12, 1049, 8137, 13], [32, 1545, 1297, 502, 546, 428, 1295, 11, 290, 703, 340, 373, 262, 1266, 26593, 1295, 287, 36782, 13, 679, 625, 24120, 340, 13, 314, 550, 262, 2769, 23018, 21385, 351, 21385, 743, 78, 355, 281, 16422, 7509, 290, 262, 5542, 371, 3889, 26593, 13, 220, 198, 198, 464, 2769, 23018, 21385, 373, 1107, 14904, 505, 13, 383, 26749, 319, 262, 21385, 373, 1107, 1633, 88, 290, 781, 539, 88, 11, 3584, 1107, 42807, 13, 18626, 340, 2492, 470, 4385, 284, 307, 588, 326, 11, 523, 314, 1101, 4753, 11679, 351, 262, 15794, 286, 262, 7072, 13, 383, 21385, 743, 78, 1422, 470, 751, 881, 284, 262, 2769, 23018, 21385, 13, 1081, 329, 262, 26593, 11, 340, 373, 635, 257, 1309, 2902, 13, 632, 19954, 286, 257, 23018], [21670, 5858, 351, 21603, 35984, 316, 290, 32083, 10746, 13, 314, 3066, 284, 651, 281, 1288, 74, 1458, 774, 355, 880, 13, 383, 26593, 373, 319, 262, 5894, 1735, 13, 383, 1458, 774, 373, 4622, 25676, 46288, 612, 4753, 2492, 470, 1576, 32083, 10746, 13, 314, 635, 1422, 470, 651, 881, 9106, 1272, 422, 262, 1288, 74, 13, 4619, 340, 373, 616, 717, 640, 1719, 1288, 74, 11, 314, 1101, 22147, 611, 340, 373, 4385, 284, 6938, 355, 340, 750, 11, 475, 314, 2936, 287, 428, 1339, 11, 257, 3218, 12023, 1458, 774, 561, 423, 424, 2108, 276, 13, 198, 198, 1026, 561, 423, 587, 3621, 284, 651, 257, 517, 880, 10945, 2196, 286, 262, 16759, 13, 314, 1101, 1654, 340, 561, 423, 587, 1365, 13, 887, 780], [286, 262, 15794, 286, 2057, 314, 1392, 428, 640, 11, 314, 1101, 407, 1654, 314, 1183, 1441, 13, 775, 1183, 766, 703, 340, 2925, 986]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'length': [128, 91, 46, 128, 128, 25], 'overflow_to_sample_mapping': [0, 0, 1, 2, 2, 2]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cantidad de chunks: {len(outputs_['input_ids'])}\")\n",
        "print(f\"Tokens en cada chunk: {(outputs_['length'])}\")\n",
        "print(f\"Mapping chunk-doc: {outputs_['overflow_to_sample_mapping']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ifX03WTlZGj",
        "outputId": "78326e14-72a1-4ceb-8f4e-2fb0edf6944a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de chunks: 6\n",
            "Tokens en cada chunk: [128, 91, 46, 128, 128, 25]\n",
            "Mapping chunk-doc: [0, 0, 1, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con tokenize() obtenemos la separación en subwords\n",
        "tokens_ = tokenizer.tokenize(ejemplos[\"text\"][0])\n",
        "print(tokens_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6kMlO1ANLEJ",
        "outputId": "c7a21e11-1ae8-43d9-f0bb-b34804df3154"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Love', 'Ġthis', 'Ġplace', '.', 'ĠStay', 'ed', 'Ġin', 'ĠFebruary', 'Ġfor', 'Ġ4', 'Ġdays', 'Ġand', 'Ġafter', 'Ġspending', 'Ġ4', 'Ġdays', 'Ġat', 'ĠRed', 'ĠRock', ',', 'ĠI', 'Ġdecided', 'ĠI', 'Ġhad', 'Ġto', 'Ġwrite', 'Ġa', 'Ġreview', 'Ġto', 'Ġcompliment', 'Ġthe', 'Ġservice', 'Ġand', 'Ġamenities', 'Ġof', 'Ġthis', 'Ġhotel', 'Ċ', 'Ċ', 'Pl', 'uses', ':', 'Ġ', 'Ċ', 'Clean', ',', 'Ġnice', 'Ġfront', 'Ġdesk', 'Ġstaff', ',', 'Ġgreat', 'Ġkitchen', 'ette', ',', 'Ġspacious', ',', 'Ġquiet', ',', 'Ġno', 'Ġsmell', 'Ġof', 'Ġsmoke', 'Ġbecause', 'Ġthe', 'Ġcasinos', 'Ġare', 'Ġdown', 'Ġthe', 'Ġwalk', 'way', 'Ġat', 'Ġthe', 'ĠMGM', ',', 'Ġnice', 'Ġlittle', 'Ġbar', 'Ġfor', 'Ġlate', 'Ġnight', 'Ġdrinks', 'Ġ(', 'better', 'Ġif', 'Ġit', 'Ġwas', 'Ġopen', 'Ġlater', '),', 'Ġgood', 'Ġbreakfast', 'Ġat', 'Ġthe', 'Ġsandwich', 'Ġshop', 'Ġdownstairs', ',', 'Ġconvenient', 'Ġcasino', '/', 'rest', 'aur', 'ants', '/', 'sh', 'opping', 'Ġat', 'Ġthe', 'ĠMGM', 'Ġbut', 'Ġnone', 'Ġof', 'Ġthe', 'Ġnoise', 'Ġand', 'Ġseed', 'iness', 'Ġbecause', 'Ġthe', 'ĠSignature', 'Ġis', 'Ġset', 'Ġapart', '.', 'ĠAlso', ',', 'Ġreasonably', 'Ġpriced', 'Ġeven', 'Ġbooking', 'Ġthrough', 'Ġthe', 'Ġhotel', '.', 'Ċ', 'Ċ', 'Min', 'uses', ':', 'Ċ', 'Would', 'Ġbe', 'Ġnice', 'Ġto', 'Ġhave', 'Ġa', 'Ġlittle', 'Ġstore', 'Ġwith', 'Ġkitchen', '/', 'break', 'fast', 'Ġessentials', 'Ġin', 'Ġthe', 'Ġhotel', 'Ġto', 'Ġmake', 'Ġgood', 'Ġuse', 'Ġof', 'Ġthe', 'Ġkitchen', 'ette', '.', 'Ċ', 'Ċ', 'I', 'Ġjust', 'Ġstayed', 'Ġat', 'Ġthe', 'ĠRed', 'ĠRock', 'Ġwhere', 'Ġthe', 'Ġfront', 'Ġdesk', 'Ġservice', 'Ġwas', 'Ġab', 'ys', 'mal', '.', 'ĠWhen', 'Ġyou', 'Ġonly', 'Ġhave', 'Ġ4', 'Ġdays', 'Ġat', 'Ġa', 'Ġhotel', ',', 'Ġit', 'Ġmakes', 'Ġa', 'Ġbig', 'Ġdifference', 'Ġwhen', 'Ġthe', 'Ġstaff', 'Ġsmile', 'Ġand', 'Ġmake', 'Ġyou', 'Ġfeel', 'Ġwelcome', 'Ġand', 'Ġtry', 'Ġto', 'Ġaddress', 'Ġconcerns', 'Ġefficiently', 'Ġand', 'Ġeffectively', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# el tokenizer de gpt2 trata a los espacios como parte de las palabras,\n",
        "# entonces codifica distinto a las palabras en el medio vs el principio de la\n",
        "# secuencia\n",
        "# https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2Tokenizer\n",
        "\n",
        "print(tokenizer.tokenize(\"Love this place\"))\n",
        "print(tokenizer(\"Love this place\")['input_ids'])\n",
        "print(tokenizer.tokenize(\" Love this place\"))\n",
        "print(tokenizer(\" Love this place\")['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WS1iebfOPqH",
        "outputId": "e2bd6b7e-d297-4758-c89e-d70db0fad499"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Love', 'Ġthis', 'Ġplace']\n",
            "[18565, 428, 1295]\n",
            "['ĠLove', 'Ġthis', 'Ġplace']\n",
            "[5896, 428, 1295]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_fn(example):\n",
        "    \"\"\"Tokeniza `text` de examples de un dataset.\n",
        "    Returns only input_ids.\n",
        "    \"\"\"\n",
        "    outputs = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    return {\"input_ids\": outputs[\"input_ids\"]}"
      ],
      "metadata": {
        "id": "uI_t441IRFk2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicamos la tokenizacion en batches y 4 procesos para acelerar la corrida\n",
        "    # descartamos el resto de columnas\n",
        "tokenized_dataset = small_dataset.map(\n",
        "    tokenize_fn, batched=True, num_proc=4,\n",
        "    remove_columns=small_dataset[\"train\"].column_names)\n",
        "\n",
        "# NOTE: si queremos conservar mas columnas, tenemos que generar la misma\n",
        "# cantidad de datos que en el output (esta tokenizacion genera mas samples\n",
        "# que la cantidad inicial de examples)"
      ],
      "metadata": {
        "id": "LQRKPv1qPGJ7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnzj6XbpoPIt",
        "outputId": "5b89b599-49ec-49dc-9aad-3cb8a6855702"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 10000\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['label', 'text'],\n",
              "        num_rows: 5000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOD9phXhP-pP",
        "outputId": "52c09050-2ced-47f4-c1fb-281d6acd9dcd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 18555\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 3799\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 9280\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el modelo\n",
        "    # Usamos el EOS token as PAD token to avoid warnings (GPT2 does not have a PAD token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint, pad_token_id=tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "nvit8xFYSZK4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
        "# numel: number of elements in tensor\n",
        "\n",
        "# gpt3 tiene 175B params, gpt4 tiene 1T..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np2ITqLySOD8",
        "outputId": "60e56342-850e-4899-ebdc-bf8722c2a9a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 81.9M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axD5FaXUmkm-",
        "outputId": "758aa421-b81e-462e-c5cd-712c55675f03"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEmeQ7Xm3l_H"
      },
      "source": [
        "## Entrenamiento\n",
        "\n",
        "Un \"collator\" es una función que forma batches de datos.\n",
        "\n",
        "Vamos a usar un \"collator\" que arma batches de ejemplos con padding. `DataCollatorForLanguageModeling` está diseñado específicamente para language models.\n",
        "\n",
        "En particular se encarga de:\n",
        "\n",
        "* armar los targets del modelo (los tokens desplazados) _on the fly_ durante el entrenamiento sin duplicar los input_ids.\n",
        "* Agregar padding donde corresponda\n",
        "\n",
        "Usamos `mlm=False` para usar **Causal Language Modeling** en lugar de Masked Language Modeling.\n",
        "\n",
        "Podemos loguear métricas durante el entrenamiento con tensorboard, wandb, etc."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# el padding se hace con el EOS token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "HwoiDTP6U457"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un ejemplo con un batch de 3 docs\n",
        "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(3)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6GxNu8NU4uk",
        "outputId": "937d117e-2306-4987-d992-df98053f2ccc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([3, 128])\n",
            "attention_mask shape: torch.Size([3, 128])\n",
            "labels shape: torch.Size([3, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hay padding:\n",
        "out[\"input_ids\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5OorNSLVdWc",
        "outputId": "445ac112-87cc-49d9-8caf-8604fda8a24d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([19744,   772, 25452,   832,   262,  7541,    13,   198,   198,  9452,\n",
              "         2664,    25,   198, 17353,   307,  3621,   284,   423,   257,  1310,\n",
              "         3650,   351,  9592,    14,  9032,  7217, 41954,   287,   262,  7541,\n",
              "          284,   787,   922,   779,   286,   262,  9592,  5857,    13,   198,\n",
              "          198,    40,   655,  9658,   379,   262,  2297,  4631,   810,   262,\n",
              "         2166,  6915,  2139,   373,   450,   893,  7617,    13,  1649,   345,\n",
              "          691,   423,   604,  1528,   379,   257,  7541,    11,   340,  1838,\n",
              "          257,  1263,  3580,   618,   262,  3085,  8212,   290,   787,   345,\n",
              "         1254,  7062,   290,  1949,   284,  2209,  4786, 18306,   290,  6840,\n",
              "           13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
              "        50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention mask para no hacer attention sobre pad_tokens:\n",
        "out[\"attention_mask\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0j4su0LVdN-",
        "outputId": "33864acb-e6ed-4a17-dd10-adba8f97b7a4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos solo el nombre del modelo para el nuevo nombre (no el usuario)\n",
        "pretrained_model_name = model_checkpoint.split(\"/\")[-1]\n",
        "finetuned_model_name = f\"{pretrained_model_name}-finetuned-yelp\"\n",
        "print(finetuned_model_name)"
      ],
      "metadata": {
        "id": "CXI-S4Rxilm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f259407b-e29f-4173-b93e-a39653293162"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distilgpt2-finetuned-yelp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si vamos a usar wndb, copiamos API key de https://wandb.ai/authorize"
      ],
      "metadata": {
        "id": "-FZLWKTS_7K_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wandb login"
      ],
      "metadata": {
        "id": "LxBx0coy7vx3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#os.environ[\"WANDB_PROJECT\"] = project_name"
      ],
      "metadata": {
        "id": "RfadvKi47pxV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definimos los parametros del entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    finetuned_model_name,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-4,\n",
        "    weight_decay=0.1, # forma de regularizacion (restringe el tamaño de updates de SGD)\n",
        "    warmup_ratio=0.1, # # warmup evita divergencia de loss en primeros steps (10%)\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    do_eval=True, # eval en validation set\n",
        "    gradient_accumulation_steps=1, # acumula gradientes por N steps --> update cada N*32 samples\n",
        "    # sirve cuando batches grandes no entran en memoria y tenemos muchos samples\n",
        "    evaluation_strategy=\"steps\", # eval en validation set\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True, # conserva mejor modelo segun eval loss\n",
        "    save_total_limit=2, # save max 2 models including best one\n",
        "    save_steps=50, # checkpoint model every N steps\n",
        "    logging_dir='./logs', # logging\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    fp16=True, # float16 en training (only on CUDA)\n",
        "    push_to_hub=False,\n",
        "#    report_to=\"wandb\",  # enable logging to W&B\n",
        "    save_safetensors=False # por un bug\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"], #.select(range(0, 128)),\n",
        "    eval_dataset=tokenized_dataset[\"val\"], #.select(range(0, 128)),\n",
        ")"
      ],
      "metadata": {
        "id": "7HAQ9gpajSRi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "e_UzidtN_y1p"
      },
      "outputs": [],
      "source": [
        "#!rm -rf ./logs # para wandb/tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YbSwEhQ63l_L"
      },
      "outputs": [],
      "source": [
        "#%reload_ext tensorboard\n",
        "#%tensorboard --logdir logs\n",
        "\n",
        "# para wandb/tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos!\n",
        "train_output = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "lbtIwaS_RhK6",
        "outputId": "aeb2518d-a871-45d4-fc88-192003431236"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='580' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [580/580 06:25, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.102400</td>\n",
              "      <td>3.934660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.065900</td>\n",
              "      <td>3.938583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.945400</td>\n",
              "      <td>3.893298</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.813100</td>\n",
              "      <td>3.858455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.892600</td>\n",
              "      <td>3.811519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>4.116500</td>\n",
              "      <td>3.783123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.063500</td>\n",
              "      <td>3.754841</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.705500</td>\n",
              "      <td>3.731509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.774200</td>\n",
              "      <td>3.712565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.752000</td>\n",
              "      <td>3.701443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.803800</td>\n",
              "      <td>3.696855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to save model:\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "mHTwgvO2YqNu"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMD2qhItpeA-"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBBGs9-XDUPP",
        "outputId": "353062fe-ab58-45ec-ca73-b354364c852b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=580, training_loss=3.8968764609303967, metrics={'train_runtime': 387.5864, 'train_samples_per_second': 47.873, 'train_steps_per_second': 1.496, 'total_flos': 606045150904320.0, 'train_loss': 3.8968764609303967, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# volvemos a calcular loss en train porque train_output.training_loss\n",
        "# se calcula con criterio distinto a trainer.evaluate()\n",
        "train_results = trainer.evaluate(tokenized_dataset[\"train\"])\n",
        "val_results = trainer.evaluate()\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "S6mr2CoiAgdQ",
        "outputId": "d1aa570c-d2dd-48d9-b096-b36fa9803fda"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='989' max='580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [580/580 01:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBm9ODF7QOV1",
        "outputId": "c9e9c582-f259-4bb7-8945-a0648579f4fb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.4339005947113037,\n",
              " 'eval_runtime': 63.5472,\n",
              " 'eval_samples_per_second': 291.988,\n",
              " 'eval_steps_per_second': 9.127,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAHxowUgA3LE",
        "outputId": "b2fa0de6-8f82-4294-b721-008756ae018a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.696854829788208,\n",
              " 'eval_runtime': 12.9842,\n",
              " 'eval_samples_per_second': 292.587,\n",
              " 'eval_steps_per_second': 9.165,\n",
              " 'epoch': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perplexity:\")\n",
        "\n",
        "print(f\"Train: {np.exp(train_results['eval_loss']):.2f}\")\n",
        "print(f\"Validation: {np.exp(val_results['eval_loss']):.2f}\")\n",
        "print(f\"Test: {np.exp(test_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVVvdO0K_7IO",
        "outputId": "1ad2a4e4-6ab1-4e30-c6f0-27d1e0a96a54"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity:\n",
            "Train: 31.00\n",
            "Validation: 40.32\n",
            "Test: 39.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# comparamos con el GPT2 no fine-tuneado\n",
        "    # un poco hackoso, instanciamos un trainer pero no vamos a entrenar\n",
        "    # es solo para replicar exactamente la evaluacion anterior, sería\n",
        "    # mejor armar una funcion adhoc\n",
        "model_original = AutoModelForCausalLM.from_pretrained(\n",
        "    model_checkpoint, pad_token_id=tokenizer.eos_token_id)\n",
        "trainer_aux = Trainer(\n",
        "    model=model_original,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"], #.select(range(0, 128)),\n",
        "    eval_dataset=tokenized_dataset[\"test\"], #.select(range(0, 128)),\n",
        ")"
      ],
      "metadata": {
        "id": "rFif8fQuRzta"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results_original = trainer_aux.evaluate(tokenized_dataset[\"test\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2RuxGVwfSD1u",
        "outputId": "e5f5ff21-b9fe-4053-897a-5ac116525a1e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [290/290 00:31]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Perplexity (no fine-tuning):\")\n",
        "\n",
        "print(f\"Test: {np.exp(test_results_original['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_GOu4xYSq_g",
        "outputId": "80f77547-0a59-47a3-be33-2fa9481b9bfc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity (no fine-tuning):\n",
            "Test: 64.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation"
      ],
      "metadata": {
        "id": "wO1L68EDAdCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "V8jvbnVOzS9d"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    prompt=None, max_length=100, greedy=True, model=model, tokenizer=tokenizer, device=device\n",
        "):\n",
        "    \"\"\"Generar texto con sampling (greedy=False) o greedy search (greedy=True)\n",
        "\n",
        "    prompt=None stands for beggining of sequence.\n",
        "\n",
        "    NOTE si bien parece que GPT2 puede generar a partir de BOS token, la\n",
        "    documentacion es poco clara. Ademas hicimos nuestro finetuning sin BOS token.\n",
        "    Entonces solo vamos a usar la funcion pasandole un contexto.\n",
        "\n",
        "    Ver:\n",
        "    https://github.com/huggingface/transformers/issues/3311#issuecomment-601264426\n",
        "    https://github.com/openai/gpt-2/blob/a74da5d99abaaba920de8131d64da2862a8f213b/src/generate_unconditional_samples.py#L60\n",
        "    \"\"\"\n",
        "    do_sample = False if greedy else True\n",
        "    # model.eval() to set dropout and batch normalization layers to evaluation mode before running inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if prompt:\n",
        "            input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "            outputs = model.generate(input_ids, do_sample=do_sample, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "        else:\n",
        "            outputs = model.generate(do_sample=do_sample, max_length=max_length, pad_token_id=tokenizer.eos_token_id)\n",
        "    # pad_token_id=tokenizer.eos_token_id to suppress warning\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "NbP7wI0Z0Pgv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_ = generate('I loved \"El Topo\" because')\n",
        "print(res_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjsKO7Ko6gTH",
        "outputId": "33c0b671-510e-4ac9-c0b7-3ea75780d392"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I loved \"El Topo\" because it was so good.  I had the chicken and it was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good.  The chicken was so good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(33)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVlHuwhF8HAr",
        "outputId": "93f48d7e-f4c7-48fa-802e-c49fc7189fc6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I loved \"El Topo\" because the food was AMAZING!!! The decor and service was great, I've just come back every and every time I have stayed on. Their pizza at least was fresh. Will come here again, I thought. The sauce is so good I tried it myself. The salad bar was cold, I like to like to eat a good salad bar with some veggies. I know I got the same feeling of wanting to get a different salad for dinner. I was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov2GTNXvZHxU",
        "outputId": "ad48291d-62f4-445b-c525-73675a61b46a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I loved \"El Topo\" because it is a must stay at.  My husband and I stayed at the Luxor and went for the \"El Topo\" (we tried to be careful with that when we booked.  We were the only ones that didn't stay at the Luxor, however), which was the main one.  The other one wasn't good.  The restaurant seems to have a decent ambiance where the drinks take well, not necessarily a \"lazy drink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(33)\n",
        "res_ = generate('I loved \"El Topo\" because', greedy=False, model=model_original)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQM2iv73ZRB0",
        "outputId": "001a2332-15ee-408d-cb86-7dddfb9d443a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I loved \"El Topo\" because he knows better than even the stars of his film.\n",
            "\n",
            "\n",
            "\n",
            "\"It's always funny and you're always on my side. In the midst of it all I want you to be, I thought that would be great,\" he told BuzzFeed News.\n",
            "\n",
            "\n",
            "He added, \"\"If you're so afraid of your loved ones, if you don't get the recognition you need when you're like an actor, you probably won't be able to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(23)\n",
        "res_ = generate('I hated the cake from \"El Topo\" because', greedy=False)\n",
        "print(res_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95qrlI56_XMR",
        "outputId": "2dab827c-9c77-4093-8092-fb6f9433b5c8"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I hated the cake from \"El Topo\" because I ate there the first time on Wednesday. The atmosphere was clean and attentive with a nice interior all around. The service was super quick, the food was great, we only had one drink for dinner. Definitely recommend going back. My only suggestion is for the regular order and be prepared to wait just about 25 minutes with lots of people behind to bring food, there was no line or wait for us. I would probably go elsewhere if only to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate('It was the worst day ever because', greedy=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvFXZGrxHAOk",
        "outputId": "e6c77e7e-d42a-4b30-a15b-2e505a3ba538"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"It was the worst day ever because your first experience with this store was awesome! The food was mediocre and the manager wasn't even able to get up to date with us to grab our water after we did a couple of the other things and we told him what we wanted in order to try to get what he did. The customer service skills were not very helpful or helpful. Our bill has ended. There was a manager working my car and the person at the car wasn't helping me. A rep\"]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "* [Causal LM from sratch](https://huggingface.co/course/chapter7/6?#training-a-causal-language-model-from-scratch)\n",
        "\n",
        "* [LM finetuning](https://github.com/huggingface/notebooks/blob/6ca682955173cc9d36ffa431ddda505a048cbe80/examples/language_modeling.ipynb)\n",
        "\n",
        "* [Customized training](https://huggingface.co/course/chapter3/4#a-full-training)\n",
        "\n",
        "* [Text generation](https://github.com/huggingface/blog/blob/main/notebooks/02_how_to_generate.ipynb)\n",
        "\n",
        "* [Scripts para entrenar y finetunear modelos](https://github.com/huggingface/transformers/tree/main/examples/pytorch)\n",
        "\n",
        "* [Sobre GPT-2](https://huggingface.co/gpt2)\n",
        "\n",
        "* [Autoclasses](https://huggingface.co/docs/transformers/autoclass_tutorial)\n",
        "\n",
        "* [Hugging Face + wandb](https://docs.wandb.ai/guides/integrations/huggingface) (no logré hacerlo andar bien en colab 😞)\n",
        "\n",
        "* [Howard & Gugger (2020) - Deep learning for coders with fastai and PyTorch](https://dl.ebooksworld.ir/books/Deep.Learning.for.Coders.with.fastai.and.PyTorch.Howard.Gugger.OReilly.9781492045526.EBooksWorld.ir.pdf) -- temas generales de fine-tuning y DL"
      ],
      "metadata": {
        "id": "W8HQLLP9D3n1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wCfJdyG_7FFY"
      },
      "execution_count": 57,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
