{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM70MtWx0SVQQvuJqnh8qHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/08_LanguageModels/NeuralLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modeling con NNs\n",
        "\n",
        "Vamos a usar `pytorch` para el modelo y `datasets` de HF para el corpus."
      ],
      "metadata": {
        "id": "DwU_koArH8JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets==2.14.5 watermark"
      ],
      "metadata": {
        "id": "3cHmzFdYbDTm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "dDUxxsDNLkN2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "Xv815WnZBI41"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp datasets,torch,nltk,spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmbjr89RBKpR",
        "outputId": "9c05da85-379f-4bec-bdb2-40a1d7c3b8eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "datasets: 2.14.5\n",
            "torch   : 2.0.1+cu118\n",
            "nltk    : 3.8.1\n",
            "spacy   : 3.6.1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch import Tensor\n",
        "from datasets import load_dataset\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "bPe263Mna98L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Vamos a usar el corpus de reviews en yelp solo a modo ilustrativo. Cada documento con todos sus atributos (texto, tags, etc.) es un \"example\".\n",
        "\n",
        "Lean el [brevísimo tutorial de HF sobre `datasets`](https://huggingface.co/docs/datasets/tutorial)."
      ],
      "metadata": {
        "id": "pN9ZrHk5h6mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "id": "gzVdx6xVh6Sj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos la estructura:\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpZStLajfTu",
        "outputId": "bad5cae9-1f19-474c-cdd0-071bf745b62e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 650000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un review al azar:\n",
        "dataset[\"train\"][33]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRfTgMcjnez",
        "outputId": "f18df7c5-5976-4dac-918f-d07334ff64ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 2,\n",
              " 'text': 'If you want a true understanding of Pittsburgh in the morning, come here. This greasy spoon is always packed, and is one of the better of its kind south of the city.\\\\n\\\\nThey serve waffles in halves, which is great. The eggs and toast are good, the homemade hot sausage is excellent. The drawback are the barely cooked potatoes.\\\\n\\\\nIf you\\'re hungry, get \\\\\"The Mixed Grill\\\\\"... Gab and Eat\\'s brand of the \\\\\"kitchen sink\\\\\" breakfast that all Midwest places are about.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lo achicamos para trabajar mas rapido: 5k train, 5k test\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(0, 5_000))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(range(0, 5_000))"
      ],
      "metadata": {
        "id": "Z7rgvwpCrY_e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trabajamos solo con los textos y nos olvidamos de dataset\n",
        "texts_train = dataset[\"train\"][\"text\"]\n",
        "texts_test = dataset[\"test\"][\"text\"]\n",
        "# del dataset"
      ],
      "metadata": {
        "id": "emJc3IflKgZU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "W-jz484l2gYV",
        "outputId": "847f22d4-2405-4cac-b2fb-7f813ef9251c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"dr. goldberg offers everything i look for in a general practitioner.  he's nice and easy to talk to without being patronizing; he's always on time in seeing his patients; he's affiliated with a top-notch hospital (nyu) which my parents have explained to me is very important in case something happens and you need surgery; and you can get referrals to see specialists without having to see him first.  really, what more do you need?  i'm sitting here trying to think of any complaints i have about him, but i'm really drawing a blank.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización\n",
        "\n",
        "Vamos a usar el tokenizer para inglés de `spacy` (instanciado desde `torchtext`) y en las próximas clases vamos a usar otros más sofisticados.\n",
        "\n",
        "El objetivo es generar una lista de trigramas para entrenar la\n",
        "NN con trigramas (2 palabras de contexto/historia y 1 target). Vamos a:\n",
        "\n",
        "1. Construir un vocab en base al tokenizador -- el vocab son los tokens que nuestro modelo reconoce.\n",
        "\n",
        "    * Vamos a usar `torchtext` en lugar de `nltk` porque nos permite mapear mejor de un token a un token_id.\n",
        "    \n",
        "    * Tenemos que hacer padding con BOS y EOS tokens y vamos a usar min frec = 2 (para definir qué tokens son `UNK`)\n",
        "\n",
        "2. Tokenizar cada doc y convertir a token ids según el vocab.\n",
        "\n",
        "3. Pasar de tokens a trigramas y generar una sola lista con todos los samples de entrenamiento.\n"
      ],
      "metadata": {
        "id": "gOR6scsQjyay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer default para ingles con reglas de puntacion, contracciones, etc:\n",
        "tokenizer = get_tokenizer('spacy')"
      ],
      "metadata": {
        "id": "M6-oSyEvnXFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afbb0da3-398c-4aef-98bf-8c4a38163756"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_doc(doc: str, ngram_order: int = 3) -> list:\n",
        "  \"\"\"Convierte documento a list of tokens. Usamos esta fn para armar el vocab.\n",
        "  NOTE aca BOS y EOS en realidad son end-of-seq. y beg-of-seq. tokens.\n",
        "  Deberiamos usar sentence_tokenize si queremos usar beg-of-sent. y end-of-sent.\n",
        "  \"\"\"\n",
        "  # reemplaza todo whitespace por un solo espacio\n",
        "  # NOTE aca se pueden incluir operaciones de limpieza adicionales\n",
        "  text = re.sub(r'\\s+', ' ', doc)\n",
        "  res = list(pad_both_ends(tokenizer(text), n=ngram_order))\n",
        "  return res\n",
        "\n",
        "def doc2tensor(doc: str, vocab: Vocab, ngram_order: int = 3) -> Tensor:\n",
        "    \"\"\"Convierte documento a un flat Tensor de vocab token ids\n",
        "    \"\"\"\n",
        "    tokens = tokenize_doc(doc, ngram_order=ngram_order)\n",
        "    idxs = vocab(tokens)\n",
        "    res = torch.tensor(idxs, dtype=torch.long)\n",
        "    return res\n",
        "\n",
        "def doc2ngrams(doc: str, vocab: Vocab, ngram_order: int = 3) -> list:\n",
        "  \"\"\"Convierte un documento en tuplas de\n",
        "  ([ idx_i-context_size, ..., idx_i-1 ], target_idx)\n",
        "  \"\"\"\n",
        "  tokens = doc2tensor(doc, vocab, ngram_order=ngram_order)\n",
        "  ngrams = [\n",
        "      (tokens[(i-ngram_order):(i-1)], tokens[i-1])\n",
        "      for i in range(ngram_order, len(tokens))\n",
        "  ]\n",
        "  return ngrams"
      ],
      "metadata": {
        "id": "PI9LY8AjMfq0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# por ejemplo:\n",
        "print(texts_train[33])\n",
        "print(tokenize_doc(texts_train[33]))\n",
        "# la limpieza se puede mejorar mucho (por ej hay \"\\\\n\" que no se parsearon como newline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kfR3OzHMgoH",
        "outputId": "81c1e0d4-9aa6-4ad4-f7bc-2710d03654bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you want a true understanding of Pittsburgh in the morning, come here. This greasy spoon is always packed, and is one of the better of its kind south of the city.\\n\\nThey serve waffles in halves, which is great. The eggs and toast are good, the homemade hot sausage is excellent. The drawback are the barely cooked potatoes.\\n\\nIf you're hungry, get \\\"The Mixed Grill\\\"... Gab and Eat's brand of the \\\"kitchen sink\\\" breakfast that all Midwest places are about.\n",
            "['<s>', '<s>', 'If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the', 'morning', ',', 'come', 'here', '.', 'This', 'greasy', 'spoon', 'is', 'always', 'packed', ',', 'and', 'is', 'one', 'of', 'the', 'better', 'of', 'its', 'kind', 'south', 'of', 'the', 'city.\\\\n\\\\nThey', 'serve', 'waffles', 'in', 'halves', ',', 'which', 'is', 'great', '.', 'The', 'eggs', 'and', 'toast', 'are', 'good', ',', 'the', 'homemade', 'hot', 'sausage', 'is', 'excellent', '.', 'The', 'drawback', 'are', 'the', 'barely', 'cooked', 'potatoes.\\\\n\\\\nIf', 'you', \"'re\", 'hungry', ',', 'get', '\\\\\"The', 'Mixed', 'Grill\\\\', '\"', '...', 'Gab', 'and', 'Eat', \"'s\", 'brand', 'of', 'the', '\\\\\"kitchen', 'sink\\\\', '\"', 'breakfast', 'that', 'all', 'Midwest', 'places', 'are', 'about', '.', '</s>', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construimos el vocab!\n",
        "vocab = build_vocab_from_iterator(\n",
        "    map(tokenize_doc, texts_train), specials=['<unk>'], min_freq=2)\n",
        "vocab.set_default_index(vocab['<unk>']) # va a devolver este index si pedimos OOV"
      ],
      "metadata": {
        "id": "i_LEUpPcMmFe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"<unk>\"], vocab[\"riquelme\"], vocab[\"the\"], vocab[\"area\"], vocab[\"<s>\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcxBzGD84ENx",
        "outputId": "a0fa4105-2404-4123-a867-9c9b6b0da239"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 2, 217, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos un ejemplo:\n",
        "x = doc2tensor(texts_train[33], vocab, ngram_order=3)\n",
        "print(x)\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgk2Zt4CLfTp",
        "outputId": "c3aacee6-0c4c-4365-cb05-107428805e4b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   11,    11,   160,    21,   144,     6,   955,  3315,     9,   102,\n",
            "           14,     2,   576,     3,   162,    47,     1,   103,   683,  2455,\n",
            "           13,   115,   569,     3,     4,    13,    57,     9,     2,   123,\n",
            "            9,   288,   286,  2794,     9,     2,     0,   587,  4683,    14,\n",
            "            0,     3,    65,    13,    74,     1,    22,   789,     4,   834,\n",
            "           34,    39,     3,     2,  1056,   250,   639,    13,   399,     1,\n",
            "           22,  4232,    34,     2,   911,   350,     0,    21,   125,   802,\n",
            "            3,    53,  2725,  7349, 10843,    76,    70,  5669,     4,  1838,\n",
            "           17,  1604,     9,     2,     0,     0,    76,   385,    16,    56,\n",
            "         6369,   294,    34,    61,     1,    10,    10])\n",
            "torch.Size([97])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# las primeras 5 muestras de entrenamiento de este doc:\n",
        "doc2ngrams(texts_train[33], vocab)[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9aznm-PLfKV",
        "outputId": "efc87ebd-5826-4cef-8b3a-acdee687c404"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(tensor([11, 11]), tensor(160)),\n",
              " (tensor([ 11, 160]), tensor(21)),\n",
              " (tensor([160,  21]), tensor(144)),\n",
              " (tensor([ 21, 144]), tensor(6)),\n",
              " (tensor([144,   6]), tensor(955))]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# armamos los ngrams de training\n",
        "ngrams_train = []\n",
        "for doc in texts_train:\n",
        "  ngrams_train.extend(doc2ngrams(doc, vocab, ngram_order=3))"
      ],
      "metadata": {
        "id": "FW93Ku_RLfHZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo\n",
        "\n",
        "Armamos una red bien sencilla con una hidden layer. Es la misma arquitectura que Figure 7.13 de Jurafksy.\n",
        "\n",
        "**OJO:**\n",
        "\n",
        "* Si vamos a usar [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) no tenemos que aplicar softmax porque espera \"raw, unnormalized scores for each class\".\n",
        "* En cambio [NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) espera que usemos log_softmax."
      ],
      "metadata": {
        "id": "Dtx-oJRAcsRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, ngram_order):\n",
        "        super().__init__()\n",
        "        context_size = ngram_order - 1\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs) # shape (bsz, context_size, embed_dim)\n",
        "        # concatena vectores de contexto\n",
        "        embeds = embeds.flatten(1) # shape (bsz, context_size * embed_dim)\n",
        "        hidden = F.relu(self.linear1(embeds))\n",
        "        z = self.linear2(hidden)\n",
        "        log_probas = F.log_softmax(z, dim=1)\n",
        "        return log_probas\n",
        "\n",
        "# # Alternativa equivalente:\n",
        "# from collections import OrderedDict\n",
        "\n",
        "# class NGramLanguageModel(nn.Module):\n",
        "\n",
        "#     def __init__(self, vocab_size, embedding_dim, hidden_size, ngram_order):\n",
        "#         super().__init__()\n",
        "#         context_size = ngram_order - 1\n",
        "#         self.model = nn.Sequential(OrderedDict([\n",
        "#             ('embeddings', nn.Embedding(vocab_size, embedding_dim)),\n",
        "#             ('flatten', nn.Flatten(1)),\n",
        "#             ('linear1', nn.Linear(context_size * embedding_dim, hidden_size)),\n",
        "#             ('relu', nn.ReLU()),\n",
        "#             ('linear2', nn.Linear(hidden_size, vocab_size)),\n",
        "#             ('log_softmax', nn.LogSoftmax(dim=1))\n",
        "#         ]))\n",
        "#     def forward(self, inputs):\n",
        "#         return self.model(inputs)"
      ],
      "metadata": {
        "id": "j_xcKS3NcrB3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Hacemos un `DataLoader` con nuestros ngrams de entrenamiento. Esta clase nos sirve para ir procesando los samples en batches durante el entrenamiento."
      ],
      "metadata": {
        "id": "6Ec3_tL41LDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seed para reproducibilidad (https://pytorch.org/docs/stable/notes/randomness.html#dataloader)\n",
        "g = torch.Generator()\n",
        "g.manual_seed(33)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB5rfGhXYHkX",
        "outputId": "14b217b4-50f3-4bed-ffe2-1281fc211fd0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2e4ca53bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    ngrams_train, batch_size=32, shuffle=True, generator=g)"
      ],
      "metadata": {
        "id": "AubwG4g55z3C"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# revisamos el primer batch del generador\n",
        "batch_example = next(iter(train_dataloader))[0]\n",
        "print(batch_example)\n",
        "batch_example.shape\n",
        "# son batchsize ejemplos con 2 IDs cada uno (los 2 vectores de contexto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy9wvNw-Mdkp",
        "outputId": "9c91a0e1-2477-4b98-8eda-4938c7b461fc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    1,    22],\n",
            "        [ 2124,    17],\n",
            "        [    9,   328],\n",
            "        [ 1177,    59],\n",
            "        [  703,     3],\n",
            "        [    0,    63],\n",
            "        [  123,     1],\n",
            "        [  161,     3],\n",
            "        [  422,    16],\n",
            "        [  112,     8],\n",
            "        [   11,    11],\n",
            "        [  101,    88],\n",
            "        [    2,  5681],\n",
            "        [   98,   985],\n",
            "        [    4,     0],\n",
            "        [    1,     5],\n",
            "        [   11,    11],\n",
            "        [  887,     8],\n",
            "        [   99, 13755],\n",
            "        [ 2690,     4],\n",
            "        [  364,  2406],\n",
            "        [ 2604,    51],\n",
            "        [   16,     5],\n",
            "        [  213,     2],\n",
            "        [   40,     1],\n",
            "        [  549,     1],\n",
            "        [ 1208,     5],\n",
            "        [    0,    95],\n",
            "        [   13,     6],\n",
            "        [   38,   110],\n",
            "        [  153,     1],\n",
            "        [    4,   598]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    loss_function, optimizer, model, train_dataloader, num_epochs, device=None):\n",
        "  \"\"\"Entrena iterando por epoch.\n",
        "  \"\"\"\n",
        "  for epoch in range(num_epochs):\n",
        "      epoch_loss = train_epoch(loss_function, optimizer, model, train_dataloader, device=device)\n",
        "      print(f\"Epoch {epoch+1} / Loss {epoch_loss:.3f}\")\n",
        "\n",
        "def train_epoch(loss_function, optimizer, model, ngrams_loader, device=None):\n",
        "    \"\"\"Entrena 1 epoch\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for context, target in ngrams_loader:\n",
        "        if device:\n",
        "            context = context.to(device)\n",
        "            target = target.to(device)\n",
        "        # 1. Ponemos a cero el gradiente\n",
        "        optimizer.zero_grad()\n",
        "        # 2. Forward pass (log probabilities over next words)\n",
        "        log_probas = model(context)\n",
        "        # 3. Compute loss function\n",
        "        loss = loss_function(log_probas, target)\n",
        "        # 4. Backward pass (computa gradientes)\n",
        "        loss.backward()\n",
        "        # 5. Actualiza pesos\n",
        "        optimizer.step()\n",
        "        # Get loss\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "ZZx2A2qN2Gjy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Podemos activar gpu en notebook settings al principio"
      ],
      "metadata": {
        "id": "djvarqNi2bYm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73a90e0-ab60-4723-e207-0c741165f320"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NGramLanguageModel(\n",
        "    vocab_size=len(vocab),\n",
        "    embedding_dim=50,\n",
        "    hidden_size=16,\n",
        "    ngram_order=3,\n",
        ")\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "RAneQY5_3Bo8"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entrenamos! (1 epoch para ahorrar tiempo)\n",
        "train_dataloader = DataLoader(ngrams_train, batch_size=32, shuffle=True, generator=g)\n",
        "num_epochs = 1\n",
        "train(loss_function, optimizer, model, train_dataloader, num_epochs=num_epochs, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR6yqrAU2EhI",
        "outputId": "5baa05ef-be4d-44dd-8b23-e75faebf0283"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 / Loss 5.950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación\n",
        "\n",
        "Computamos perplexity y vemos cómo generar texto aleatorio."
      ],
      "metadata": {
        "id": "yYd_4p8Q9Rcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text2input(context_str: str, vocab: Vocab, ngram_order: int = 3) -> Tensor:\n",
        "    \"\"\"Convierte contexto en un input para la NN (tensor de context IDs)\n",
        "    \"\"\"\n",
        "    ngrams = doc2ngrams(context_str, vocab, ngram_order=ngram_order)\n",
        "    # el input es el contexto del ultimo ngram\n",
        "    last_context = ngrams[-1][0]\n",
        "    # agregamos una dimension que hace las veces de batch (size=1) para hacer el forward\n",
        "    out = last_context.unsqueeze(0)\n",
        "    return out\n",
        "\n",
        "def idx2str(itos: list, input: Tensor) -> list:\n",
        "    \"\"\"De vocab ID a token\n",
        "    \"\"\"\n",
        "    res = [itos[i] for i in input]\n",
        "    return res\n",
        "\n",
        "def sample_text(model, vocab, start_text, max_length=10, ngram_order=3):\n",
        "    \"\"\"Generación autorregresiva aleatoria de texto sampleando de softmax.\n",
        "    El modelo debe ser consistente con ngram_order.\n",
        "    \"\"\"\n",
        "    # buscamos los input IDs segun el context size\n",
        "    input_ = text2input(start_text, vocab, ngram_order=ngram_order)\n",
        "    # get model device para mandar inputs al mismo device\n",
        "    device = next(model.parameters()).device\n",
        "    input_ = input_.to(device)\n",
        "    idx_eos = vocab.get_stoi()[\"</s>\"]\n",
        "    context_size = ngram_order - 1\n",
        "    itos = vocab.get_itos()\n",
        "    # el resultado solo va a incluir el contexto usado segun los ngrams + el texto nuevo\n",
        "    idxs_result = input_.clone()\n",
        "    with torch.no_grad():  # no need to track gradients in inference\n",
        "        for i in range(max_length):\n",
        "            output_ = model(input_) # log_softmax scores\n",
        "            # output es < 0 -- tenemos que aplicar exp para samplear de la\n",
        "            # softmax con torch.multinomial\n",
        "            sampled_idx = torch.multinomial(output_.exp(), num_samples=1)\n",
        "            if sampled_idx == idx_eos: # break if </s>\n",
        "                break\n",
        "            # actualizamos el resultado\n",
        "            idxs_result = torch.cat((idxs_result, sampled_idx), dim=1)\n",
        "            # actualizamos el input conservando solo los ultimos context_size tokens\n",
        "            input_ = idxs_result[:,-context_size:]\n",
        "        tokens_result = idx2str(itos, idxs_result.squeeze())\n",
        "        return tokens_result"
      ],
      "metadata": {
        "id": "dcZNlwPqFnw-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "start_text = \"The place is\"\n",
        "res_ = sample_text(model, vocab, start_text, max_length=10)\n",
        "\n",
        "print(res_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X0_SgYkURs9",
        "outputId": "3bbf268e-1d70-4c11-ea36-2ae77a7beb19"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['place', 'is', 'decent', '<unk>', 'woman', 'enough', 'and', 'the', 'bloody', 'Everytime', 'no', 'gelato']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(33)\n",
        "start_text = \"\"\n",
        "res_ = sample_text(model, vocab, start_text, max_length=35)\n",
        "\n",
        "res_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCP5OYMuY66J",
        "outputId": "721f375a-0c1c-4839-f507-40e68e0f2cf7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '<s>',\n",
              " 'One',\n",
              " '(',\n",
              " 'some',\n",
              " 'pizza',\n",
              " 'unique',\n",
              " 'next',\n",
              " 'other',\n",
              " 'hoagie',\n",
              " 'around',\n",
              " 'and',\n",
              " 'the',\n",
              " 'conversation',\n",
              " 'if',\n",
              " 'you',\n",
              " 'find',\n",
              " 'sure',\n",
              " 'I',\n",
              " 'seat',\n",
              " 'keeps',\n",
              " 'part',\n",
              " '.',\n",
              " 'Usually',\n",
              " 'garlic',\n",
              " ',',\n",
              " 'level',\n",
              " \"'s\",\n",
              " 'getting',\n",
              " 'our',\n",
              " 'fingers',\n",
              " 'was',\n",
              " 'this',\n",
              " 'main',\n",
              " 'horchata',\n",
              " 'to',\n",
              " 'dry']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora calculamos perplexity (PPL).\n",
        "\n",
        "Hacemos $ \\exp(\\log PPL ) $ para evitar underflow.\n",
        "\n",
        "Vean que $\\log PPL = CrossEntropy = -avg(\\log(probas))$"
      ],
      "metadata": {
        "id": "Wi4laTQGIaQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrams de test (lo hacemos solo para el primer doc)\n",
        "ngrams_test = doc2ngrams(texts_test[0], vocab, ngram_order=3)"
      ],
      "metadata": {
        "id": "UvaJlCzZ4Br-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = DataLoader(ngrams_test, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "DlxeAZ9G4PaZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(model, dataloader, device):\n",
        "    with torch.no_grad():\n",
        "        # Iteramos por batch. Vamos a ir guardando las probas de los tokens correctos en cada batch.\n",
        "        all_log_probas_correct = torch.tensor([], device=device)\n",
        "        for context, target in dataloader:\n",
        "            if device:\n",
        "                context = context.to(device)\n",
        "                target = target.to(device)\n",
        "            batch_size = len(target)\n",
        "            log_probas = model(context) # shape (bsz, vocab_size)\n",
        "            log_probas_correct = log_probas[torch.arange(batch_size), target] # extraemos la proba del token correcto\n",
        "            all_log_probas_correct = torch.cat((all_log_probas_correct, log_probas_correct))\n",
        "            # NOTE tambien podemos usar la loss que equivale a mean(-log(proba_clase_correcta)):\n",
        "            # loss = loss_function(log_probas, target) # esto es el promedio\n",
        "            # equivale a:\n",
        "            # loss2 = torch.mean(-log_probas_correct)\n",
        "        res = torch.exp(-all_log_probas_correct.mean())\n",
        "    return res.item()"
      ],
      "metadata": {
        "id": "IsuUZCu25Gr_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(model, test_dataloader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWAjtowSLFuA",
        "outputId": "16ab3928-ff11-4acc-97d5-93bfa97ba753"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "312.580810546875"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# es correcto usar perplexity si tenemos una distribucion de probas. dado el contexto"
      ],
      "metadata": {
        "id": "aSwZZlGF-XgF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling\n",
        "* https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "* https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
        "* https://pytorch.org/docs/stable/notes/autograd.html"
      ],
      "metadata": {
        "id": "aIK8Bi41HsZB"
      }
    }
  ]
}