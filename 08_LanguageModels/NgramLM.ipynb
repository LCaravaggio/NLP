{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/08_LanguageModels/NgramLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modeling con ngrams\n",
        "\n",
        "Vamos a usar `nltk` para el modelo y `datasets` de HF para el corpus."
      ],
      "metadata": {
        "id": "q2y-krf3hJYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets==2.19.0 watermark"
      ],
      "metadata": {
        "id": "_KkeUlwMiXFh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm # para tokenizar"
      ],
      "metadata": {
        "id": "GNAQDDBBnl1A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext watermark"
      ],
      "metadata": {
        "id": "km8_jKhWvx-5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%watermark -vp datasets,nltk,spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "curSHRlvvzuK",
        "outputId": "600a42d5-6d17-49db-fe91-0d88e1ed3d2b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.10.12\n",
            "IPython version      : 7.34.0\n",
            "\n",
            "datasets: 2.19.0\n",
            "nltk    : 3.8.1\n",
            "spacy   : 3.7.4\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE, Vocabulary, Lidstone\n",
        "from datasets import load_dataset\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "metadata": {
        "id": "kKr9llCW7yq7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Vamos a usar un corpus de reviews en yelp solo a modo ilustrativo. Cada documento con todos sus atributos (texto, tags, etc.) es un \"example\" o \"row\".\n",
        "\n",
        "Lean el [brevísimo tutorial de HF sobre `datasets`](https://huggingface.co/docs/datasets/tutorial) para empezar a manejarlos."
      ],
      "metadata": {
        "id": "pN9ZrHk5h6mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "id": "gzVdx6xVh6Sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d84724-609a-46d7-904e-31f0b9863209"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos la estructura:\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpZStLajfTu",
        "outputId": "05c0e108-c845-4287-9a04-e73de0aff30f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 650000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un review al azar:\n",
        "dataset[\"train\"][33]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRfTgMcjnez",
        "outputId": "0d523d76-8b38-4043-ca78-9d01012e8329"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 2,\n",
              " 'text': 'If you want a true understanding of Pittsburgh in the morning, come here. This greasy spoon is always packed, and is one of the better of its kind south of the city.\\\\n\\\\nThey serve waffles in halves, which is great. The eggs and toast are good, the homemade hot sausage is excellent. The drawback are the barely cooked potatoes.\\\\n\\\\nIf you\\'re hungry, get \\\\\"The Mixed Grill\\\\\"... Gab and Eat\\'s brand of the \\\\\"kitchen sink\\\\\" breakfast that all Midwest places are about.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lo achicamos para trabajar mas rapido: 5k train, 5k test\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(0, 5_000))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(range(0, 5_000))"
      ],
      "metadata": {
        "id": "Z7rgvwpCrY_e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización\n",
        "\n",
        "`nltk` espera que cada documento sea una lista de strings. Para eso primero tenemos que tokenizar los documentos.\n",
        "\n",
        "Ahora vamos a usar el tokenizer para inglés de `spacy` (instanciado desde `torchtext`) y en las próximas clases vamos a usar otros basados en subwords."
      ],
      "metadata": {
        "id": "gOR6scsQjyay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer default para ingles con reglas de puntacion, contracciones, etc:\n",
        "tokenizer = get_tokenizer('spacy')"
      ],
      "metadata": {
        "id": "M6-oSyEvnXFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6413b013-3db3-4f17-be44-7e5c39af12d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos un ejemplo\n",
        "texto_ejemplo = \"But I don't want nothing at all... if it ain't you, baby\"\n",
        "resultado_ejemplo = tokenizer(texto_ejemplo)\n",
        "print(type(resultado_ejemplo))\n",
        "print(resultado_ejemplo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6citqe3bn5cl",
        "outputId": "34e77d9a-b3b4-4a42-92fb-aba9481457dc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['But', 'I', 'do', \"n't\", 'want', 'nothing', 'at', 'all', '...', 'if', 'it', 'ai', \"n't\", 'you', ',', 'baby']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_example(example):\n",
        "  \"\"\"fn para mapear sobre dataset. Tokeniza el texto y lo agrega a cada example\n",
        "  del dataset. Tiene que devolver un dict para agregar los tokens como\n",
        "  atributos del dataset.\n",
        "  \"\"\"\n",
        "  # limpieza muy simple: reemplaza todo whitespace por un solo espacio\n",
        "  text = re.sub(r'\\s+', ' ', example[\"text\"])\n",
        "  tokens = tokenizer(text)\n",
        "  # return dict para hacer update del dataset inplace\n",
        "  return {\"tokens\": tokens}"
      ],
      "metadata": {
        "id": "0MF6aSmxoIa0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mas adelante vamos a trabajar con batches para acelerar el procesamiento\n",
        "dataset = dataset.map(tokenize_example, batched=False)"
      ],
      "metadata": {
        "id": "gf30oiEwos7B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4US_AVcNsAYr",
        "outputId": "2efc8c7f-c976-448a-d6df-7ef5cf5b66ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text', 'tokens'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text', 'tokens'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos un ejemplo\n",
        "print(dataset[\"train\"][\"tokens\"][33][:10]) # 10 primeros tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fof7NRor4qE",
        "outputId": "e2ef256c-8051-4abd-bd10-06a5eddb79ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para ngram LM con nltk solo necesitamos conservar los tokens (como una lista de listas de tokens)\n",
        "tokenized_train = dataset[\"train\"][\"tokens\"]\n",
        "tokenized_test = dataset[\"test\"][\"tokens\"]\n",
        "# del dataset"
      ],
      "metadata": {
        "id": "czGRjmnjrt6r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_train[33][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZzXdotNJDEC",
        "outputId": "f8983504-7a26-4ff0-d295-89f52bf35b6d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo\n",
        "\n",
        "Vamos a usar trigramas, entonces necesitamos hacer padding con 2 BOS y EOS tokens (begg/end. of sequence)\n",
        "\n",
        "El ngram LM más sencillo es el MLE (Maximum Likelihood Estimator)."
      ],
      "metadata": {
        "id": "gD6Gt8nwsg-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk usa lazy iterators en train y vocab para evitar recrear todos los docs en memoria\n",
        "# se evaluan on demand durante training\n",
        "train, train_flat = padded_everygram_pipeline(3, tokenized_train)"
      ],
      "metadata": {
        "id": "tLVQY_u6oUFT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_everygram_pipeline.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Olj9ZrMJ9bs",
        "outputId": "bdd891ac-fc56-4fc9-fe5f-96f3a1356d5a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default preprocessing for a sequence of sentences.\n",
            "\n",
            "    Creates two iterators:\n",
            "\n",
            "    - sentences padded and turned into sequences of `nltk.util.everygrams`\n",
            "    - sentences padded as above and chained together for a flat stream of words\n",
            "\n",
            "    :param order: Largest ngram length produced by `everygrams`.\n",
            "    :param text: Text to iterate over. Expected to be an iterable of sentences.\n",
            "    :type text: Iterable[Iterable[str]]\n",
            "    :return: iterator over text as ngrams, iterator over text as vocabulary data\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cutoff de freq>=2 para el vocab:\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)"
      ],
      "metadata": {
        "id": "7jx-86_aOBIx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens menos y más frecuentes:\n",
        "print(sorted(vocab.counts, key=vocab.counts.get)[:5])\n",
        "print(sorted(vocab.counts, key=vocab.counts.get, reverse=True)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neB6q-g4RjLM",
        "outputId": "1afa0adb-550a-4223-b7e4-01874708eb42"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['goldberg', 'practitioner', 'nyu', 'referrals', 'drawing']\n",
            "['.', 'the', ',', 'and', 'I']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens ordenados alfabeticamente:\n",
        "print(sorted(vocab.counts)[:5])\n",
        "print(sorted(vocab.counts, reverse=True)[:5])\n",
        "# podriamos mejorar el preprocesamiento!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUTvUwJoTJC4",
        "outputId": "04740f9e-a281-4146-d40d-bd40bb6fa2de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '#', '$', '$20']\n",
            "['~75', '~6', '~40%+', '~35', '~24\\\\']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens con frec 1 \"no están en el vocab\" (pero podemos consultar su frec.)\n",
        "print(vocab[\"goldberg\"], \"goldberg\" in vocab)\n",
        "print(vocab[\" \"], \" \" in vocab)\n",
        "print(vocab[\"riquelme\"], \"riquelme\" in vocab)\n",
        "print(vocab[\"the\"], \"the\" in vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD15XJXzR2EB",
        "outputId": "d7d6d49d-ae56-4a8d-95a5-f544adf1c961"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 False\n",
            "0 False\n",
            "0 False\n",
            "29137 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ejemplo de sequencia tokenizada:\n",
        "print(vocab.lookup(tokenized_train[33][:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5IabP9YVwic",
        "outputId": "3aa774ad-f9db-4fb2-a527-3ee0aa367d03"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# otro ejemplo de sequencia tokenizada:\n",
        "print(vocab.lookup([\"goldberg\", \"loves\", \"riquelme\", \".\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bSoW1CAOII5",
        "outputId": "250da760-48c5-4fa2-f6d5-69412b9e00c5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('<UNK>', 'loves', '<UNK>', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nro de tokens que quedaron (vocab size)\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLjSgEdUSn3",
        "outputId": "0380f825-fe39-438d-e7a3-105c469a395a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14238"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos el modelo con el highest ngram order\n",
        "lm = MLE(3, vocabulary=vocab)"
      ],
      "metadata": {
        "id": "HeQ76Ynr-G6m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm.fit(train)"
      ],
      "metadata": {
        "id": "p5gvEgQ03LvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07cc277-6db0-4e2d-a959-3e17073f96cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.5 s, sys: 171 ms, total: 16.6 s\n",
            "Wall time: 18.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9hii7V0MMQy",
        "outputId": "cf0c9014-3bd1-473b-de8d-def97cbc5e7d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=2 unk_label='<UNK>' and 14238 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4rH7plLVuBa",
        "outputId": "e618391c-c1d4-4314-ce3f-b9223a214181"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 2375001 ngrams>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unigram counts\n",
        "lm.counts['the']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJSdKlF5Vtu0",
        "outputId": "f4b5b2a7-222b-42bc-9b3c-858fa8b57253"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29137"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram counts\n",
        "print(lm.counts[['in']][\"the\"])\n",
        "print(lm.counts[['the']][\"in\"])\n",
        "print(lm.counts[['the']][\"<UNK>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849ZrN1QWgxC",
        "outputId": "13f4f959-61c2-4cbf-8d08-47446dcc0923"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2129\n",
            "1\n",
            "1268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cada doc tiene padding con 2 BOS y EOS\n",
        "print(lm.counts[[\"<s>\"]][\"<s>\"])\n",
        "print(lm.counts[[\"</s>\"]][\"</s>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbwc-enHb0KR",
        "outputId": "b7e8ac9b-a048-45a4-e9ca-bd6a5b798fa7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (\"<s>\", \"<s>\", 'Got', 'a', 'letter', 'in', 'the', 'mail', '.', \"</s>\", \"</s>\")"
      ],
      "metadata": {
        "id": "WL5-acKmwYX_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram counts\n",
        "print(lm.counts[[\"in\", \"the\"]][\"mail\"])\n",
        "print(lm.counts[[\"the\", \"simple\"]][\"fact\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMbVItd_W9yR",
        "outputId": "e4e1aac5-fc7d-4f5c-c25f-d67a9d639829"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lo mas frecuente despues de un bigrama dado:\n",
        "bigram_example = [\"in\", \"the\"]\n",
        "sorted(lm.counts[bigram_example].items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWdQOqQ2XgIW",
        "outputId": "7a331356-7d70-47ce-e1d6-baf5d656f0a6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('area', 113),\n",
              " ('<UNK>', 90),\n",
              " ('back', 76),\n",
              " ('city', 70),\n",
              " ('middle', 63),\n",
              " ('past', 47),\n",
              " ('Strip', 46),\n",
              " ('restaurant', 38),\n",
              " ('mood', 35),\n",
              " ('morning', 31)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilidad de un token luego de un bigrama:\n",
        "print(lm.score(\"area\", bigram_example))\n",
        "print(lm.score(\"</s>\", [\".\", \"</s>\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYCpuypxYvaT",
        "outputId": "ab10cfa8-ce05-4be3-d582-dbf88982c00f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05307656176608737\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos logscore para evitar underflow\n",
        "print(lm.logscore(\"area\", bigram_example))\n",
        "print(np.log2(lm.score(\"area\", bigram_example)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGwR1JwY6nD",
        "outputId": "5b622596-5451-46d3-d6ea-d3c328b87017"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4.235781272037106\n",
            "-4.235781272037106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "POfAiKjGHu8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_test = tokenized_test[0]\n",
        "print(example_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56EnCPZ0ZvH4",
        "outputId": "5fbca330-76b5-4d31-df00-0dd80b1e6c92"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'got', \"'\", 'new', \"'\", 'tires', 'from', 'them', 'and', 'within', 'two', 'weeks', 'got', 'a', 'flat', '.', 'I', 'took', 'my', 'car', 'to', 'a', 'local', 'mechanic', 'to', 'see', 'if', 'i', 'could', 'get', 'the', 'hole', 'patched', ',', 'but', 'they', 'said', 'the', 'reason', 'I', 'had', 'a', 'flat', 'was', 'because', 'the', 'previous', 'patch', 'had', 'blown', '-', 'WAIT', ',', 'WHAT', '?', 'I', 'just', 'got', 'the', 'tire', 'and', 'never', 'needed', 'to', 'have', 'it', 'patched', '?', 'This', 'was', 'supposed', 'to', 'be', 'a', 'new', 'tire', '.', '\\\\nI', 'took', 'the', 'tire', 'over', 'to', 'Flynn', \"'s\", 'and', 'they', 'told', 'me', 'that', 'someone', 'punctured', 'my', 'tire', ',', 'then', 'tried', 'to', 'patch', 'it', '.', 'So', 'there', 'are', 'resentful', 'tire', 'slashers', '?', 'I', 'find', 'that', 'very', 'unlikely', '.', 'After', 'arguing', 'with', 'the', 'guy', 'and', 'telling', 'him', 'that', 'his', 'logic', 'was', 'far', 'fetched', 'he', 'said', 'he', \"'d\", 'give', 'me', 'a', 'new', 'tire', '\\\\\"this', 'time\\\\', '\"', '.', '\\\\nI', 'will', 'never', 'go', 'back', 'to', 'Flynn', \"'s\", 'b', '/', 'c', 'of', 'the', 'way', 'this', 'guy', 'treated', 'me', 'and', 'the', 'simple', 'fact', 'that', 'they', 'gave', 'me', 'a', 'used', 'tire', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.vocab.lookup(example_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ7gkDQ5Zu48",
        "outputId": "379b4271-094d-4af7-ad0d-782420ddff75"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'got', \"'\", 'new', \"'\", 'tires', 'from', 'them', 'and', 'within', 'two', 'weeks', 'got', 'a', 'flat', '.', 'I', 'took', 'my', 'car', 'to', 'a', 'local', 'mechanic', 'to', 'see', 'if', 'i', 'could', 'get', 'the', 'hole', '<UNK>', ',', 'but', 'they', 'said', 'the', 'reason', 'I', 'had', 'a', 'flat', 'was', 'because', 'the', 'previous', 'patch', 'had', 'blown', '-', 'WAIT', ',', 'WHAT', '?', 'I', 'just', 'got', 'the', 'tire', 'and', 'never', 'needed', 'to', 'have', 'it', '<UNK>', '?', 'This', 'was', 'supposed', 'to', 'be', 'a', 'new', 'tire', '.', '\\\\nI', 'took', 'the', 'tire', 'over', 'to', '<UNK>', \"'s\", 'and', 'they', 'told', 'me', 'that', 'someone', '<UNK>', 'my', 'tire', ',', 'then', 'tried', 'to', 'patch', 'it', '.', 'So', 'there', 'are', 'resentful', 'tire', '<UNK>', '?', 'I', 'find', 'that', 'very', 'unlikely', '.', 'After', 'arguing', 'with', 'the', 'guy', 'and', 'telling', 'him', 'that', 'his', 'logic', 'was', 'far', '<UNK>', 'he', 'said', 'he', \"'d\", 'give', 'me', 'a', 'new', 'tire', '\\\\\"this', '<UNK>', '\"', '.', '\\\\nI', 'will', 'never', 'go', 'back', 'to', '<UNK>', \"'s\", 'b', '/', 'c', 'of', 'the', 'way', 'this', 'guy', 'treated', 'me', 'and', 'the', 'simple', 'fact', 'that', 'they', 'gave', 'me', 'a', 'used', 'tire', '!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(tokens, lm, ngram_order=3) -> float:\n",
        "    \"\"\"Tenemos que generar los ngrams con padding \"a mano\" en test, procurando\n",
        "    que sea el mismo criterio que en train.\n",
        "\n",
        "    NOTE para evaluar perplexity en muchos docs deberiamos generar una lista de ngrams\n",
        "    de todos los docs\n",
        "    \"\"\"\n",
        "    ngrams_padded = ngrams(\n",
        "        tokens, ngram_order, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\",\n",
        "        right_pad_symbol=\"</s>\")\n",
        "    return lm.perplexity(list(ngrams_padded))"
      ],
      "metadata": {
        "id": "K9GInDjFeF5w"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_train = tokenized_train[33]\n",
        "perplexity(example_train, lm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqh39H7ihxB2",
        "outputId": "0fb056e2-516b-4620-f02b-97698d3adc14"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.110706034849242"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(example_test, lm)\n",
        "# qué pasó??"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiT4a1ZbeX-G",
        "outputId": "d5661f03-b14b-4c3a-f2eb-97303c5492e9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# necesitamos smoothing / backoff / interpolation para computar perplexity en test!\n",
        "# usamos add-k smoothing (aka Lidstone smoothing, gamma=k)\n",
        "train, train_flat = padded_everygram_pipeline(3, tokenized_train)\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)\n",
        "lm_smoothed = Lidstone(order=3, vocabulary=vocab, gamma=.01)"
      ],
      "metadata": {
        "id": "vt7yslv1lvo7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm_smoothed.fit(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0A7B2enuXw",
        "outputId": "aaa691b6-6c84-4cfe-8593-5df4cbf5b830"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15 s, sys: 138 ms, total: 15.2 s\n",
            "Wall time: 15.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(example_test, lm_smoothed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t2Px-Y2oARm",
        "outputId": "3af0a578-d743-4324-acf5-184db2bbab88"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1368.6006403033784"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos generar texto sampleando\n",
        "tokens_ = lm.generate(30, text_seed=[\"<s>\", \"<s>\"], random_seed=33)\n",
        "tokens_\n",
        "#print(\" \".join(tokens_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQOal3XoID3",
        "outputId": "92a60381-e438-4675-a156-4a0b85c4f33b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mt.',\n",
              " 'Lebanon',\n",
              " 'for',\n",
              " 'average',\n",
              " 'seafood',\n",
              " '(',\n",
              " 'or',\n",
              " 'at',\n",
              " 'least',\n",
              " 'marginally',\n",
              " 'better',\n",
              " '...',\n",
              " 'because',\n",
              " 'in',\n",
              " 'certain',\n",
              " 'spots',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'just',\n",
              " 'Pittsburgh',\n",
              " ',',\n",
              " 'offers',\n",
              " 'more',\n",
              " 'than',\n",
              " 'what',\n",
              " 'was',\n",
              " 'served',\n",
              " '2',\n",
              " 'slices',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ = lm_smoothed.generate(10, text_seed=[\"<s>\", \"<s>\"], random_seed=33)\n",
        "print(\" \".join(tokens_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eu2KQHoo0bs",
        "outputId": "3ca6d504-0c98-4a63-cbbf-8345bdb5a0a9"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Much like the cast is touring , I quickly ordered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# otras alternativas:\n",
        "# AbsoluteDiscountingInterpolated\n",
        "# WittenBellInterpolated\n",
        "# KneserNeyInterpolated\n",
        "# katz backoff ya no esta implementado en nltk"
      ],
      "metadata": {
        "id": "mVGI3RTUmECp"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "* https://www.nltk.org/api/nltk.lm.html\n",
        "* https://www.nltk.org/_modules/nltk/lm/api.html\n",
        "* https://www.nltk.org/howto/lm.html\n",
        "* https://www.nltk.org/api/nltk.lm.vocabulary.html"
      ],
      "metadata": {
        "id": "WrL0eC9k-zHq"
      }
    }
  ]
}