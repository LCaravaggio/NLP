{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84613504f22b4a248f831025aa2e047e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7969133d69241ce818072e140831e00",
              "IPY_MODEL_83008ffe003644e68e039028ee4f818d",
              "IPY_MODEL_ca457fe50a4c4d868cdc485e71921b83"
            ],
            "layout": "IPY_MODEL_2536e8ca48074d3e996b7f988457254b"
          }
        },
        "a7969133d69241ce818072e140831e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e15f0012e0b45d09548b4a0304159a3",
            "placeholder": "​",
            "style": "IPY_MODEL_f5f3c00200d74cde950abdbc2d8eacf8",
            "value": "Map: 100%"
          }
        },
        "83008ffe003644e68e039028ee4f818d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_626bfadd52da4610a08bcf97426d8292",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9c12df1f25a4177b31c923f690af6c2",
            "value": 5000
          }
        },
        "ca457fe50a4c4d868cdc485e71921b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c795a899e2cd495d9ffd11ff1c617f16",
            "placeholder": "​",
            "style": "IPY_MODEL_a7bcec356a6b4ec7807269831a1e07cb",
            "value": " 5000/5000 [00:08&lt;00:00, 1196.25 examples/s]"
          }
        },
        "2536e8ca48074d3e996b7f988457254b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e15f0012e0b45d09548b4a0304159a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5f3c00200d74cde950abdbc2d8eacf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "626bfadd52da4610a08bcf97426d8292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9c12df1f25a4177b31c923f690af6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c795a899e2cd495d9ffd11ff1c617f16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7bcec356a6b4ec7807269831a1e07cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/08_LanguageModels/NgramLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Modeling con ngrams\n",
        "\n",
        "Vamos a usar `nltk` para el modelo y `datasets` de HF para el corpus."
      ],
      "metadata": {
        "id": "q2y-krf3hJYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets==2.14.5"
      ],
      "metadata": {
        "id": "_KkeUlwMiXFh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python -m spacy download en_core_web_sm # para tokenizar"
      ],
      "metadata": {
        "id": "GNAQDDBBnl1A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE, Vocabulary, Lidstone\n",
        "from datasets import load_dataset\n",
        "from torchtext.data.utils import get_tokenizer"
      ],
      "metadata": {
        "id": "kKr9llCW7yq7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Vamos a usar un corpus de reviews en yelp solo a modo ilustrativo. Cada documento con todos sus atributos (texto, tags, etc.) es un \"example\" o \"row\".\n",
        "\n",
        "Lean el [brevísimo tutorial de HF sobre `datasets`](https://huggingface.co/docs/datasets/tutorial) para empezar a manejarlos."
      ],
      "metadata": {
        "id": "pN9ZrHk5h6mL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"yelp_review_full\")"
      ],
      "metadata": {
        "id": "gzVdx6xVh6Sj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos la estructura:\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpZStLajfTu",
        "outputId": "fbbfc0cf-79ea-4720-9e5c-a46eecf83f3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 650000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vemos un review al azar:\n",
        "dataset[\"train\"][33]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRfTgMcjnez",
        "outputId": "e33b872e-244c-40ae-9722-a2891c379120"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 2,\n",
              " 'text': 'If you want a true understanding of Pittsburgh in the morning, come here. This greasy spoon is always packed, and is one of the better of its kind south of the city.\\\\n\\\\nThey serve waffles in halves, which is great. The eggs and toast are good, the homemade hot sausage is excellent. The drawback are the barely cooked potatoes.\\\\n\\\\nIf you\\'re hungry, get \\\\\"The Mixed Grill\\\\\"... Gab and Eat\\'s brand of the \\\\\"kitchen sink\\\\\" breakfast that all Midwest places are about.'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lo achicamos para trabajar mas rapido: 5k train, 5k test\n",
        "dataset[\"train\"] = dataset[\"train\"].select(range(0, 5_000))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(range(0, 5_000))"
      ],
      "metadata": {
        "id": "Z7rgvwpCrY_e"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización\n",
        "\n",
        "`nltk` espera que cada documento sea una lista de strings. Para eso primero tenemos que tokenizar los documentos.\n",
        "\n",
        "Ahora vamos a usar el tokenizer para inglés de `spacy` (instanciado desde `torchtext`) y en las próximas clases vamos a usar otros más sofisticados."
      ],
      "metadata": {
        "id": "gOR6scsQjyay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer default para ingles con reglas de puntacion, contracciones, etc:\n",
        "tokenizer = get_tokenizer('spacy')"
      ],
      "metadata": {
        "id": "M6-oSyEvnXFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb415628-6ede-496d-8636-f01f91abc40b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py:105: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos un ejemplo\n",
        "texto_ejemplo = \"But I don't want nothing at all... if it ain't you, baby\"\n",
        "resultado_ejemplo = tokenizer(texto_ejemplo)\n",
        "print(type(resultado_ejemplo))\n",
        "print(resultado_ejemplo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6citqe3bn5cl",
        "outputId": "7c53fbdd-3a81-4543-f70a-4e937e88c140"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['But', 'I', 'do', \"n't\", 'want', 'nothing', 'at', 'all', '...', 'if', 'it', 'ai', \"n't\", 'you', ',', 'baby']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_example(example):\n",
        "  \"\"\"fn para mapear sobre dataset. Tokeniza el texto y lo agrega a cada example\n",
        "  del dataset. Tiene que devolver un dict para agregar los tokens como\n",
        "  atributos del dataset.\n",
        "  \"\"\"\n",
        "  # limpieza muy simple: reemplaza todo whitespace por un solo espacio\n",
        "  text = re.sub(r'\\s+', ' ', example[\"text\"])\n",
        "  tokens = tokenizer(text)\n",
        "  # return dict para hacer update del dataset inplace\n",
        "  return {\"tokens\": tokens}"
      ],
      "metadata": {
        "id": "0MF6aSmxoIa0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mas adelante vamos a trabajar con batches para acelerar el procesamiento\n",
        "dataset = dataset.map(tokenize_example, batched=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "84613504f22b4a248f831025aa2e047e",
            "a7969133d69241ce818072e140831e00",
            "83008ffe003644e68e039028ee4f818d",
            "ca457fe50a4c4d868cdc485e71921b83",
            "2536e8ca48074d3e996b7f988457254b",
            "1e15f0012e0b45d09548b4a0304159a3",
            "f5f3c00200d74cde950abdbc2d8eacf8",
            "626bfadd52da4610a08bcf97426d8292",
            "c9c12df1f25a4177b31c923f690af6c2",
            "c795a899e2cd495d9ffd11ff1c617f16",
            "a7bcec356a6b4ec7807269831a1e07cb"
          ]
        },
        "id": "gf30oiEwos7B",
        "outputId": "4f9d6b32-1b4c-4429-bff4-e161564da820"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84613504f22b4a248f831025aa2e047e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4US_AVcNsAYr",
        "outputId": "feb48e6f-d3bb-4603-aca9-4f00ef08ef53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['label', 'text', 'tokens'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['label', 'text', 'tokens'],\n",
            "        num_rows: 5000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# veamos un ejemplo\n",
        "print(dataset[\"train\"][\"tokens\"][33][:10]) # 10 primeros tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fof7NRor4qE",
        "outputId": "38ebebe1-30bc-4755-84e2-bcdd1a1861d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# para LM con nltk solo necesitamos conservar los tokens (como una lista de listas de tokens)\n",
        "tokenized_train = dataset[\"train\"][\"tokens\"]\n",
        "tokenized_test = dataset[\"test\"][\"tokens\"]\n",
        "# del dataset"
      ],
      "metadata": {
        "id": "czGRjmnjrt6r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_train[33][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZzXdotNJDEC",
        "outputId": "d9f08810-75dc-43e4-93bf-af02bd01230b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo\n",
        "\n",
        "Vamos a usar trigramas, entonces necesitamos hacer padding con 2 BOS y EOS tokens (begg/end. of sequence)\n",
        "\n",
        "El ngram LM más sencillo es el MLE (Maximum Likelihood Estimator)."
      ],
      "metadata": {
        "id": "gD6Gt8nwsg-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk usa lazy iterators en train y vocab para evitar recrear todos los docs en memoria\n",
        "# se evaluan on demand durante training\n",
        "train, train_flat = padded_everygram_pipeline(3, tokenized_train)"
      ],
      "metadata": {
        "id": "tLVQY_u6oUFT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_everygram_pipeline.__doc__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Olj9ZrMJ9bs",
        "outputId": "15636649-16d2-41a0-f988-21472c923f66"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default preprocessing for a sequence of sentences.\n",
            "\n",
            "    Creates two iterators:\n",
            "\n",
            "    - sentences padded and turned into sequences of `nltk.util.everygrams`\n",
            "    - sentences padded as above and chained together for a flat stream of words\n",
            "\n",
            "    :param order: Largest ngram length produced by `everygrams`.\n",
            "    :param text: Text to iterate over. Expected to be an iterable of sentences.\n",
            "    :type text: Iterable[Iterable[str]]\n",
            "    :return: iterator over text as ngrams, iterator over text as vocabulary data\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cutoff de freq>=2 para el vocab:\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)"
      ],
      "metadata": {
        "id": "7jx-86_aOBIx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens menos y más frecuentes:\n",
        "print(sorted(vocab.counts, key=vocab.counts.get)[:5])\n",
        "print(sorted(vocab.counts, key=vocab.counts.get, reverse=True)[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neB6q-g4RjLM",
        "outputId": "fe664db1-f9ce-4d9c-a1be-862309397765"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['goldberg', 'practitioner', 'nyu', 'referrals', 'drawing']\n",
            "['.', 'the', ',', 'and', 'I']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens ordenados alfabeticamente:\n",
        "print(sorted(vocab.counts)[:5])\n",
        "print(sorted(vocab.counts, reverse=True)[:5])\n",
        "# podriamos mejorar el preprocesamiento!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUTvUwJoTJC4",
        "outputId": "6337639d-b818-4df5-8cb1-d3d880a9d48a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', '#', '$', '$20']\n",
            "['~75', '~6', '~40%+', '~35', '~24\\\\']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# los tokens con frec 1 \"no están en el vocab\" (pero podemos consultar su frec.)\n",
        "print(vocab[\"goldberg\"], \"goldberg\" in vocab)\n",
        "print(vocab[\" \"], \" \" in vocab)\n",
        "print(vocab[\"riquelme\"], \"riquelme\" in vocab)\n",
        "print(vocab[\"the\"], \"the\" in vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD15XJXzR2EB",
        "outputId": "c8dda324-4617-4a2c-8a86-13a1875918bd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 False\n",
            "0 False\n",
            "0 False\n",
            "29137 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ejemplo de sequencia tokenizada:\n",
        "print(vocab.lookup(tokenized_train[33][:10]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5IabP9YVwic",
        "outputId": "65daa3cc-c552-4c5b-c4c5-33fedffb8fca"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('If', 'you', 'want', 'a', 'true', 'understanding', 'of', 'Pittsburgh', 'in', 'the')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# otro ejemplo de sequencia tokenizada:\n",
        "print(vocab.lookup([\"the\", \"goldberg\", \"riquelme\", \".\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bSoW1CAOII5",
        "outputId": "ffcd3376-222f-44e1-c7fa-930fc26e2b11"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the', '<UNK>', '<UNK>', '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nro de tokens que quedaron (vocab size)\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onLjSgEdUSn3",
        "outputId": "3fe6c954-614b-4835-b49f-48255582b52b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14238"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instanciamos el modelo con el highest ngram order\n",
        "lm = MLE(3, vocabulary=vocab)"
      ],
      "metadata": {
        "id": "HeQ76Ynr-G6m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm.fit(train)"
      ],
      "metadata": {
        "id": "p5gvEgQ03LvT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98cb0cde-7d4f-4a5b-811a-4794b45c8f24"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.2 s, sys: 164 ms, total: 16.3 s\n",
            "Wall time: 16.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9hii7V0MMQy",
        "outputId": "217f3941-b1ed-4df3-89d0-c2f206d7e9f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Vocabulary with cutoff=2 unk_label='<UNK>' and 14238 items>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4rH7plLVuBa",
        "outputId": "02432b64-1370-45f8-e55b-62d86edd7f76"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<NgramCounter with 3 ngram orders and 2375001 ngrams>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unigram counts\n",
        "lm.counts['the']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJSdKlF5Vtu0",
        "outputId": "90a727c9-1d74-48e8-9522-7aee04789446"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29137"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bigram counts\n",
        "print(lm.counts[['in']][\"the\"])\n",
        "print(lm.counts[['the']][\"in\"])\n",
        "print(lm.counts[['the']][\"<UNK>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "849ZrN1QWgxC",
        "outputId": "57ccb6e6-2739-4f63-c7ee-63972f329e96"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2129\n",
            "1\n",
            "1268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cada doc tiene padding con 2 BOS y EOS\n",
        "print(lm.counts[[\"<s>\"]][\"<s>\"])\n",
        "print(lm.counts[[\"</s>\"]][\"</s>\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbwc-enHb0KR",
        "outputId": "2d1bb66c-ac7d-4d8c-f956-fe768a3c47a1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (\"<s>\", \"<s>\", 'Got', 'a', 'letter', 'in', 'the', 'mail', '.', \"</s>\", \"</s>\")"
      ],
      "metadata": {
        "id": "WL5-acKmwYX_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trigram counts\n",
        "print(lm.counts[[\"in\", \"the\"]][\"mail\"])\n",
        "print(lm.counts[[\"the\", \"simple\"]][\"fact\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMbVItd_W9yR",
        "outputId": "ea76b875-df4d-4283-c7f9-8a7f92cecc06"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lo mas frecuente despues de un bigrama dado:\n",
        "bigram_example = [\"in\", \"the\"]\n",
        "sorted(lm.counts[bigram_example].items(), key=lambda x: x[1], reverse=True)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWdQOqQ2XgIW",
        "outputId": "2ce66ce7-d553-48e0-e6bb-86be3cd43f48"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('area', 113),\n",
              " ('<UNK>', 90),\n",
              " ('back', 76),\n",
              " ('city', 70),\n",
              " ('middle', 63),\n",
              " ('past', 47),\n",
              " ('Strip', 46),\n",
              " ('restaurant', 38),\n",
              " ('mood', 35),\n",
              " ('morning', 31)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# probabilidad de un token luego de un bigrama:\n",
        "print(lm.score(\"area\", bigram_example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYCpuypxYvaT",
        "outputId": "f0d3d3fe-191b-48db-b14c-8932f7c8e93a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.05307656176608737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos logscore para evitar underflow\n",
        "print(lm.logscore(\"area\", bigram_example))\n",
        "print(np.log2(lm.score(\"area\", bigram_example)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGwR1JwY6nD",
        "outputId": "0546607b-7431-41cd-8a0a-2fb0d62035b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-4.235781272037106\n",
            "-4.235781272037106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "POfAiKjGHu8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_test = tokenized_test[0]\n",
        "print(example_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56EnCPZ0ZvH4",
        "outputId": "831db129-d15c-4243-d2dd-367237916350"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'got', \"'\", 'new', \"'\", 'tires', 'from', 'them', 'and', 'within', 'two', 'weeks', 'got', 'a', 'flat', '.', 'I', 'took', 'my', 'car', 'to', 'a', 'local', 'mechanic', 'to', 'see', 'if', 'i', 'could', 'get', 'the', 'hole', 'patched', ',', 'but', 'they', 'said', 'the', 'reason', 'I', 'had', 'a', 'flat', 'was', 'because', 'the', 'previous', 'patch', 'had', 'blown', '-', 'WAIT', ',', 'WHAT', '?', 'I', 'just', 'got', 'the', 'tire', 'and', 'never', 'needed', 'to', 'have', 'it', 'patched', '?', 'This', 'was', 'supposed', 'to', 'be', 'a', 'new', 'tire', '.', '\\\\nI', 'took', 'the', 'tire', 'over', 'to', 'Flynn', \"'s\", 'and', 'they', 'told', 'me', 'that', 'someone', 'punctured', 'my', 'tire', ',', 'then', 'tried', 'to', 'patch', 'it', '.', 'So', 'there', 'are', 'resentful', 'tire', 'slashers', '?', 'I', 'find', 'that', 'very', 'unlikely', '.', 'After', 'arguing', 'with', 'the', 'guy', 'and', 'telling', 'him', 'that', 'his', 'logic', 'was', 'far', 'fetched', 'he', 'said', 'he', \"'d\", 'give', 'me', 'a', 'new', 'tire', '\\\\\"this', 'time\\\\', '\"', '.', '\\\\nI', 'will', 'never', 'go', 'back', 'to', 'Flynn', \"'s\", 'b', '/', 'c', 'of', 'the', 'way', 'this', 'guy', 'treated', 'me', 'and', 'the', 'simple', 'fact', 'that', 'they', 'gave', 'me', 'a', 'used', 'tire', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lm.vocab.lookup(example_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ7gkDQ5Zu48",
        "outputId": "bd5f68ea-abd7-42d1-fbfa-e94153871e96"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('I', 'got', \"'\", 'new', \"'\", 'tires', 'from', 'them', 'and', 'within', 'two', 'weeks', 'got', 'a', 'flat', '.', 'I', 'took', 'my', 'car', 'to', 'a', 'local', 'mechanic', 'to', 'see', 'if', 'i', 'could', 'get', 'the', 'hole', '<UNK>', ',', 'but', 'they', 'said', 'the', 'reason', 'I', 'had', 'a', 'flat', 'was', 'because', 'the', 'previous', 'patch', 'had', 'blown', '-', 'WAIT', ',', 'WHAT', '?', 'I', 'just', 'got', 'the', 'tire', 'and', 'never', 'needed', 'to', 'have', 'it', '<UNK>', '?', 'This', 'was', 'supposed', 'to', 'be', 'a', 'new', 'tire', '.', '\\\\nI', 'took', 'the', 'tire', 'over', 'to', '<UNK>', \"'s\", 'and', 'they', 'told', 'me', 'that', 'someone', '<UNK>', 'my', 'tire', ',', 'then', 'tried', 'to', 'patch', 'it', '.', 'So', 'there', 'are', 'resentful', 'tire', '<UNK>', '?', 'I', 'find', 'that', 'very', 'unlikely', '.', 'After', 'arguing', 'with', 'the', 'guy', 'and', 'telling', 'him', 'that', 'his', 'logic', 'was', 'far', '<UNK>', 'he', 'said', 'he', \"'d\", 'give', 'me', 'a', 'new', 'tire', '\\\\\"this', '<UNK>', '\"', '.', '\\\\nI', 'will', 'never', 'go', 'back', 'to', '<UNK>', \"'s\", 'b', '/', 'c', 'of', 'the', 'way', 'this', 'guy', 'treated', 'me', 'and', 'the', 'simple', 'fact', 'that', 'they', 'gave', 'me', 'a', 'used', 'tire', '!')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(tokens, lm, ngram_order=3) -> float:\n",
        "    \"\"\"Tenemos que generar los ngrams con padding \"a mano\" en test, procurando\n",
        "    que sea el mismo criterio que en train.\n",
        "    NOTE para evaluar perplexity en muchos docs deberiamos generar una lista de ngrams\n",
        "    de todos los docs\n",
        "    \"\"\"\n",
        "    ngrams_padded = ngrams(\n",
        "        tokens, ngram_order, pad_right=True, pad_left=True, left_pad_symbol=\"<s>\",\n",
        "        right_pad_symbol=\"</s>\")\n",
        "    return lm.perplexity(list(ngrams_padded))"
      ],
      "metadata": {
        "id": "K9GInDjFeF5w"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_train = tokenized_train[33]\n",
        "perplexity(example_train, lm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqh39H7ihxB2",
        "outputId": "b615879f-6966-401e-f552-767067f03f30"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.110706034849242"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(example_test, lm)\n",
        "# necesitamos smoothing / backoff / interpolation para computar perplexity en test!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiT4a1ZbeX-G",
        "outputId": "8cc879af-ccf0-4ea3-db2e-038440380519"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# usamos add-k smoothing (aka Lidstone smoothing, gamma=k)\n",
        "train, train_flat = padded_everygram_pipeline(3, tokenized_train)\n",
        "vocab = Vocabulary(train_flat, unk_cutoff=2)\n",
        "lm_smoothed = Lidstone(order=3, vocabulary=vocab, gamma=.01)"
      ],
      "metadata": {
        "id": "vt7yslv1lvo7"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "lm_smoothed.fit(train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0A7B2enuXw",
        "outputId": "042c6d68-6714-4768-f446-d4cb88bd6c5b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.2 s, sys: 250 ms, total: 17.5 s\n",
            "Wall time: 17.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(example_test, lm_smoothed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t2Px-Y2oARm",
        "outputId": "3cbe1d29-e663-43ca-b5e2-267050757f50"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1368.6006403033784"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# podemos generar texto sampleando\n",
        "tokens_ = lm.generate(30, text_seed=[\"<s>\", \"<s>\"], random_seed=33)\n",
        "tokens_\n",
        "#print(\" \".join(tokens_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQOal3XoID3",
        "outputId": "7f7c2eb4-3411-4571-c847-586f57579ce1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mt.',\n",
              " 'Lebanon',\n",
              " 'for',\n",
              " 'average',\n",
              " 'seafood',\n",
              " '(',\n",
              " 'or',\n",
              " 'at',\n",
              " 'least',\n",
              " 'marginally',\n",
              " 'better',\n",
              " '...',\n",
              " 'because',\n",
              " 'in',\n",
              " 'certain',\n",
              " 'spots',\n",
              " 'it',\n",
              " \"'s\",\n",
              " 'just',\n",
              " 'Pittsburgh',\n",
              " ',',\n",
              " 'offers',\n",
              " 'more',\n",
              " 'than',\n",
              " 'what',\n",
              " 'was',\n",
              " 'served',\n",
              " '2',\n",
              " 'slices',\n",
              " 'of']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_ = lm_smoothed.generate(10, text_seed=[\"<s>\", \"<s>\"], random_seed=33)\n",
        "print(\" \".join(tokens_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Eu2KQHoo0bs",
        "outputId": "182827e2-0c70-4a93-e1ae-6aae6a413bca"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Much like the cast is touring , I quickly ordered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# otras alternativas:\n",
        "# AbsoluteDiscountingInterpolated\n",
        "# WittenBellInterpolated\n",
        "# KneserNeyInterpolated\n",
        "# katz backoff ya no esta implementado en nltk"
      ],
      "metadata": {
        "id": "mVGI3RTUmECp"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Referencias\n",
        "\n",
        "* https://www.nltk.org/api/nltk.lm.html\n",
        "* https://www.nltk.org/_modules/nltk/lm/api.html\n",
        "* https://www.nltk.org/howto/lm.html\n",
        "* https://www.nltk.org/api/nltk.lm.vocabulary.html"
      ],
      "metadata": {
        "id": "WrL0eC9k-zHq"
      }
    }
  ]
}