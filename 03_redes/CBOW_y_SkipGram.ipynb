{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPQVNVfCOEYxV0OHFsKf9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/03_redes/CBOW_y_SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBOW"
      ],
      "metadata": {
        "id": "wlecQodyVoGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg \n",
        "import nltk \n",
        "import numpy as np\n",
        "from string import punctuation \n",
        "import re"
      ],
      "metadata": {
        "id": "ZshI7hEDTwTT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvFo8ynaT0R7",
        "outputId": "dace7be6-d210-4141-dfeb-ee9858e8ed1f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "GTauap3RU7Kf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bible = gutenberg.sents(\"bible-kjv.txt\")\n",
        "remove_terms = punctuation + '0123456789'"
      ],
      "metadata": {
        "id": "Qul02jWqT2Oy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bible"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2cyhos7UK-q",
        "outputId": "bcc0f8e1-6078-41de-ff77-841c36413d3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'The', 'King', 'James', 'Bible', ']'], ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wpt = nltk.WordPunctTokenizer()\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "metadata": {
        "id": "mZ40C0O0USuQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
      ],
      "metadata": {
        "id": "pZmFAl8MUbkJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tmjrciO5TFWd"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import text\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "word2id = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary of unique words\n",
        "word2id['PAD'] = 0\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "\n",
        "vocab_size = len(word2id)\n",
        "embed_size = 100\n",
        "window_size = 2 # context window size\n",
        "\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSpmCsF6UKHI",
        "outputId": "302bac4e-66ce-40fb-acd4-c96e25aace5b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 12425\n",
            "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []            \n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "            \n",
        "            context_words.append([words[i] \n",
        "                                 for i in range(start, end) \n",
        "                                 if 0 <= i < sentence_length \n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n",
        "            \n",
        "            \n",
        "# Test this out for some samples\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "    \n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZeUlyrATKFV",
        "outputId": "012b9a20-faf3-4ce3-d0de-8dc372d8a99e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
            "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
            "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
            "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
            "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
            "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
            "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
            "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
            "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
            "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
            "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# build CBOW architecture\n",
        "cbow = Sequential()\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "# view model summary\n",
        "print(cbow.summary())\n",
        "\n",
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "X46-IBVITNg9",
        "outputId": "81546f56-1210-4ea0-aa8f-2bbab2d5aee7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            1242500   \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 12425)             1254925   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,497,425\n",
            "Trainable params: 2,497,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"320pt\" height=\"405pt\" viewBox=\"0.00 0.00 240.00 304.00\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.75 0.75) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-300 236,-300 236,4 -4,4\"/>\n<!-- 140693872851456 -->\n<g id=\"node1\" class=\"node\">\n<title>140693872851456</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"11,-249.5 11,-295.5 221,-295.5 221,-249.5 11,-249.5\"/>\n<text text-anchor=\"middle\" x=\"49.5\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">InputLayer</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-249.5 88,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"88,-272.5 143,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"115.5\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-249.5 143,-295.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-280.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"143,-272.5 221,-272.5 \"/>\n<text text-anchor=\"middle\" x=\"182\" y=\"-257.3\" font-family=\"Times,serif\" font-size=\"14.00\">[(None, 4)]</text>\n</g>\n<!-- 140693855288528 -->\n<g id=\"node2\" class=\"node\">\n<title>140693855288528</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-166.5 0,-212.5 232,-212.5 232,-166.5 0,-166.5\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\">Embedding</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-166.5 80,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"80,-189.5 135,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-166.5 135,-212.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-197.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"135,-189.5 232,-189.5 \"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n</g>\n<!-- 140693872851456&#45;&gt;140693855288528 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140693872851456-&gt;140693855288528</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-249.37C116,-241.15 116,-231.66 116,-222.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-222.61 116,-212.61 112.5,-222.61 119.5,-222.61\"/>\n</g>\n<!-- 140693895008320 -->\n<g id=\"node3\" class=\"node\">\n<title>140693895008320</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"9,-83.5 9,-129.5 223,-129.5 223,-83.5 9,-83.5\"/>\n<text text-anchor=\"middle\" x=\"40\" y=\"-102.8\" font-family=\"Times,serif\" font-size=\"14.00\">Lambda</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"71,-83.5 71,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"71,-106.5 126,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"98.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"126,-83.5 126,-129.5 \"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-114.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 4, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"126,-106.5 223,-106.5 \"/>\n<text text-anchor=\"middle\" x=\"174.5\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n</g>\n<!-- 140693855288528&#45;&gt;140693895008320 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140693855288528-&gt;140693895008320</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-166.37C116,-158.15 116,-148.66 116,-139.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-139.61 116,-129.61 112.5,-139.61 119.5,-139.61\"/>\n</g>\n<!-- 140693855277552 -->\n<g id=\"node4\" class=\"node\">\n<title>140693855277552</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"15.5,-0.5 15.5,-46.5 216.5,-46.5 216.5,-0.5 15.5,-0.5\"/>\n<text text-anchor=\"middle\" x=\"40.5\" y=\"-19.8\" font-family=\"Times,serif\" font-size=\"14.00\">Dense</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"65.5,-0.5 65.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"93\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">input:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"65.5,-23.5 120.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"93\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">output:</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"120.5,-0.5 120.5,-46.5 \"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-31.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 100)</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"120.5,-23.5 216.5,-23.5 \"/>\n<text text-anchor=\"middle\" x=\"168.5\" y=\"-8.3\" font-family=\"Times,serif\" font-size=\"14.00\">(None, 12425)</text>\n</g>\n<!-- 140693895008320&#45;&gt;140693855277552 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140693895008320-&gt;140693855277552</title>\n<path fill=\"none\" stroke=\"black\" d=\"M116,-83.37C116,-75.15 116,-65.66 116,-56.73\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"119.5,-56.61 116,-46.61 112.5,-56.61 119.5,-56.61\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 6):\n",
        "    loss = 0.\n",
        "    i = 0\n",
        "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
        "        i += 1\n",
        "        loss += cbow.train_on_batch(x, y)\n",
        "        if i % 100000 == 0:\n",
        "            print('Processed {} (context, word) pairs'.format(i))\n",
        "\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
        "    print()"
      ],
      "metadata": {
        "id": "cfUf_NFbVenr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = cbow.get_weights()[0]\n",
        "weights = weights[1:]\n",
        "print(weights.shape)\n",
        "\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
      ],
      "metadata": {
        "id": "6YQ9zoaMVilc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# compute pairwise distance matrix\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "# view contextually similar words\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
        "\n",
        "similar_words"
      ],
      "metadata": {
        "id": "TgLL_NM0VlIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-Gram"
      ],
      "metadata": {
        "id": "bpny2ZLiVxX-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvP43P_m511a"
      },
      "source": [
        "tokenizer = text.Tokenizer()\n",
        "tokenizer.fit_on_texts(norm_bible)\n",
        "\n",
        "word2id = tokenizer.word_index\n",
        "id2word = {v:k for k, v in word2id.items()}\n",
        "\n",
        "vocab_size = len(word2id) + 1 \n",
        "\n",
        "\n",
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Vocabulary Sample:', list(word2id.items())[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMADbZwT51zW"
      },
      "source": [
        "# generate skip-grams\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
        "\n",
        "# view sample skip-grams\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
        "for i in range(10):\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
        "          id2word[pairs[i][0]], pairs[i][0], \n",
        "          id2word[pairs[i][1]], pairs[i][1], \n",
        "          labels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kAxBZzo51yS"
      },
      "source": [
        "# build skip-gram architecture\n",
        "embed_size = 100\n",
        "word_model = Sequential()\n",
        "word_model.add(Embedding(vocab_size, embed_size,\n",
        "                         embeddings_initializer=\"glorot_uniform\",\n",
        "                         input_length=1))\n",
        "word_model.add(Reshape((embed_size, )))\n",
        "\n",
        "context_model = Sequential()\n",
        "context_model.add(Embedding(vocab_size, embed_size,\n",
        "                  embeddings_initializer=\"glorot_uniform\",\n",
        "                  input_length=1))\n",
        "context_model.add(Reshape((embed_size,)))\n",
        "\n",
        "merged_output = add([word_model.output, context_model.output])  \n",
        "\n",
        "model_combined = Sequential()\n",
        "model_combined.add(Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
        "\n",
        "final_model = Model([word_model.input, context_model.input], model_combined(merged_output))\n",
        "final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
        "\n",
        "final_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqE6krKluvev"
      },
      "source": [
        "# visualize model structure\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "\n",
        "SVG(model_to_dot(final_model, show_shapes=True, show_layer_names=False, \n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPywp81h51vq"
      },
      "source": [
        "for epoch in range(1, 3):\n",
        "    loss = 0\n",
        "    for i, elem in enumerate(skip_grams):\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [pair_first_elem, pair_second_elem]\n",
        "        Y = labels\n",
        "        if i % 10000 == 0:\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
        "        loss += final_model.train_on_batch(X,Y)  \n",
        "\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8WawBoc9F-a"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "word_embed_layer = word_model.layers[0]\n",
        "weights = word_embed_layer.get_weights()[0][1:]\n",
        "\n",
        "distance_matrix = euclidean_distances(weights)\n",
        "print(distance_matrix.shape)\n",
        "\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
        "                   for search_term in ['god', 'jesus','egypt', 'john', 'famine']}\n",
        "\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}