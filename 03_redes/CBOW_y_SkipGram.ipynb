{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LCaravaggio/NLP/blob/main/03_redes/CBOW_y_SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlecQodyVoGG"
   },
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZshI7hEDTwTT"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvFo8ynaT0R7",
    "outputId": "a264031d-4b1f-4536-cf8e-ee4f8eaef50c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GTauap3RU7Kf"
   },
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Qul02jWqT2Oy"
   },
   "outputs": [],
   "source": [
    "bible = gutenberg.sents(\"bible-kjv.txt\")\n",
    "remove_terms = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2cyhos7UK-q",
    "outputId": "93ae1b99-3397-43db-fb02-d3903c5a8858"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'King', 'James', 'Bible', ']'], ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'], ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mZ40C0O0USuQ"
   },
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc,re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pZmFAl8MUbkJ"
   },
   "outputs": [],
   "source": [
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c4QzH7M7sS6",
    "outputId": "ff782260-cfc4-4844-80db-96e7a3958f45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king james bible',\n",
       " 'old testament king james bible',\n",
       " 'first book moses called genesis',\n",
       " 'beginning god created heaven earth',\n",
       " 'earth without form void darkness upon face deep']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tmjrciO5TFWd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TSpmCsF6UKHI",
    "outputId": "6fbe3137-9dd1-49e6-84ab-157ae56aa518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12425\n",
      "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5), ('god', 6), ('ye', 7), ('said', 8), ('thee', 9), ('upon', 10)]\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary of unique words\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0ZeUlyrATKFV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            start = max(0, index - window_size)\n",
    "            end = min(sentence_length, index + window_size + 1)\n",
    "            context_words = [words[i] for i in range(start, end) if i != index]\n",
    "            label_word = word\n",
    "\n",
    "            if len(context_words) == 0:\n",
    "                continue\n",
    "\n",
    "            x = pad_sequences([context_words], maxlen=context_length, padding='post', truncating='post')\n",
    "            y = to_categorical([label_word], num_classes=vocab_size)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X46-IBVITNg9",
    "outputId": "79fb3bd4-cc24-441d-8fe8-b9af8e760c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 100)         1242500   \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# build CBOW architecture\n",
    "cbow = tf.keras.Sequential()\n",
    "cbow.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size))\n",
    "cbow.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(tf.keras.layers.Dense(vocab_size, activation='softmax'))\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# view model summary\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfUf_NFbVenr",
    "outputId": "5e786a8f-41d7-4ef7-9153-1a10fa01064c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 1 \tLoss: 4286634.107102927\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 2 \tLoss: 5572901.875400365\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 3 \tLoss: 5602049.088066552\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 4 \tLoss: 5306262.972947944\n",
      "\n",
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n",
      "Epoch: 5 \tLoss: 4765571.131131787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Este entrenamiento tarda unas cuantas horas. Más incluso que la cuota libre de Colab. Corre local y no en clase\n",
    "\n",
    "for epoch in range(1, 6):\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6YQ9zoaMVilc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unto</th>\n",
       "      <td>-2.223604</td>\n",
       "      <td>2.866140</td>\n",
       "      <td>-2.132128</td>\n",
       "      <td>-2.797038</td>\n",
       "      <td>2.601964</td>\n",
       "      <td>2.681766</td>\n",
       "      <td>-2.872133</td>\n",
       "      <td>-2.583190</td>\n",
       "      <td>-2.300686</td>\n",
       "      <td>-2.342377</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.618478</td>\n",
       "      <td>-2.057792</td>\n",
       "      <td>-3.091557</td>\n",
       "      <td>-2.620062</td>\n",
       "      <td>-3.111323</td>\n",
       "      <td>-2.372203</td>\n",
       "      <td>-2.468651</td>\n",
       "      <td>2.786049</td>\n",
       "      <td>-3.365383</td>\n",
       "      <td>-2.642241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lord</th>\n",
       "      <td>-1.971760</td>\n",
       "      <td>2.025880</td>\n",
       "      <td>-1.871813</td>\n",
       "      <td>-2.630960</td>\n",
       "      <td>1.844649</td>\n",
       "      <td>2.134659</td>\n",
       "      <td>-2.308367</td>\n",
       "      <td>-2.249089</td>\n",
       "      <td>-1.853627</td>\n",
       "      <td>-2.492036</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.294261</td>\n",
       "      <td>-1.557231</td>\n",
       "      <td>-2.767344</td>\n",
       "      <td>-1.919199</td>\n",
       "      <td>-2.471757</td>\n",
       "      <td>-2.223515</td>\n",
       "      <td>-2.739373</td>\n",
       "      <td>2.338367</td>\n",
       "      <td>-2.891318</td>\n",
       "      <td>-2.369243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thou</th>\n",
       "      <td>-1.575344</td>\n",
       "      <td>1.881439</td>\n",
       "      <td>-2.076868</td>\n",
       "      <td>-2.321306</td>\n",
       "      <td>1.814253</td>\n",
       "      <td>2.600374</td>\n",
       "      <td>-2.433875</td>\n",
       "      <td>-2.268668</td>\n",
       "      <td>-1.838522</td>\n",
       "      <td>-1.919915</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.383743</td>\n",
       "      <td>-2.052448</td>\n",
       "      <td>-2.069975</td>\n",
       "      <td>-2.088795</td>\n",
       "      <td>-1.974004</td>\n",
       "      <td>-2.089131</td>\n",
       "      <td>-1.774704</td>\n",
       "      <td>2.263765</td>\n",
       "      <td>-2.727866</td>\n",
       "      <td>-1.878055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thy</th>\n",
       "      <td>-1.868516</td>\n",
       "      <td>2.095453</td>\n",
       "      <td>-2.162520</td>\n",
       "      <td>-2.344458</td>\n",
       "      <td>2.252535</td>\n",
       "      <td>2.563588</td>\n",
       "      <td>-2.368776</td>\n",
       "      <td>-2.587917</td>\n",
       "      <td>-1.901774</td>\n",
       "      <td>-2.274439</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.156907</td>\n",
       "      <td>-2.147224</td>\n",
       "      <td>-2.367692</td>\n",
       "      <td>-2.516188</td>\n",
       "      <td>-2.642004</td>\n",
       "      <td>-2.152353</td>\n",
       "      <td>-2.485777</td>\n",
       "      <td>2.384677</td>\n",
       "      <td>-3.720293</td>\n",
       "      <td>-2.286330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>god</th>\n",
       "      <td>-1.902769</td>\n",
       "      <td>2.590968</td>\n",
       "      <td>-2.166515</td>\n",
       "      <td>-2.294959</td>\n",
       "      <td>2.159779</td>\n",
       "      <td>2.387039</td>\n",
       "      <td>-2.059349</td>\n",
       "      <td>-2.666387</td>\n",
       "      <td>-2.395756</td>\n",
       "      <td>-2.151420</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.238323</td>\n",
       "      <td>-2.098771</td>\n",
       "      <td>-2.594616</td>\n",
       "      <td>-2.025050</td>\n",
       "      <td>-2.382270</td>\n",
       "      <td>-2.340272</td>\n",
       "      <td>-2.612186</td>\n",
       "      <td>2.441282</td>\n",
       "      <td>-2.854055</td>\n",
       "      <td>-2.107414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "unto -2.223604  2.866140 -2.132128 -2.797038  2.601964  2.681766 -2.872133   \n",
       "lord -1.971760  2.025880 -1.871813 -2.630960  1.844649  2.134659 -2.308367   \n",
       "thou -1.575344  1.881439 -2.076868 -2.321306  1.814253  2.600374 -2.433875   \n",
       "thy  -1.868516  2.095453 -2.162520 -2.344458  2.252535  2.563588 -2.368776   \n",
       "god  -1.902769  2.590968 -2.166515 -2.294959  2.159779  2.387039 -2.059349   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "unto -2.583190 -2.300686 -2.342377  ... -2.618478 -2.057792 -3.091557   \n",
       "lord -2.249089 -1.853627 -2.492036  ... -2.294261 -1.557231 -2.767344   \n",
       "thou -2.268668 -1.838522 -1.919915  ... -2.383743 -2.052448 -2.069975   \n",
       "thy  -2.587917 -1.901774 -2.274439  ... -2.156907 -2.147224 -2.367692   \n",
       "god  -2.666387 -2.395756 -2.151420  ... -2.238323 -2.098771 -2.594616   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "unto -2.620062 -3.111323 -2.372203 -2.468651  2.786049 -3.365383 -2.642241  \n",
       "lord -1.919199 -2.471757 -2.223515 -2.739373  2.338367 -2.891318 -2.369243  \n",
       "thou -2.088795 -1.974004 -2.089131 -1.774704  2.263765 -2.727866 -1.878055  \n",
       "thy  -2.516188 -2.642004 -2.152353 -2.485777  2.384677 -3.720293 -2.286330  \n",
       "god  -2.025050 -2.382270 -2.340272 -2.612186  2.441282 -2.854055 -2.107414  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TgLL_NM0VlIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 12424)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'god': ['ye', 'also', 'made', 'unto', 'one'],\n",
       " 'jesus': ['spirit', 'time', 'many', 'cast', 'heaven'],\n",
       " 'noah': ['flood', 'kind', 'uncleanness', 'likeness', 'birds'],\n",
       " 'egypt': ['smote', 'cut', 'servants', 'princes', 'anger'],\n",
       " 'john': ['peter', 'knew', 'whether', 'entered', 'new'],\n",
       " 'gospel': ['hope', 'grace', 'hearts', 'entered', 'sound'],\n",
       " 'moses': ['kept', 'power', 'prophets', 'whole', 'received'],\n",
       " 'famine': ['fallen', 'slay', 'consumed', 'edom', 'rulers']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# compute pairwise distance matrix\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "# view contextually similar words\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "                   for search_term in ['god', 'jesus', 'noah', 'egypt', 'john', 'gospel', 'moses','famine']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpny2ZLiVxX-"
   },
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nvP43P_m511a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 12425\n",
      "Vocabulary Sample: [('shall', 1), ('unto', 2), ('lord', 3), ('thou', 4), ('thy', 5)]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1\n",
    "\n",
    "\n",
    "wids = [[word2id[w] for w in text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BMADbZwT51zW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bible (5766), king (13)) -> 1\n",
      "(bible (5766), drove (2932)) -> 0\n",
      "(bible (5766), james (1154)) -> 1\n",
      "(james (1154), king (13)) -> 1\n",
      "(james (1154), sorcery (11693)) -> 0\n",
      "(king (13), weaken (10773)) -> 0\n",
      "(james (1154), bible (5766)) -> 1\n",
      "(king (13), james (1154)) -> 1\n",
      "(king (13), shecaniah (7503)) -> 0\n",
      "(james (1154), ithream (7275)) -> 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0],\n",
    "          id2word[pairs[i][1]], pairs[i][1],\n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4kAxBZzo51yS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "embedding_7_input (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8_input (InputLayer)  [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 1, 100)       1242500     embedding_7_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 1, 100)       1242500     embedding_8_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 100)          0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, 100)          0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 100)          0           reshape_4[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "sequential_14 (Sequential)      (None, 1)            101         add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,485,101\n",
      "Trainable params: 2,485,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build skip-gram architecture\n",
    "embed_size = 100\n",
    "word_model = tf.keras.Sequential()\n",
    "word_model.add(tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                         embeddings_initializer=\"glorot_uniform\",\n",
    "                         input_length=1))\n",
    "word_model.add(tf.keras.layers.Reshape((embed_size, )))\n",
    "\n",
    "context_model = tf.keras.Sequential()\n",
    "context_model.add(tf.keras.layers.Embedding(vocab_size, embed_size,\n",
    "                  embeddings_initializer=\"glorot_uniform\",\n",
    "                  input_length=1))\n",
    "context_model.add(tf.keras.layers.Reshape((embed_size,)))\n",
    "\n",
    "merged_output = tf.keras.layers.add([word_model.output, context_model.output])\n",
    "\n",
    "model_combined = tf.keras.Sequential()\n",
    "model_combined.add(tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "\n",
    "final_model = tf.keras.models.Model([word_model.input, context_model.input], model_combined(merged_output))\n",
    "final_model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\n",
    "\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hPywp81h51vq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
      "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 1 Loss: 3634.3396557169035\n",
      "Processed 0 (skip_first, skip_second, relevance) pairs\n",
      "Processed 10000 (skip_first, skip_second, relevance) pairs\n",
      "Processed 20000 (skip_first, skip_second, relevance) pairs\n",
      "Epoch: 2 Loss: 3199.733184985118\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += final_model.train_on_batch(X,Y)\n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "D8WawBoc9F-a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12424, 12424)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'god': ['man', 'way', 'lord', 'upon', 'stand'],\n",
       " 'jesus': ['marvel', 'stir', 'offend', 'searcheth', 'imputed'],\n",
       " 'egypt': ['drew', 'mount', 'gilead', 'battle', 'wickedness'],\n",
       " 'john': ['pleasures', 'council', 'denied', 'angels', 'devil'],\n",
       " 'famine': ['strange', 'tents', 'guard', 'pay', 'army']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0][1:]\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "print(distance_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1]\n",
    "                   for search_term in ['god', 'jesus','egypt', 'john', 'famine']}\n",
    "\n",
    "similar_words"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyMwPiBBOt/MPbFWpIOyvYty",
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
